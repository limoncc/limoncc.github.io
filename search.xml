<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[macOS_mojave_10.14.6下Tensorflow2.6的安装]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2021-09-18-mac%E5%AE%89%E8%A3%85tensorflow2.6%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2021-09-18-mac安装tensorflow2.6/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、下载源码本人电脑macOS mojave 10.14.6 先下载，进入目录定位到v2.6.0123git clone https://github.com/tensorflow/tensorflow.gitcd tensorflowgit checkout v2.6.0 二、准备必要环境准备必要环境，请安装好java,和minconda 12345678brew install bazelisk conda create -n pencil python=3.8conda activate pencilpip install -U pip pip install -U sixpip install numpy=1.19.5pip install -U wheelpip install -U keras_preprocessing --no-deps 三、开始编译关键语句，-march=native会做cpu指令集优化 bazelisk build -c opt –copt=-march=native //tensorflow/tools/pip_package:build_pip_package 12345678910111213141516171819202122232425262728293031323334353637383940./configureYou have bazel 3.7.2 installed.Please specify the location of python. [Default is /Users/xiaobai/miniconda/envs/dawn/bin/python3]:Found possible Python library paths: /Users/xiaobai/miniconda/envs/dawn/lib/python3.8/site-packagesPlease input the desired Python library path to use. Default is [/Users/xiaobai/miniconda/envs/dawn/lib/python3.8/site-packages]Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -Wno-sign-compare]:Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Do you wish to build TensorFlow with iOS support? [y/N]: nNo iOS support will be enabled for TensorFlow.Preconfigured Bazel build configs. You can use any of the below by adding "--config=&lt;&gt;" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=mkl_aarch64 # Build with oneDNN and Compute Library for the Arm Architecture (ACL). --config=monolithic # Config for mostly static monolithic build. --config=numa # Build with NUMA support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects. --config=v1 # Build with TensorFlow 1 API instead of TF 2 API.Preconfigured Bazel build configs to DISABLE default on features: --config=nogcp # Disable GCP support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finishedbazelisk build -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 在/tmp/tensorflow_pkg找到wheel文件，pip安装即可。目前编译了这些，看官按需下载 下载链接 tensorflow-2.6.0-cp38-cp38-numpy_1.21.2-macosx_10_14_x86_64.whl 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2021-09-18-mac安装tensorflow2.6/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>工程问题</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测度论与革命一]]></title>
    <url>%2F%E7%A7%91%E6%99%AE%2F2019-01-01-%E6%B5%8B%E5%BA%A6%E8%AE%BA%E4%B8%8E%E9%9D%A9%E5%91%BD%E4%B8%80%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/科普/2019-01-01-测度论与革命一/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文主要用革命的比喻，通俗的总结了测度论的基本问题，并且添加了我自己的一些体会。若有错误，请大家指正。关键词: 测度论,科普,革命 一、开篇大家好，这里用黑帮或者革命的故事，通俗讲解集合论、测度论的基本知识。帮助大家理解这门颇为难懂的数学知识。本系列的初衷，是以前在学习时的一些困惑，教科书过于冰冷与严肃，很多概念不知所云，理解学习数学最好的方法之一就是举例子，看数学书本身就是解读故事的过程，故事的精彩取决于你的理解。希望这种看数学书就像看故事书的体验，大家也能有所感受。这就是我的初衷。 初次开篇，写的还不够科普，在之后的岁月我会尽量通俗化，清晰化，故事化，大家敬请期待。也是我博客很久没有更新了，未来推进自身构建可计算知识，我准备开始写些文章，更新不定。本文主要内容，在知乎问答回答过，属于原创，请遵守版权声明。 集合是数学的基石，初等集合论是非常直观的。问题是一旦引入无限的概念，理论就开始复杂起来。在无限的影响下诞生了很多概念。下面我们就来说明一下常用的点集，我们先回顾一下下面点集的定义： 内点：$\displaystyle \exists r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：某种关系下的小弟（包括他自己） 外点：$\displaystyle \forall r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：所有各种关系下的小弟（包括他自己） 边界点：$\displaystyle \forall r &gt;0\,,\mathring{U}(x_0,r)=\{x \in S-\{x_0\}\mid d(x,x_0)&lt;r\}$：所有各种关系（不包括他自己）的小弟 此处应该有配图，稍后补上。 其中$\displaystyle S$是某一集合、 $\displaystyle x_0$是我们的点集里面点、 $\displaystyle d(x,x_0)$表示距离，这个距离是抽象的，并不一定是欧几里得距离。我们将点集的定义翻译为人话，或者一个故事情节。基于此，我们来开始我们的旅程。 二、点集の想像一如果某国 $\displaystyle S$有一个无限人数组成的黑帮 $\displaystyle A=\{x \in R \mid x $是黑帮一成员$\} \subset S$ 说 $ x$是内点：意思是说 $ x$在某种关系下所有小弟（包括他自己）是黑帮的成员。显然 $ x$自己是黑帮成员 说 $ x$是外点：意思是说 $ x$在某种关系下的所有小弟（包括他自己）都不是的黑帮成员，显然x 自己不是黑帮成员 说 $ x$是边界点：意思是说 $ x$所有各种关系下的有一部分小弟（包括他自己）都是的黑帮成员，显然 $ x$自己可能是，也可能不是黑帮成员 说 $ x$是孤立点：意思是说 $ x$在所有关系下的小弟（不包括他自己）都不是的黑帮成员，但是他自己是黑帮成员，所以孤立点这个名词：还是非常形象的。显然孤立点是边界点的成员并且是黑帮成员。 三、点集の想像二1、可见孤立点 $\displaystyle x_0$ 是忠诚的，那么他应该是黑帮最可以信赖的人，因为他只和组织 $A$ 有关系 $\displaystyle x_0\in A$。但是关系似乎不是很好，是个独行侠。是一把利刃。但是不忠诚的话，也有可能是警方卧底。因为他和组织外有关系。 2、内点，很显然他们组织的基础。孤立点是边界点中，是黑帮成员那一部分。 3、边界点是组织要提防的争取的人。 4、外点，显然不是非常重要的。 同理：某国 $\displaystyle S$有一个有限人数组成的黑帮，上述定义也是有意义的。 四、点集の想像三我们来继续考虑 $\displaystyle S$国的一个无限人数组成的黑帮 $\displaystyle A$$\displaystyle x$是极限点： 意思是说 $\displaystyle x$在所有各种关系（不包括他自己）总有一个小弟是黑帮成员。 显然 $ x$ 自己可能是，也可能不是黑帮成员。 $\displaystyle x$是聚点：意思是说 $ x$在所有各种关系下有无数个小弟是黑帮成员。 五、点集の想像四1、我们发现极限点和聚点其实是一个意思。证明请看 bady rudin 2.20定理。还有一些书上有附着点 $ x$的定义，是一个意思。2、显然聚点：是带特殊关系的黑帮成员或者是非黑帮成员。通过组织其他的人，总可以环环相扣找到他聚点。3、如果聚点不是黑帮成员，可以设想他是某个高官。是这个黑帮的后台，但是不是黑帮成员，但是通过组织其他的人，总可以环环相扣找到他。4、如果聚点是黑帮成员，可以想想，他混的不错，是个能人 六、点集の想像五如果某国 $\displaystyle S$有一个有限人数组成的黑帮 B我们可以发现，黑帮 $\displaystyle B$中没有极限点。证明请看baby rudin 2.20定理。 七、点集の想像六显然我们思考有限和无限时，内、边、外和孤是可以的。对于无限，还有聚点的概念。而且有 Weierstrass Theorem魏尔斯特拉斯聚点定理： $\displaystyle \Bbb{R}^n$中每个有界无限子集在 $\displaystyle \Bbb{R}^n$中有聚点。 换句话说， 每个黑帮都有一个强大的高官后台或者内部的能人。这就是神秘无限世界的一瞥。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/科普/2019-01-01-测度论与革命一/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>科普</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>科普</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型的维特比算法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2018-01-09-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2018-01-09-隐马尔可夫模型的维特比算法/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清维特比算法的基础问题。本文通过清晰的数学符号，说明问题，大大易于理解。若有错误，请大家指正。关键词: 隐马尔可夫模型,维特比算法,动态规划 一、问题描述定义隐变量数据集 $\displaystyle \mathcal{D}_T^z$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}$,完全数据集 $\displaystyle \mathcal{D}^+_T$。维特比算法要解决的是求隐变量最可能状态序列。可以理解为：已知观测变量数据集，推断隐变量数据集：$$\begin{align}\hat{\mathcal{D}_T^\bm{z}}=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^\bm{z}}\,p \big(\mathcal{D}_T^\bm{z}\mid\mathcal{D}_T^\bm{x}\big)\end{align}$$注意到：$$\begin{align}p \big(\mathcal{D}_{t}^z\mid \mathcal{D}_t^\bm{x}\big)=p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\big/p \big(\mathcal{D}_t^\bm{x}\big)\propto p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\end{align}$$相差一个常数 $ p \big(\mathcal{D}_t^\bm{x}\big)$，通过直接优化完全数据集会简化问题。我们选择最大化下式：$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{\mathcal{D}_t^z}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\\end{align}$$其中完全数据集的概率 $\displaystyle \pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]$，写成对数形式有：$$\begin{align}\ln\pi[i_1]+\ln\rho_1[i_1]+\sum_{t=2}^T \bigg[\ln A_t[i_{t-1},i_{t}]+\ln \rho_t[i_t]\bigg]\end{align}$$这个形式有利于我们着手分析。 二、前向计算我们可以拆分隐变量数据集 $ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t\}$，拆分的关键直觉是时刻 $ t$的最可能路径必须有是由时刻 $ t-1$的最可能路径组成。问题变为$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{z_t}\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$追寻这一关键思想，下面来具体化：我们假设 $ t$时刻的状态为 $ i_t$，进而定义路径$ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t=i_t\}$的最大概率(权重)：$$\begin{align}\delta_t[i_t]=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$为了充分利用隐马尔可夫模型的条件独立性质和动态规划思想，假设 $ t-1$的状态为 $ i_{t-1}$。继续拆分数据集于是有：$$\begin{align}\delta_{t}[i_t]&amp;=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\\&amp;=\max_{\mathcal{D}_{t-2}^z,z_{t-1}=i_{t-1}}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\&amp;=\max_{i_{t-1}}\bigg[p\big(z_t=i_t\mid z_{t-1}=i_{t-1}\big)p\big(\bm{x}_t\mid z_t=i_t\big)\max_{\mathcal{D}_{t-2}^z} p \big(\mathcal{D}_{t-2}^\bm{x},\mathcal{D}_{t-1}^z,z_{t-1}=i_{t-1}\big)\bigg]\\&amp;=\max_{i_{t-1}} \delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\\\end{align}$$也就是说：时刻 $ t$行至状态 $ i_t$的最可能路径必须有是由时刻 $ t-1$ 行至其他状态 $ i_{t-1}$的最可能路径组成。$$\begin{align}\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$定义初始状态 $ \delta_1[i_1]=\pi[i_1]\rho_1[i_1]$。利用这公式，已知观测变量数据集 $ \mathcal{D}_T^\bm{x}$，可以求时刻 $ t$状态为 $ i_t$的最大概率，有$$\begin{align}\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\end{align}$$ 三、后向回溯回到我们的问题：已知观测变量数据集，推断隐变量数据集。最大化联合概率问题变为$$\begin{align}\max_{\mathcal{D}_T^z} p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=\max_{i_T}\max_{i_{T-1}}A[i_{T-1},i_t] \rho_T[i_T]\cdots\max_{i_1}\delta_1[i_1]A[i_{1},i_2] \rho_2[i_2]\end{align}$$这样一种转换称为 max-product操作。 为了解决这个问题，回顾动态规划思想：最优路径 $ \hat{\mathcal{D}}_{1:T}^z$的一部分 $ \hat{\mathcal{D}}_{t:T}^z$对于 $ t:T$的所有可能路径 $ \mathcal{D}_{t:T}^z$必然是最优。如果存在另外一条路径 $ \tilde{\mathcal{D}}_{t:T}^z$是最优的，那么会出现矛盾 $ \hat{\mathcal{D}}_{1:t}^z\cup \tilde{\mathcal{D}}_{t:T}^z\neq \hat{\mathcal{D}}_{1:T}^z $，所以 $ \hat{\mathcal{D}}_{t:T}^z$ 必须是最优的。根据这一思想，我们定义回溯操作 traceback: $ \omega_t[\cdot]$，来从后向前还原最优状态序列。$$\begin{align}\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$定义 $ T$时刻最优状态 $\displaystyle \hat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]$。应用回溯操作，得到最优路径：$$\begin{align}\hat{\mathcal{D}}_{T}^z=\{\hat{z}_{t-1}=\omega_t[\hat{z}_{t}]\}_{t=T}^1\end{align}$$ 为了解决数据下溢问题，我们可以取对数$$\begin{align}&amp;\ln\delta_{t}[i_t]=\max_{i_{t-1}}\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\end{align}$$ 算法：维特比算法 $1\displaystyle \bm{\delta}_1=\bm{\pi}^\text{T}\bm{A}$$2\displaystyle \text{ for }\,t=2:T$$\displaystyle\quad\begin{array}{|lc}\text{ for }\,i_{t}=1:K \\\quad\begin{array}{|lc}\displaystyle \big[\ln\delta_t[i_t],\omega_{t-1}[i_t]\big]=\max_{i_{t-1}}\ln \bm{\delta}_{t-1}\odot \ln\bm{A}+\bm{I}\ln B_{t}[i_{t},x_t]\big]\end{array}\\\text{ end}\\\end{array}$3end$\displaystyle [\ln\delta_T,\hat{z}_{T}]=\max_{i_T}\,\ln\bm{\delta}_T$$4\displaystyle \text{ for }\,t=T:2$ $\displaystyle\quad\begin{array}{|lc}\hat{z}_{t-1}=\omega_{t}[\hat{z}_{t}]\end{array}$5end$6\displaystyle \hat{\bm{z}}$ 四、评述1、在切分数据集后，同过充分利用隐马尔可夫模型的条件独立假设，我们得到迭代方程和回溯方程。$$\begin{align}&amp;\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$2、使用 max-product，我们认识到寻找最优隐状态序列问题是个动态规划问题。3、然后很自然，使用动态规划，解决了问题。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2018-01-09-隐马尔可夫模型的维特比算法/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>维特比算法</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类分布大意]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-05-06-%E5%88%86%E7%B1%BB%E5%88%86%E5%B8%83%E5%A4%A7%E6%84%8F%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清分类分布的基础问题。若有错误，请大家指正。关键词: multinoulli,Categorical,softmax 一、分类分布的若干形式1.1、分类分布指示形式第一，回顾一下猫猫分布 (multinoulli distribution)或者叫 (Categorical distribution)。因为有英文 cat，我又叫它猫猫分布🐈：$$\begin{align}\mathrm{Cat}(x\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{\mathbb{I}(x=c)},x\in\{1,…,C\}\end{align}$$其中$\displaystyle \bm{\mu}=[\mu_1,\mu_2,…,\mu_C]^\text{T}\,,\sum_{c=1}^{C}\mu_c=\bm{1}^\text{T}\bm{\mu}=1$ 1.2、分类分布0-1编码形式第二、特别的，我们使用0-1编码来表示分类时，我们有：$$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{x_c}\,,\bm{x}=[x_1,x_2,…,x_C]^\text{T}\,,\bm{x}\in\{0,1\}^C\end{align}$$其中 $\displaystyle \sum_{c=1}^{C}x_c=\bm{1}^\text{T}\bm{x}=1$ 1.3、分类分布指数族形式第三、继续使用0-1编码，现在我们使用指数族的思想，对分类分布，也就是这个猫猫加以变形，这形式是我们要经常用到的形式： $$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})&amp;=\prod_{c=1}^{C}\mu_c^{x_c}=\exp\bigg[\sum_{c=1}^C x_c\ln\mu_c\bigg]=\exp\bigg[\bm{x}^\text{T}\ln \bm{\mu}\bigg]\\&amp;=\exp\bigg[\bm{x}^\text{T}\ln\frac{\bm{\mu}}{\mu_C}+\ln \mu_C\bigg]\end{align}$$我们定义一个新的向量 $\displaystyle \bm{\eta}=\ln\frac{\bm{\mu}}{\mu_C}$，于是有：$$\begin{align}\bm{\mu}=\mathcal{S}\big(\bm{\eta}\big)=\mathrm{softmax}\big(\bm{\eta}\big)=\Bigg[\frac{\mathrm{e}^{\eta_c}}{\sum_{c=1}^{C}\mathrm{e}^{\eta_c}}\Bigg]_{1\times C}=\frac{\mathrm{e}^\bm{\eta}}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\end{align}$$亦有$$\begin{align}\bm{x}\sim \mathrm{Cat}\big(\bm{x}\mid \mathcal{S}\big(\bm{\eta}\big)\big)=\exp \big[\bm{x}^\text{T}\bm{\eta}-\ln [\bm{I}^\text{T}\mathrm{e}^\bm{\eta}]\big]\end{align}$$ 二、分类分布、sofxmax和多元logistic回归。2.1、多元logistic回归模型考虑如下广义线性模型连接函数 $\displaystyle g \big(\bm{\mu}\big)=\bm{\eta}=\bm{W}\bm{x}\to \bm{\mu}=\mathcal{S}\big(\bm{W}\bm{x}\big)$。这一模型的形式具有十分重要的意义，首先让我们把诸多特征与与分类建立的了数学模型，其次分类分布的期望是一组归一化的概率，直接代表了我们对一次特征观测应该对应于哪个分类的信心。辅助以决策论，很容易做出推断。$$\begin{align}\bm{y}\sim \mathrm{Cat}\big(\bm{y}\mid \mathcal{S}\big(\bm{W}\bm{x}\big)\big)\end{align}$$我们有对数似然：$$\begin{align}\ell \big(\mathcal{D}\mid \bm{W}\big)&amp;=\ln \prod_{i=1}^N\prod_{c=1}^{C}\mathcal{S}_{ic}^{y_{ic}}=\sum_{i=1}^N \bm{y}_i ^\text{T}\ln \mathcal{S}\big(\bm{W}\bm{x}_i\big)=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{W}\bm{x}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{W}\bm{x}_i\big]\bigg)\bigg]\\&amp;=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{\eta}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]\bigg)\bigg]\end{align}$$ 2.2、多元logistic回归的梯度我们有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\sum_{i=1}^N \Bigg[\bm{y}_i- \frac{\mathrm{diag}\big[\mathrm{e}^{\bm{\eta}_i}\big]\bm{I}}{\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]}\Bigg]=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\end{align}$$有如下微分$$\begin{align}\mathrm{d}\ell=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i=\mathrm{tr}\bigg(\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i\bigg)\end{align}$$注意到 $\displaystyle \bm{\eta}_i=\bm{W}\bm{x}_i\to \mathrm{d}\bm{\eta}_i=\mathrm{d}\bm{W}\bm{x}_i$于是根据数量函数与矩阵微分的地定义有 ：$$\begin{align}&amp;\mathrm{d}\ell=\mathrm{tr}\bigg(\frac{\partial \ell}{\partial \bm{\eta}_i ^\text{T}}\cdot\mathrm{d}\bm{\eta}_i\bigg)=\sum_{i=1}^N\mathrm{tr}\Big( \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\bm{x}_i\Big)=\sum_{i=1}^N\mathrm{tr}\Big( \bm{x}_i\big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\Big)\\&amp;\Longrightarrow \frac{\partial \ell}{\partial \bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}\end{align}$$也就是说我们有梯度$$\begin{align}\nabla_{\bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}=\sum_{i=1}^N \big[\bm{y}_i- \bm{\mu}_i\big]\bm{x}_i^\text{T}\end{align}$$ 2.3、多元logistic回归的海赛矩阵注意到梯度的维数是 $\displaystyle D\times D$，应用 $\displaystyle \frac{\partial \big[a(\bm{x})\bm{f}(\bm{x})\big]}{\partial \bm{x}^\text{T}}=a\frac{\partial \bm{f}}{\partial \bm{x}^\text{T}}+\bm{f}\frac{\partial a}{\partial \bm{x}^\text{T}}$，于是我们注意到有如下结论$$\begin{align}\dot{\mathcal{S}}&amp;=\frac{\partial \mathcal{S}\big(\bm{\eta}\big)}{\partial \bm{\eta}^\text{T}}=\frac{1}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]-\mathrm{e}^\bm{\eta}\left[\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]\bm{I}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}\right]^\text{T}\\&amp;=\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}-\frac{ \mathrm{e}^\bm{\eta}\big[\mathrm{e}^\bm{\eta}\big]^\text{T}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}=\mathrm{diag}\big[\bm{\mu}\big]-\bm{\mu}\bm{\mu}^\text{T}\end{align}$$现在我们更进一步求这个矩阵梯度的微分，同时注意到有( $\displaystyle \mathrm{vec}\big(\bm{A}\bm{X}\bm{B}\big)=\big[\bm{B}^\text{T}\otimes \bm{A}\big]\mathrm{vec}\big(\bm{X}\big)$)，于是我们可以导出矩阵导数：$$\begin{align}&amp;\mathrm{d}\nabla=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{\eta}_i \cdot\bm{x}_i ^\text{T}=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\\\Longrightarrow&amp;\mathrm{vec}\big(\mathrm{d}\nabla\big)=\mathrm{vec}\bigg(\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\bigg)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N \mathrm{vec}\big(\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\big)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N\big[\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\big]\mathrm{d}\mathrm{vec}\big(\bm{W}\big)\\\Longrightarrow&amp;\bm{H}=\frac{\partial \mathrm{vec}\big(\nabla\big)}{\partial \mathrm{vec}^\text{T}\big(\bm{W}\big)}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{align}$$为了方便计算，我们把梯度也向量化，于是我们有$$\begin{align}\begin{cases}\displaystyle\nabla=\mathrm{vec}\big(\nabla_{\bm{W}}\big)=\sum_{i=1}^N\bm{x}_i \otimes\big[\bm{y}_i- \bm{\mu}_i\big]\\\displaystyle\bm{H}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{cases}\end{align}$$牛顿-拉弗迭代法有：$$\begin{align}\mathrm{vec}\big(\bm{W}\big):=\mathrm{vec}\big(\bm{W}\big)-\bm{H}^{-1}\nabla\end{align}$$当然各种优化方法都可以加以应用。 三、评述1、分类分布，有人又叫多项式分布，这里n=1，不过我不太赞成这个说法，使用分类分布更加恰当。当然更萌(๑•ᴗ•๑)一点可以叫猫猫分布。2、分类分布是一种很特殊的分布：对任意一个连续分布，如果我们将其离散化，我们都可以归结为一个分类分布。3、分类问题可以归结为：对特征向量分类，或者说对特征空间进行划分。由于与分类分布的关系使得 softmax函数经常出现，一般把它放在神经网络的最后一层来输出概率。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>分类分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-04-05-%E6%84%9F%E7%9F%A5%E5%99%A8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A013%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习13/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 神经网络神经网络是由简单处理单元构成的大规模并行分布式处理器。 神经元 $$\begin{align}o_j(t)=f\{[\sum_{i=1}^{n}w_{ij}x_i(t-\tau_{ij})]-T_j\}\end{align}$$ 单神经元—感知机模型（Perceptron）模型的建立1943年生理学家W.S.McCulloch和W.A.Pitts提出了形式神经元数学模型，史称M-P模型。开创了神经科学理论研究的新时代！ 1949年心理学家Donald Olding Hebb在《行为构成》（Organization of Behavior）中提出了Hebb算法。而且首次提出了连接主义(connectionism)：大脑活动是靠脑细胞的组合连接实现的。 1958年美国Frank Rosenblatt提出了感知机（Perceptron）这应该是世界上第一个真正优秀的人工神经网络 概念与符号下面我们将说明这个模型，1、 $\displaystyle \boldsymbol{x}=[x_1,…,x_k]^\text{T}$为特征向量， 或者叫输入向量2、 $\displaystyle \boldsymbol{w}=[w_1,…,w_k]^\text{T}$为权值向量，也可以叫突触权值向量 ，模仿神经元的突触。3、 $\displaystyle w_0$为阈值。 $\displaystyle o=f$4、为简洁记令 $\displaystyle x_0=1$,于是有增广特征向量 $\displaystyle \boldsymbol{x}:=[x_0,x_1,…,x_k]^\text{T}$。5、增广权值向量$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$。6、定义输出 $\displaystyle o$，7、激活函数：硬限函数 $\displaystyle \mathrm{hardlims}(x)=\begin{cases} 1, &amp;x\geqslant0 \\\ -1, &amp;x&lt;0\end{cases}$。于是有：$$\begin{align}o=\mathrm{hardlims}\left(\boldsymbol{w}^\text{T}\boldsymbol{x}\right)\end{align}$$ 神经元 感知机学习与训练感知机学习算法：误差修正学习算法对于二分类增广数据集 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$，注意我们用的都是增广数据，$\displaystyle \boldsymbol{x}:=[1,x_1,…,x_k]^\text{T}$，$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$，同时 $\displaystyle y=+1\text{ or }-1$，表示 $\displaystyle c_1 \text{ or } c_2$类。 学习误差$\displaystyle y_t-o_t$于是有： 算法：误差修正学习算法1 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\eta_t(y_t-o_t)\boldsymbol{x}_t\end{array}\\$3 #end while 如果我们定义 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样取得新的数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$，一般称之为二分类规范增广数据集。在这个数据集下，线性可分意味着：存在一个 $\displaystyle \boldsymbol{w}$使得对于任意的 $\displaystyle \boldsymbol{z}_i$，$\displaystyle \boldsymbol{w}\boldsymbol{z}_i&gt;0$成立。 算法：误差修正学习算法2 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+2\eta_t\mathbb{I}(\boldsymbol{w}_t^\text{T} \boldsymbol{z}\leqslant 0)\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：梯度下降算法分析之前，令 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样我们把属于 $\displaystyle c_2$的特征变成了 $\displaystyle -\boldsymbol{x}$。定义目标函数： $\displaystyle J(\boldsymbol{w})=k \left(|\boldsymbol{w}^\text{T}\boldsymbol{z}|-\boldsymbol{w}^\text{T}\boldsymbol{z}\right),k&gt;0$ 通常令 $\displaystyle k=1$易知： $\displaystyle \min J(\boldsymbol{w})=0$时， $\displaystyle \boldsymbol{w}^\text{T}\boldsymbol{z}\geqslant 0$，于是分类正确。问题转化为：$$\begin{align}\min_{\boldsymbol{w}}J(\boldsymbol{w})\end{align}$$我们使用梯度下降算法：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_t\nabla_t\end{align}$$ 我们知道： $\displaystyle \nabla_t=\frac{\partial J}{\partial\boldsymbol{w}_t}=k[\boldsymbol{z}_t\times\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-\boldsymbol{z}_t]$ 代入3式得：$$\begin{align} \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t \end{align}$$当 $\displaystyle k=1$时，就是误差修正学习算法。 算法：梯度下降算法（Gradient Descent） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：最小均方误差算法取任意正数 $\displaystyle \nu_i$，有 $\displaystyle \boldsymbol{\nu}=[\nu_1,…,\nu_k]^\text{T}$。定义误差： $\displaystyle \boldsymbol{\epsilon}=\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}$ $$\begin{align} \mathrm{MES}(\boldsymbol{w}) =\boldsymbol{\epsilon}^\text{T}\boldsymbol{\epsilon} =\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right] \end{align}$$容易知道问题化为$$\begin{align}\min_{\boldsymbol{w}} \mathrm{MES}(\boldsymbol{w})\end{align}$$知道这就最小二乘解： $\displaystyle \hat{\boldsymbol{w}}=\left[\boldsymbol{X}^\text{T}\boldsymbol{X}\right]^{-1}\boldsymbol{X}^\text{T}\boldsymbol{\nu}=\boldsymbol{X}^\dagger \boldsymbol{\nu}$。求解伪逆的计算量大。我们使用梯度下降方法，知道$$\begin{align}\nabla\mathrm{MES}(\boldsymbol{w})=2\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]\end{align}$$ 算法：最小均方误差算法（Least Mean-Square Error） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{X}^\text{T}\boldsymbol{\nu},\boldsymbol{\nu}&gt;0$$2\displaystyle \alpha_0=\alpha$$3\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\alpha_t=\alpha\div t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-2\alpha_t\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}_t-\boldsymbol{\nu}\right]\\\end{array}\\$4 # end while$5\displaystyle \text{ if }\,\boldsymbol{\epsilon}_{end}=\boldsymbol{X}\boldsymbol{w}_{end}-\boldsymbol{\nu}\geqslant 0$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性可分’)}\\\end{array}$$\displaystyle \,\,\,\,\,\text{else }$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性不可分’)}\\\end{array}$6 # end if 感知器收敛定律如果样本集合线性可分，那么感知器存在收敛解。证明：1、我们使用规范增广数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n} $2、假定解向量 $\displaystyle \hat{\boldsymbol{w}}$，则对任意的 $\displaystyle \boldsymbol{z}$有 $\displaystyle \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}&gt;0$。我们注意到对于 $\displaystyle \delta&gt;0$ ，$\displaystyle \delta\cdot\hat{\boldsymbol{w}}$也是解向量。3、我们令 $\displaystyle \boldsymbol{z}_t$是所有错分样本， 有$$\begin{align}\boldsymbol{w}_t\boldsymbol{z}_t\leqslant 0\end{align}$$误差修正学习算法中令 $\displaystyle \eta_t=\frac{1}{2}$于是有：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\end{align}$$另外我们还令规范增广特征向量最大长度，与解向量最小内积$$\begin{align}\beta=\max_t ||\,\boldsymbol{z}_t||\end{align}$$$$\begin{align}\alpha=\min_t \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t&gt;0\end{align}$$4、现在我们把解向量 $\displaystyle \delta\hat{\boldsymbol{w}}$考虑进来于是：$$\begin{align}&amp;\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\\&amp;\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}=\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t\\&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t||^2\end{align}$$5、范数公式 $\displaystyle ||\boldsymbol{x}+\boldsymbol{y}||^2=||\boldsymbol{x}||^2+ 2&lt;\boldsymbol{x},\boldsymbol{y}&gt;+||\boldsymbol{y||^2},\,&lt;\boldsymbol{x},\boldsymbol{y}&gt;=\boldsymbol{x}^\text{T}\boldsymbol{y}$于是我们有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2+2 \boldsymbol{w}_t^\text{T}\boldsymbol{z}_t-2\delta\cdot\hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t+||\boldsymbol{z}_t||^2\end{align}$$6、根据上面的1、2、3的分析，有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-2\delta\cdot\alpha+\beta^2\end{align}$$7、为了简洁令： $\displaystyle \delta=\frac{\beta^2}{\alpha}$于是有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-\beta^2\end{align}$$8、考虑上式 $\displaystyle t=\{0,…,m\}$。并累加得：$$\begin{align}&amp;0\leqslant||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2-m\cdot\beta^2\end{align}$$9、可以看到随着 $\displaystyle m$的不断增加，范数 $\displaystyle ||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\to 0$，于是：$$\begin{align}m_{max}= \frac{||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2}{\beta^2}\end{align}$$10、如果我们令 $\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$,有：$$\begin{align}m_{max}=\frac{\delta^2}{\beta^2}||\,\hat{\boldsymbol{w}}||=\frac{\beta^2}{\alpha^2}||\,\hat{\boldsymbol{w}}||\end{align}$$这时感知机的解收敛于： $\displaystyle \frac{\beta^2}{\alpha}\cdot\hat{\boldsymbol{w}}$。史称感知机固定增量收敛定律。#### 评述1、 历史说明：分类是科学之始，为解决分类问题，人类殚精竭虑。其中模仿神经元的感知器就是其中之一。2、上帝给世界分了两类 $\displaystyle c_1$和 $\displaystyle c_2$。人类观测到了 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$。为了解决问题，人类思考了最简单的方法：劈下一刀，不就两半了。于是有了对称硬限函数。3、有个负号还是很麻烦，于是定义了 $\displaystyle \boldsymbol{z}=y\times \boldsymbol{x}$ 就有了新的数据集$\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$ 劈下一刀，变成了的折一下，然后裁一刀的问题：$$\begin{align} \forall \boldsymbol{z}_i,\boldsymbol{w}^\text{T}\boldsymbol{z}_i&gt;0\end{align}$$4、满足上式的问题，我们叫线性可分。并且根据学习规则和对问题认识，人类很块发现了感知机固定增量收敛定律。5、但是好景不长，人类的智者很快发现 线性不过是人类的YY，非线性才是上帝的YY。人类继续着征程： 我们的征途是星辰大海！ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习13/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>感知机</tag>
        <tag>人工神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[狄利克雷-多项式模型(Dirichlet-Multionmial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-08-%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 简介: 本文总结了狄利克雷-多项式模型的部分和我自己的一些体会，我们将再一次熟悉贝叶斯方法的基本概念、流程、特点。把我们思维进化到更高的维度。摘要：本文意在狄利克雷-多项式模型的问题。若有错误，请大家指正。关键词: 狄利克雷-多项式模型,分类分布,狄利克雷分布 一、狄利克雷-多项式模型(dirichlet-multionmial model)分类分布(Categorical distribution) 或者又叫1-C分布(multinoulli distribution) $$\begin{align}\bm{x}\sim \mathrm{Cat}(\bm{x}\mid\bm{\mu})\end{align}$$其中 $\displaystyle \bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\\\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1$ 在这里有必要解释一下符号问题。首先分类分布是对0-1分布的推广，这句话可能不好理解。我们可以这样思考，0-1分布是抛硬币。而分类分布类似于掷骰子。为了让符号统一我们使用小写 $\displaystyle c$表示有C个分类，例如 $\displaystyle c=6$可以类比于掷骰子。这样很多问题就好理解了。变量 $\displaystyle \bm{x}=\begin{bmatrix} x_1\\\ \vdots \\x_j\\\vdots\\x_c \end{bmatrix}$ 实例或者一个观测 $\displaystyle \bm{x}_i=\begin{bmatrix} x_{i1}\\\ \vdots \\x_{ij}\\\vdots\\x_{ic} \end{bmatrix}$ 例子： $\displaystyle \bm{x}_s=\begin{bmatrix} 0\ \\\vdots \\1\\\vdots\\0 \end{bmatrix} $ 1、分类分布(multinoulli distribution)概率质量函数：$$\begin{align}\mathrm{PMF：} \mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{i=1}^{c}\mu_i^{x_i}\end{align}$$ 其中： $\displaystyle \bm{x}=[x_1,x_2,…,x_c]^\text{T}\,,\bm{\mu}=[\mu_1,\mu_2,…,\mu_c]^\text{T}\,,\bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\,,\sum_{i=1}^{c}\mu_i=\bm{I}^\text{T}\bm{\mu}=1$这个表示方法也称为 $\displaystyle 1\,of\,c$编码方法。这个方法方便计算。 其他表示方法：$\displaystyle \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)},x\in\{1,…,c\}$,这个示性函数表示方法，有重要应用,降低表示维度，节约了有限的数学符号。 2、均值与方差我们知道向量函数微分结果： $\displaystyle \bm{f}(\bm{x})=\mathrm{e}^{\bm{A}\bm{x}} $ 有 $\displaystyle \frac{\partial\bm{f}}{\partial \bm{x}^\text{T}}=\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\bm{A}$。且有： $\displaystyle f(\bm{x})=\bm{\mu}^\text{T}\mathrm{e}^{\bm{A}\bm{x}}$，易得：$$\begin{align} &amp;\frac{\partial{f}}{\partial{\bm{x}}} =\mathrm{diag}\left[\mathrm{e}^{\bm{A}\bm{x}}\right]\bm{A}\bm{\mu}\\ &amp;\frac{\partial^2{f}}{\partial{\bm{x}}^2} =\bm{A}^\text{T}\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\mathrm{diag}[\bm{\mu}]\bm{A}\end{align}$$ 我们又知道特征函数： $\displaystyle \varphi_\bm{x}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{x}}]=\sum_{\bm{I}^\text{T}\bm{x}=1}p_j\mathrm{e}^{\mathrm{i}t x_j}=\sum_{j=1}^{c}\mu_i\mathrm{e}^{\mathrm{i}t x_j}=\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}$。于是可分析得到分类分布 $\displaystyle \bm{x}\sim \mathrm{Cat}(\bm{x}\mid \bm{\mu})$的期望与方差(协方差矩阵)： $\displaystyle \mathrm{E}[\bm{x}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}\right]\bm{X}\bm{\mu}=\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{x}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{x}]\mathrm{E}^\text{T}[\bm{x}]$$\displaystyle =\left.\bm{X}^\text{T}\mathrm{diag}[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}]\mathrm{diag}[\bm{\mu}]\bm{X}\right| _{\bm{t}=\bm{0}}-\bm{\mu}\bm{\mu}^\text{T}\\=\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}$ 其中： $\displaystyle \bm{X}=\bm{I}_{c\times c}$是 $\displaystyle c\times c$维的单位矩阵。也就说遍历了 $\displaystyle \bm{x}$所有可能取值组成的矩阵。为了表示简洁，我们在其中选用了$\displaystyle c\times c$维的单位矩阵。 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^{n}$于是有似然函数$$\begin{align}\mathrm{L}(\bm{\mu})=p(\mathcal{D}\mid\bm{\mu})&amp;=\prod_{i=1}^{n}\prod_{j=1}^{c}\mu_j^{x_j}=\prod_{j=1}^{c}\mu_j^{\sum_{i=1}^{n}x_{ij}}=\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中： $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$表示第 $\displaystyle j$个分类在 $\displaystyle n$次观测中发生了 $\displaystyle k_j$次。同时我们知道 $\displaystyle \sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1,\,\sum_{j=1}^{c}k_j=\bm{I}^\text{T}\bm{k}=n$ 4、对数似然函数$$\begin{align}\ell(\bm{\mu},\lambda)&amp;\propto\ln\prod_{j=1}^{c}\mu_j^{k_j}+\lambda(\sum_{j=1}^{c}\mu_j-1)\\&amp;=\bm{k}^\text{T}\ln\bm{\mu}+\lambda[\bm{I}^\text{T}\bm{\mu}-1]\end{align}$$ 5、求极大似然估计$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}&amp;=\frac{\bm{k}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}&amp;=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$分析可得 $\displaystyle \bm{\mu}_{MLE}=-\frac{\bm{k}}{\lambda} $ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}-1=0$得： $\displaystyle \frac{\bm{I}^\text{T}\bm{k}}{\lambda}=-1$由于 $\displaystyle \bm{I}^\text{T}\bm{k}=n$可得：$$\begin{align}\lambda=-n\end{align}$$ $$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 其中 $\displaystyle \mu_j^{MLE}=\frac{k_j}{n},\,k_j=\sum_{i=1}^{n}x_{ij}$。 6、多项式分布我们定义 $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$是分类 $\displaystyle j$在 $\displaystyle n$次观测中发生的次数。同时令 $\displaystyle \bm{k}=[k_1,…,k_j,…,k_c]^\text{T}$ 则有：$$\begin{align}\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)=\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中 $\displaystyle \sum_{j=1}^{c}k_j=n\,,\sum_{j=1}^{c}\mu_j=1$ 我们有特征函数：$\displaystyle \varphi_\bm{k}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{k}}]=\sum_{\bm{I}^\text{T}\bm{k}=n}p_i\mathrm{e}^{\mathrm{i}t k_j}=\sum_{\bm{I}^\text{T}\bm{k}=n}\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\left(\mu_j\mathrm{e}^{\mathrm{i}t}\right)^{k_j}=\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^n$。于是可得多项式分布的均值与协方差矩阵 $\displaystyle \mathrm{E}[\bm{k}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=-(\mathrm{i})^2n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}=n\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{k}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{k}]\mathrm{E}^\text{T}[\bm{k}]$$=\left.n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\mathrm{diag}[\bm{\mu}]\right| _{\bm{t}=\bm{0}}+\left.n(n-1)\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-2}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\left[\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\right]^\text{T}\right| _{\bm{t}=\bm{0}}-n^2\bm{\mu}\bm{\mu}^\text{T}$$=n\mathrm{diag}[\bm{\mu}]+n(n-1)\bm{\mu}\bm{\mu}^\text{T}-n^2\bm{\mu}\bm{\mu}^\text{T}\\=n\left[\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}\right]$ 7、共轭先验分布把 $\displaystyle \bm{\mu}$作为变量。观察似然函数，我们知道$$\begin{align}p(\bm{\mu})\propto p(\mathcal{D}\mid\bm{\mu})= \prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 我们知道狄利克雷分布 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\Gamma(a_0)}{\Gamma(a_1)\Gamma(a_2)…\Gamma(a_c)}\prod_{j=1}^{c}\mu_j^{a_j-1}$。而这正是我们需要的共轭先验，即后验与先验有相同的函数形式：$$\begin{align}\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\end{align}$$ 其中： $\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1\,,a_0=\bm{I}^\text{T}\bm{a}=a_1+…+a_c,\,a_j&gt;0$ 利用 $\displaystyle \Gamma,\,\mathrm{B}$函数性质容易知道：$\displaystyle \mathrm{E}(\bm{\mu})=\frac{\bm{a}}{\bm{I}^\text{T}\bm{a}}=\frac{\bm{a}}{a_0}$ $\displaystyle \mathrm{cov}(\bm{\mu})=\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\bm{a}-\bm{1}}{a_0-c}$ 为了方便使用，我们把它写成离散形式：$\displaystyle \mathrm{E}(\mu_j)=\frac{a_j}{a_0}$ $\displaystyle \mathrm{var}(\mu_j)=\frac{(a_0-a_j)a_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{cov}(\mu_i,\mu_j)=\frac{-a_ia_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\mu_j]=\mathop{\mathrm{argmax}}_{\mu_j}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{a_i-1}{a_0-c}$ 8、后验分布我们用似然函数乘以狄利克雷先验得到后验分布：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})\propto\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\end{align}$$ 归一化得：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})\end{align}$$ 1、在线学习我们发现狄利克雷分布也具有再生性质，和在线性学习性质，我们假设，陆续观测到两个的数据集 $\displaystyle \mathcal{D}_1\,\mathrm{D}_2$。1、当我们观察到 $\displaystyle \mathcal{D}_1$时，有$$ p(\bm{\mu}\mid\mathcal{D}_1)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_1+\bm{a}) $$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\bm{\mu}) p(\bm{\mu}\mid\mathcal{D}_1) $$易得： $$\begin{align}p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_2+\bm{k}_1+\bm{a})\end{align}$$ 2、后验分析最大后验估计$$\begin{align}\bm{\mu}_{MAP}=\mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}\end{align}$$ 极大似然估计：$$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 后验均值:$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}\end{align}$$ 后验均值是先验均值和最大似然估计的凸组合。知道 $\displaystyle \bm{\mu}=\frac{\bm{a}}{a_0}$$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}=\frac{a_0}{n+a_0}\bm{\mu}_0+\frac{n}{n+a_0}\bm{\mu}_{MLE}\end{align}$$我们发现 $\displaystyle a_0$可以理解为先验对于后验的等价样本大小。 $\displaystyle \frac{a_0}{n+a_0}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计，知道 $\displaystyle \mathrm{mode}[\bm{\mu}_0]=\frac{\bm{a}-\bm{1}}{a_0-c}$$$\begin{align}\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}=\frac{a_0-c}{n+a_0-c}\mathrm{mode}[\bm{\mu}_0]+\frac{n}{n+a_0-c}\bm{\mu}_{MLE}\end{align}$$ 3、 拉格朗日乘数法首先为了让符号有意义我们定义向量点除运算 $\displaystyle \frac{1}{\bm{a}}=1./\bm{a}=[1/a_1,…,1/a_n]^\text{T}$下面我们用拉格朗日乘数法在推理一遍，我们知道 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1 $，于是有：$$\begin{align}\ell(\bm{\mu},\lambda)=\ln \prod_{j}^{c}\mu_j^{k_j+a_j-1}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)=[\bm{k}+\bm{a}-\bm{1}]^\text{T}\ln\bm{\mu}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)\end{align}$$求解得：$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}=\frac{\bm{k}+\bm{a}-\bm{1}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$ 知道： $\displaystyle \bm{\mu}_{MAP}=-\frac{\bm{k}+\bm{a}-\bm{1}}{\lambda}$ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1$得： $\displaystyle \lambda=n+a_0-c$$$\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c} $$ 写成离散形式有$$\begin{align}\mu_j^{MAP}=\frac{k_j+a_j-1}{n+a_0-c}\end{align}$$ 4、后验协方差矩阵$$\begin{align}\mathrm{cov}(\bm{\mu})=\frac{(n+a_0)\mathrm{diag}[\bm{k}+\bm{a}]-[\bm{k}+\bm{a}][\bm{k}+\bm{a}]^\text{T}}{(n+a_0)^2(n+a_0+1)}\end{align}$$当 N足够大时： $$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降 5、后验预测分布开始分析之前，我们回忆一下B函数：$\displaystyle \mathrm{B}(\bm{a})=\int_{\bm{x}\in [0,1]^c}x_i^{a_i-1}\mathrm{d}\bm{x}\,,a_i&gt;0\,and\,\sum_{i-1}^{n}x_i=1$且有： $\displaystyle \mathrm{B}(\bm{a})=\frac{\prod_{i=1}^{n}\Gamma(a_1)\cdots \Gamma(a_n)}{\Gamma(a_0)}$。同时为了简化符号，我们令后验分布 $\displaystyle p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{a})$现在我们令下一次观测 $\displaystyle \bm{x}_{n+1}=\tilde{\bm{x}}$。我们现在想知道 $\displaystyle \tilde{\bm{x}}$各种情况下概率，以辅助决策。考虑到 $\displaystyle \tilde{\bm{x}}\in\{0,1\}^c \,,\bm{\mu}\in[0,1]^c=\mathcal{U}$。我们有：$$\begin{align}p(\tilde{\bm{x}}\mid \mathcal{D})&amp;=\int_{\mathcal{U}}p(\tilde{\bm{x}}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\mathrm{Cat}(\bm{x}\mid\bm{\mu})\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{\tilde{x}_j}\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{a_j+\tilde{x}_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma\left(\bm{I}^\text{T}[\bm{a}+\bm{x}]\right)}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma(a_0+1)}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}a_j^{\tilde{x}_j}\prod_{j=1}^{c}\Gamma(a_j)}{a_0^{\tilde{x}_j}\Gamma(a_0)}=\prod_{j=1}^{c}\frac{a_j}{a_0}^{\tilde{x}_j}=\prod_{j=1}^{c}\mathrm{E}^{\tilde{x}_j}(\bm{\mu}\mid\mathcal{D})\\&amp;=\mathrm{Cat}\left(\tilde{\bm{x}}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于 $\displaystyle plug-in\,\mathrm{E}[\mu\mid\mathcal{D}]$。我们也可以分析出类似贝塔-伯努利模型的结论。 6、多试验后验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么分类 $\displaystyle j$发生 $\displaystyle s_j$次的概率。写成向量形式 $\displaystyle \bm{s}$。于是有：$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\frac{m!}{s_1!…s_c!}\prod_{j=1}^{c}\mu_j^{s_j}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 我们称 $\displaystyle \mathrm{Dm}(\bm{s}\mid \mathcal{D},m)=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}$为狄利克雷-多项式分布(Dirichlet-multionmial distribution)。后验预测分布的均值与协方差矩阵问题 后验预测分布均值：$$\begin{align} \mathrm{E}[s_j\mid \mathcal{D},m] &amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\mathrm{E}(s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}=\int_{\mathcal{U}}m\mu_j\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=m\frac{\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+1)}=m\frac{a_j}{a_0}\end{align}$$ $\displaystyle \mathrm{E}[\bm{s}\mid \mathcal{D},m]=m\frac{\bm{a}}{a_0}$ 后验预测分布方差：$$\begin{align} \mathrm{var}[s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{E}(s_j^2)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}[m\mu_j+m(m-1)\mu_j^2]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{\Gamma(a_j+2)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{(a_j+1)a_j}{a_0(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=\frac{ma_j(a_0-a_j)}{a_0^2}\frac{a_0+m}{a_0+1}\end{align}$$ 后验预测分布协方差：$$\begin{align} \mathrm{cov}[s_i,s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{cov}(s_i,s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}[-m\mu_i\mu_j]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{\Gamma(a_i+1)\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_i)\Gamma(a_j)\Gamma(a_0+2)}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{a_ia_j}{(a_0+1)a_0}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=\frac{-ma_ia_j}{a_0^2}\frac{a_0+m}{a_0+1}\\\end{align}$$ 后验预测分布协方差矩阵：$\displaystyle \mathrm{cov}[\bm{s}\mid \mathcal{D},m]=m(m+a_0)\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ 对于 $plug−in\,\bm{\mu}_{MAP}$插值 $\displaystyle \mathrm{Mu}(\bm{s}\mid \bm{\mu}_{MAP},m)$，我们知道 $\displaystyle \bm{\mu}_{MAP}=\frac{\bm{a}-\bm{1}}{a_0-c}$其协方差矩阵为：$\displaystyle \mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]=m\left[\mathrm{diag}[\bm{\mu}_{MAP}]-\bm{\mu}_{MAP}\bm{\mu}_{MAP}^\text{T}\right]$ $\displaystyle \mathrm{var}[s_j\mid \bm{\mu}_{MAP},m]=\frac{m(a_j-1)(a_0-a_j+1-c)}{(a_0-c)^2}$ $\displaystyle \mathrm{cov}[s_i,s_j\mid \bm{\mu}_{MAP},m]=\frac{-m(a_i-1)(a_j-1)}{(a_0-1)^2}$比较大小容易证明：$$\begin{align}\mathrm{cov}[\bm{s}\mid \mathcal{D},m]\geqslant\mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]\end{align}$$所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论。 7、数据集后验预测与边缘似然函数有数据集 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，这有$$\begin{align} p(\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_t\mid\bm{\mu})p(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^t}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ $$\begin{align} p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid\mathcal{D}_t,\bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^{t+1}}\frac{1}{\mathrm{B}(\bm{k}_t+\bm{a})}\prod_{j=1}^{c}\mu_j^{k_j+a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{k}_t+\bm{a})}\end{align}$$ 所以有：$$\begin{align} p(\mathcal{D}_{t+1}\mid\mathcal{D}_t,\bm{a})=\frac{p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid\bm{a})}{ p(\mathcal{D}_t\mid\bm{a})}=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 9、评述通过狄利克雷-多项式模型，我们从离散二维拓展到了离散多维。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)}, x\in\{1,..,c\},\bm{\mu}\in[0,1]^c $ 这个分布由 $\displaystyle \bm{\mu}$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \bm{\mu}$。于是人类搞了个假设空间 $\displaystyle \mathcal{H}=\{\bm{\mu}_i\}$，为了找到上帝的那个 $\displaystyle \hat{\bm{\mu}}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。并且动用了自己的经验和感觉，人类假设 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}$。另外我们还可以结合，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}…\mathcal{D}_{\infty}$，来给假设空间的每个 $\displaystyle \bm{\mu}$更新概率。于是这个表达式横空出世 $\displaystyle p(\bm{\mu}\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid \mathcal{D}_{t})$。于是我们得到了： $$p(\bm{\mu}\mid\mathcal{D}_t)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_t+\bm{a})$$这样我们就把假设空间的 $\displaystyle \bm{\mu}$都给了个概率。这样我们就有关于 $\displaystyle \bm{\mu}$决策的信息。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$p(\tilde{x}\mid \mathcal{D})=\int_{\mathcal{U}}p(\tilde{x}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}=\mathrm{Cat}\left(\tilde{x}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)$$于是基于对假设空间再次赋概，我们对上帝有了新的认识 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{\bm{x}}$的次数是$\bm{s}$次的概率$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\frac{n!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$这样我们对 $\displaystyle \bm{s}$的可能值也赋概了。这个式子称之为狄利克雷-多项式分布。我们对上帝又有了新的认识 6、至此，由于 $\displaystyle p(\bm{\mu}\mid\mathcal{D}_t,\mathcal{D}_{t+1})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_{t+1}+\bm{k}_t+\bm{a})$。当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。因为$$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降，我们发现人类的认识 $\displaystyle \hat{\bm{\mu}}$会越来越逼近上帝的那个 $\displaystyle \bm{\mu}$概率。也就是说$$\begin{align}p(\hat{\bm{\mu}}\to \bm{\mu})\to1\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>分类分布</tag>
        <tag>狄利克雷-多项式模型</tag>
        <tag>狄利克雷分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝塔-伯努利模型(Beta-Binomial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-07-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝塔-伯努利模型(beta-binomial model)伯努利分布： $$\begin{align}x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1]\end{align}$$ 1、0-1分布(bernoulli distribution)概率质量函数可以表示为：$$\begin{align}\mathrm{PMF}：\mathrm{Ber}(x\mid \mu)=\mu^x(1-\mu)^{1-x}\end{align}$$其他表示方法：$\displaystyle \mathrm{Ber}(x\mid \mu)=\mu^{\mathbb{I}(x=1)}(1-\mu)^{\mathbb{I}(x=0)} $ $\displaystyle \mathrm{Ber}(x\mid \mu)=\begin{cases}\mu&amp;\text{if }x=1\\\nu=1-\mu&amp;\text{if }x=0\end{cases} $ 2、均值与方差：知道 $\displaystyle \varphi_x(t)=\mu\mathrm{e}^{\mathrm{i}t}+\nu$ $\displaystyle \mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\mu$ $\displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\mu(1-\mu)=\mu\nu$ 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$于是有似然函数 $$\begin{align}\mathrm{L}(\mu)=p(\mathcal{D}\mid\mu)=p(\boldsymbol{X}\mid\mu)=\prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 其中 $\displaystyle N_1=\sum_{i=1}^{n}\mathbb{I}(x_i=1)$， $\displaystyle N_0=\sum_{i=1}^{n}\mathbb{I}(x_i=0)$ 4、对数似然函数$$\begin{align}\mathcal{L}(\mu)&amp;=\ln p(\mathcal{D}\mid\mu)=\ln \prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\sum_{i=1}^{n}\left(x_i\ln\mu+(1-x_i)\ln(1-\mu)\right)\\&amp;=\boldsymbol{I}^\text{T}\boldsymbol{x}\ln\mu+\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})\ln(1-\mu)\end{align}$$ 5、求极大似然估计：$$ \frac{\partial{\mathcal{L}}}{\partial{\mu}}=\frac{1}{\mu}\boldsymbol{I}^\text{T}\boldsymbol{x}-\frac{1}{1-\mu}\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})=0\\\mu\boldsymbol{I}^\text{T}\boldsymbol{I}=\boldsymbol{I}^\text{T}\boldsymbol{x}$$$\begin{align}\mu_{MLE}=\frac{\boldsymbol{I}^\text{T}\boldsymbol{x}}{\boldsymbol{I}^\text{T}\boldsymbol{I}}=\frac{1}{n}\sum_{i=1}^{n}x_i=\bar{x}\end{align}$极大似然估计分析：我们知道 $\displaystyle x\in\{0,1\}$，在0-1分布的n次试验中, $\displaystyle x=1$的次数为 $\displaystyle k$。于是$$\begin{align}\mu_{MLE}=\frac{1}{n}\sum_{i=1}^{n}=\frac{k}{n}\end{align}$$现在我们假设抛硬币3次，3次都是正面朝上，那么 $\displaystyle n=k=3$，那么 $\displaystyle \mu_{MLE}=1$。这种情况下，极大似然的结果预测所有未来的观察值都是正面向上。常识告诉我们这是不合理的。事实上，这就是极大似然中过拟合现象的一个极端例子。下面我们引入 $\displaystyle \mu$的先验分布，我们会得到一个更合理的结论。 6、二项式分布(binomial distribution)我们现在把 $\displaystyle k$作为随机变量 $\displaystyle k=x_1+x_2+…+x_n$，于是我们有 $\displaystyle k\in\{N_1\}$，易知二项式分布和0-1分布的似然函数有相同的形式，是正比关系。最大化它们的 $\displaystyle \mu$都是 $\displaystyle \frac{k}{n}$。 $$\begin{align}\mathrm{L}(\mu)\propto\mathrm{Bin}(k\mid \mu,n)=\mathrm{C}_{n}^k\,\mu^k(1-\mu)^{n-k}\end{align}$$ 为了与后面符号衔接：我们令 $\displaystyle k=N_1,n=N,N_0=n-k$于是又有：$$\begin{align}p(\mathcal{D}\mid\mu)=\mathrm{L}(\mu)\propto\mathrm{Bin}(N_1\mid \mu,N_1+N_0)=\mathrm{C}_{N}^{N_1}\,\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 我们知道 $\displaystyle \mathrm{C}_{n}^{k}=\frac{n!}{(n-k)!k!}$。同时我们有离散随机变量特征函数$\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x_i}]=\sum_{i=1}^{\infty}p_i\mathrm{e}^{\mathrm{i}t x_i}$，注意虚数 $\displaystyle \mathrm{i}$和变量$\displaystyle i$的区别。我们令 $\displaystyle \nu=1-\mu$于是有：$$\begin{align}\varphi_k(t)=(\mu\mathrm{e}^{\mathrm{i}t}+\nu)^n\end{align} $$$\displaystyle \mathrm{E}[k]=\sum_{k=0}^{n}k\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=n\mu$$\displaystyle \mathrm{var}[k]=\sum_{k=0}^{n}(k-\mathrm{E}[k])^2\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[k]=n\mu(1-\mu)=n\mu\nu$ 7、共轭先验分布如果先验和似然函数有相同形式，那就非常方便：这样后验也有相同形式。这个时候我们称之为共轭先验(conjugate prior)。现在，我们把 $\displaystyle \mu$做为一个变量观察似然函数得到$$\begin{align}p(\mu)\propto\mu^{\rho_1}(1-\mu)^{\rho_2}\end{align}$$这样计算后验就很容易了： $\displaystyle p(\mu\mid\mathcal{D})\propto p(\mathcal{D}\mid \mu)p(\mu)=\mu^{N_1}(1-\mu)^{N_0}\mu^{\rho_1}(1-\mu)^{\rho_2}=\mu^{N_1+\rho_1}(1-\mu)^{N_0+\rho_2}$ 在给出先验分布之前，我们看看这个两个函数。之后我们会补充 $\displaystyle \Gamma(x),\mathrm{B}(a,b)$函数的性质的证明。$\displaystyle \Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u, x&gt;0$$\displaystyle \mathrm{B}(a,b)=\int_{0}^{1}x^{a-1}(1-x)^{b-1}\mathrm{d}x,a&gt;0,b&gt;0$$\displaystyle \Gamma(x+1)=x\Gamma(x)$$\displaystyle \Gamma(n+1)=n!$$\displaystyle \mathrm{B}(a,b)=\mathrm{B}(b,a)$$\displaystyle \mathrm{B}(a,b)=\frac{b-1}{a+b-1}\mathrm{B}(a,b-1),a&gt;0,b&gt;1$ $\displaystyle \mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ 然后我们知道贝塔分布： $\displaystyle \mathrm{Beta}(\mu\mid a,b)=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $。这正是我们要的共轭先验。 $$\begin{align}\mathrm{Beta}(\mu\mid a,b)=\frac{1}{\mathrm{B}(a,b)}\mu^{a-1}(1-\mu)^{b-1}\end{align}$$利用 $\displaystyle \Gamma$函数性质易知：$\displaystyle \mathrm{E}[\mu]=\frac{a}{a+b} $$\displaystyle \mathrm{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}$$\displaystyle \mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,\mathrm{Beta}(\mu\mid a,b)=\frac{a-1}{a+b-2},a&gt;0,b&gt;0$ 8、后验分布我们用似然函数乘以贝塔先验得到后验分布：$$\begin{align}p(\mu\mid\mathcal{D})\propto\mathrm{Bin}(N_1\mid\mu, N_1+N_0)\mathrm{Beta}(\mu \mid a,b)\end{align}$$ 归一化得：$$\begin{align}p(\mu\mid\mathcal{D})=\mathrm{Beta}(\mu\mid N_1+a,N_0+b)\end{align}$$ 1、在线学习容易证明 $\displaystyle \mathrm{Beta}$分布具有再生性质。于是我们可以假设我们有两个数据集 $\displaystyle \mathcal{D}_1,\mathcal{D}_2$。于是有：1、当我们观察到 $\displaystyle \mathrm{D}_1$时，有$$p(\mu\mid\mathcal{D}_1)=\mathrm{Beta}(\mu\mid N_1^1+a,N_0^1+b)$$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\mu) p(\mu\mid\mathcal{D}_1)$$易得：$$\begin{align}p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Beta}(\mu\mid N_1^1+N_1^2+a,N_0^1+N_0^2+b)\end{align}$$这表明 该贝叶斯推断具有在线学习的良好性质。 2、后验均值与众数最大后验估计：$$\begin{align}\mu_{MAP}=\mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,p(\mu\mid\mathcal{D})=\frac{N_1+a-1}{N+a+b-2}=\frac{k+a-1}{n+a+b-2}\end{align}$$极大似然估计： $$\begin{align} \mu_{MLE}=\frac{N_1}{N}=\frac{k}{n} \end{align}$$ 后验均值: $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{N_1+a}{N+a+b}=\frac{k+a}{n+a+b}\end{align}$$我们发现众数和均值不同。我们还发现 后验均值是先验均值和最大似然估计的凸组合。下面我们下证明这一点。 令先验均值$\displaystyle \frac{a}{a+b}=\mu_0,a+b=c$。 $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+a}{n+a+b}=\frac{c}{n+c}\mu_0+\frac{n}{n+c}\mu_{MLE}=\lambda\mu_0+(1-\lambda)\mu_{MLE}\end{align}$$ 我们发现 $\displaystyle c$可以理解为先验对于后验的等价样本大小。 $\displaystyle \lambda=\frac{c}{n+c}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计：$$\begin{align}\mu_{MAP}=\frac{c-2}{n+c-2}\mathrm{mode}[\mu_0]+\frac{n}{n+c-2}\mu_{MLE}=\eta\mathrm{mode}[\mu_0]+(1-\eta)\mu_{MLE}\end{align}$$ 3、后验方差$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]=\frac{(N_1+a)(N_0+b)}{(N_1+a+N_0+b)^2(N_1+a+N_0+b+1)}\end{align}$$当 $\displaystyle N$足够大时：$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]\approx\frac{N_1N_0}{NNN}=\frac{\mu_{MLE}(1-\mu_{MLE})}{n}\end{align}$$后验标准差 $$\begin{align}\sigma=\sqrt{\frac{\mu_{MLE}(1-\mu_{MLE})}{n}}\end{align}$$1、它随着我们数据的增加以 $\displaystyle \sqrt{\frac{1}{n}}$的速度下降。2、 $\displaystyle \mu=\mathop{\mathrm{argmax}}_{\mu}\sigma(\mu)=0.5$3、 $\displaystyle \mu=\mathop{\mathrm{argmin}}_{\mu}\sigma(\mu)=0\,or\,1$ 4、后验预测分布(posterior predictive distribution)上述，我们讨论了未知参数的推断，现在我们来讨论预测问题。在这之前：我们令后验分布 $\displaystyle p(\mu\mid\mathcal{D})=\mathrm{Beta}(a,b)$以简化符号 。考虑 $\displaystyle x_{n+1}$发生了，那么我们想预测 $\displaystyle x_{n+1}=0\,or\,1$的概率。为了简记，令 $\displaystyle \tilde{x}$为一随机变量，代表 $\displaystyle n$次观测后的值。同时考虑到 $\displaystyle \tilde{x}\in\{0,1\}$于是有：$$\begin{align}p(\tilde{x}\mid \mathcal{D})&amp;=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D})\mathrm{d}\mu\\&amp;=\int_0^1\mu^\tilde{x}(1-\mu)^{1-\tilde{x}}\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\int_0^1\mu^{a+\tilde{x}-1}(1-\mu)^{b+1-\tilde{x}-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\frac{\Gamma(a+\tilde{x})\Gamma(b+1-\tilde{x})}{\Gamma(a+b+1)}\\&amp;=\frac{a^{\tilde{x}}b^{1-\tilde{x}}}{(a+b)^{\tilde{x}}(a+b)^{1-\tilde{x}}}\\&amp;=\left(\frac{a}{a+b}\right)^{\tilde{x}}\left(\frac{b}{a+b}\right)^{1-\tilde{x}}\\&amp;=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于plug-in$\displaystyle \mathrm{E}[\mu\mid\mathcal{D}]$。这里的plug-in x是插值近似 plug-in approximation的意思。 5、过拟合与黑天鹅悖论如果我们使用 $\displaystyle plug-in\,\mu_{MLE}$：$$\begin{align}p(\tilde{x}\mid \mathcal{D})\approx\mathrm{Ber}\left(\tilde{x}\mid\mu_{MLE}\right)\end{align}$$1、当数据集较小，例如 $\displaystyle n=3,k=0,then,\mu_{MLE}=\frac{0}{3}=0$。这样我们预测 $\displaystyle \tilde{x}=0$的概率为 $\displaystyle 0$。这叫零数问题(zero count problem)，或者叫数据匮乏问题(sparse data problem)。这种问题在小样本情况，经常发生。当然有人就要说了，现在是大数据时代，干嘛关注这个问题？如果我们基于特定的目的，划分数据：例如特定的人做特别的事——个人购物推荐。这种情况下即使是在大数据时代，数据也不会很大。所以 $$即使在大数据当道的时代，贝叶斯方法依然是有用的！[Jordan2011]$$2、零数问题类似于黑天鹅事情。3、我们来用贝叶斯方法来解决这个问题： 使用均匀分布先验 $\displaystyle \mathrm{Beat}(1,1)=\mathrm{U}[0,1]$，plug-$\displaystyle in\,\mathrm{E}[\mu\mid\mathcal{D}]$给出了拉普拉斯继承规则(laplace’s rule of succession):$$\begin{align}p(\tilde{x}=1\mid\mathcal{D})=\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+1}{n+2}\end{align}$$可以理解为：在实践中，通常在数据集中加一应该是正确的做法。称之为加一平滑(add-one smoothing)4、注意到：plug-in$\displaystyle \mu_{MAP}$没有这个性质。 6、多试验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么发生 $\displaystyle \tilde{x}=1$的次数是 $\displaystyle s$次的概率。$$\begin{align}p(s\mid\mathcal{D},m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid a.b)\mathrm{d}\mu\\&amp;=\frac{\mathrm{C}_m^s}{\mathrm{B}(a.b)}\int_0^1\mu^{s+a-1}(1-\mu)^{m-s+b-1}\mathrm{d}\mu\\&amp;=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}\end{align}$$我们称 $\displaystyle \mathrm{Bb}(s\mid a,b,m)=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}$为贝塔-二项式分布(beta-binomial distribution)。易知(使用积分表达式,同时利用 $\displaystyle \mathrm{Bin}(s\mid \mu,m)$容易求得)：$\displaystyle \mathrm{E}[s\mid\mathcal{D},m]=m\frac{a}{a+b}$ $\displaystyle \mathrm{var}[s\mid\mathcal{D},m]=\frac{mab}{(a+b)^2}\frac{a+b+m}{a+b+1}$ $\displaystyle \mathrm{var}[s\mid\mu_{MAP}]=\frac{m(a-1)(b-1)}{(a+b-2)^2}$(插值求得的)于是有：$$\begin{align} \mathrm{var}[s\mid\mathcal{D},m]\geqslant\mathrm{var}[s\mid\mu_{MAP}]\end{align}$$至于证明，可以分析这个式子： $\displaystyle f(a,b)=ab \left( a+b-2 \right) ^{2} \left( a+b+m \right) - \left( a-1 \right) \left( b-1 \right) \left( a+b \right) ^{2} \left( a+b+1 \right) \geqslant 0,(a&gt;0,b&gt;0)$可以使用微积分一阶导数，二阶导数证明。所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论$$ 9、评述通过贝塔-伯努利模型，我们熟悉了贝叶斯方法的基本概念、流程、特点。一旦我们熟悉了伯努利、二项、贝塔、均匀、贝塔-伯努利分布后，很多关键的东西就可以用文字表述清楚了。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1] =\mu^x(1-\mu)^{1-x} $ 这个分布由 $\displaystyle \mu$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \mu$。于是人类搞了假设空间 $\displaystyle \mathcal{H}=\{\mu_i\}$，为了找到上帝的那个 $\displaystyle \hat{\mu}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。因为我们可以根据我们在某个时刻，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，来给假设空间的每个 $\displaystyle \mu$更新概率。于是这个表达式横空出世 $\displaystyle p(\mu\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\mu)p(\mu\mid \mathcal{D}_{t})$。于是我们得到了：$$p(\mu\mid\mathcal{D}_t)=\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b) $$这样我们就把假设空间的 $\displaystyle \mu$的都给了个概率。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$ p(\tilde{x}\mid \mathcal{D}_t)=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}_t]\right)$$于是基于对假设空间赋概，我们对未来的可能值也赋概。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{x}=1$的次数是$s$次的概率$$\begin{align}p(s\mid\mathcal{D}_t,m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu\\&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b)\mathrm{d}\mu\\&amp;=\mathrm{Bb}(s\mid N_1^t+a,N_0^t+b,m)\end{align}$$这样我们对 $\displaystyle s$的可能值也赋概了。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 6、至此，当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。 7、靠那个贝叶斯是谁，怎么总要出来嘟囔一下。决策论是啥？被冷落的贝叶斯微微一笑。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>神经网络</tag>
        <tag>大数据</tag>
        <tag>贝塔-伯努利模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯统计学概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝叶斯统计学框架经典统计学利用总计和样本信息来做统计分析，而贝叶斯统计学还加入了先验信息。下面我们用单参数一维随机变量加以说明： 1、记号以一维随机变量为例：频率学派中，依赖参数的概率密度(质量)函数表示为 $\displaystyle p_\beta(x) $或者 $\displaystyle p(x\,;\beta) $。表示在参数空间 $\displaystyle \mathcal{B}=\{\beta_i\} $中，不同 $\displaystyle \beta $对应不同密度概率(质量)函数。而在贝叶斯学派中，表示为 $\displaystyle p(x\mid \beta) $，代表了随机变量 $\displaystyle \beta $给定某个值时，总体 $x$的条件分布。而频率学派中不认为$\displaystyle \beta $是随机变量。它们认为上帝不玩骰子。 2、先验概率根据参数 $\displaystyle \beta $的先验信息确定先验分布$$\displaystyle p(\beta) $$ 3、样本的产生与似然函数贝叶斯观点认为：一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $产生要分两步：1、上帝从先验分布 $\displaystyle p(\beta) $中选了一个 $\displaystyle \beta_k $我们人类不知道，但是可以$设想$。2、从总体分布 $\displaystyle p(x\mid\beta_k) $产生一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $，这是具体的，我们人类能看到的。(按照频率学派观点，这里我们蕴含了 $\displaystyle x_i $是随机变量，且独立同分布的，贝叶斯认为这不需要，不过通常情况下，我们是使用IID，因为这样方便。)：$$\mathrm{L}(\beta_k)=p(\boldsymbol{x}\mid \beta_k)=\prod_{i=1}^{n}p(x_i\mid\beta_k) $$我们称 $\displaystyle \mathrm{L}(\beta_i) $为似然函数，它综合了总体信息和样本信息。 4、样本与参数的联合分布由于 $\displaystyle \beta_k $是上帝选的，我们人类$设想$的，它仍然是未知的，所以要把这个未知考虑进来，也是就 $\displaystyle \beta $的先验信息，对 $\displaystyle \beta $的一切可能加以考虑，而不仅仅是 $\displaystyle \beta_k $。这样我们人类就有了 $\displaystyle \boldsymbol{x} $和 $\displaystyle \beta $的联合分布：$$p(\boldsymbol{x},\beta)=p(\boldsymbol{x}\mid \beta)p(\beta) $$ 5、贝叶斯推断在没有样本信息时，人类只能根据先验分布 $\displaystyle p(\beta) $对 $\beta$做出推断。现在我们人类有了 $\displaystyle p(\boldsymbol{x},\beta) $，这样我们就可以做出新的推断了。1、先分解： $\displaystyle p(\boldsymbol{x},\beta)=p(\beta\mid\boldsymbol{x})p(\boldsymbol{x}) $。2、其中 $\displaystyle p(\boldsymbol{x})=\int_{\mathcal{B}}p(\boldsymbol{x},\beta)\mathrm{d}\beta=\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta$ ，它与$\beta$无关，或者说 $\displaystyle p(\boldsymbol{x}) $不含 $\displaystyle \beta $的任何信息。因此能用来推断的仅仅是条件分布：$$ p(\beta\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta)p(\beta)}{\displaystyle\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta} $$这就是贝叶斯公式的概率密度函数形式。 $\displaystyle p(\beta\mid\boldsymbol{x}) $史称后验分布，它集中了总体、样本、先验的一切信息，又排除了一切与 $\displaystyle \beta $无关的信息之后得到的结果。所以基于后验分布 $\displaystyle p(\beta\mid\boldsymbol{x}) $对 $\displaystyle \beta $进行统计推断是更为有效，也是最合理的。考虑离散情形：$$p(\beta_k\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta_k)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta_k)p(\beta_k)}{\displaystyle \sum_{\beta\in \mathcal{B}}p(\boldsymbol{x}\mid \beta)p(\beta)}$$ 自然语言表述的贝叶斯定理：$$\text{posterior}\propto\text{likeihood}\times\text{prior} $$或者说：$$p(\beta\mid\boldsymbol{x})\propto p(\boldsymbol{x}\mid \beta)p(\beta) $$ 6、贝叶斯统计分析的关键问题：1、确定先验分布： $\displaystyle p(\beta) $2、求联合分布： $\displaystyle p(\boldsymbol{x},\beta) $3、求后验分布： $\displaystyle p(\beta\mid \boldsymbol{x})$ 二、一个精彩的入门例子下面我们来通过一个入门的例子说明，贝叶斯定理是如何工作的。 [例子1.0] 为了提高相亲的成功率，小美考虑打扮一下自己，于是决定买一件羊绒大衣。预计要花费2000块。但是对相亲效果的影响，闺蜜们有2种意见： $\displaystyle \beta_1 $：相亲成功率提高到90%$\displaystyle \beta_2 $：相亲成功率提高到70% 小美当然希望 $\displaystyle \beta_1 $发生，有一个喜欢自己的男朋友，这笔花费还是值得的。根据一个好朋友的情况，先验概率：小美认为 $\displaystyle \beta_1 $的可信度只有40%， $\displaystyle \beta_2 $的可信度是60%。即： $$p(\beta_1)=0.4,\quad p(\beta_2)=0.6 $$小美不想花冤枉钱，于是她做了一个测试：把自己看中大衣，ps一下照片，给5个男性朋友看，结果：$$A: 5个男性朋友都认为小美更漂亮了 $$小美对测试很满意，于是她改变了看法，由二项分布知：$$ p(A\mid \beta_1)=0.9^5=0.590,\quad p(A\mid\beta_2)=0.7^5=0.168 $$由全概率公式 $\displaystyle p(A)=p(A\mid \beta_1)p(\beta_1)+p(A\mid \beta_2)p(\beta_2)=0.337 $。于是有后验概率 $$p(\beta_1\mid A)=\frac{p(A\mid \beta_1)p(\beta_1)}{p(A)}=0.7,\quad p(\beta_2)=\frac{p(A\mid \beta_2)p(\beta_2)}{p(A)}=0.3 $$这个概率综合了小美主观和实验的结果获得，要比小美之前认识的更有吸引力，更贴近实际。经过测试后，小美对买大衣有了兴趣，但是毕竟2000块还是很多的，于是小美再ps了一张图片，给她的男性朋友，结果如下：$$B: 10个男性朋友中，有9个都认为小美更漂亮了 $$$$ p(B\mid \beta_1)=C_{10}^{9}0.9^90.1=0.387,\quad p(B\mid\beta_2)=C_{10}^{9}0.7^90.3=0.121 $$由全概率公式 $\displaystyle p(B)=p(B\mid \beta_1)p(\beta_1)+p(B\mid \beta_2)p(\beta_2)=0.307 $。于是小美再次更新了自己的看法$$p(\beta_1\mid B)=\frac{p(B\mid \beta_1)p(\beta_1)}{p(B)}=0.883,\quad p(\beta_2)=\frac{p(B\mid \beta_2)p(\beta_2)}{p(B)}=0.117 $$小美经过两次测试，$\displaystyle \beta_1(相亲成功率提高到90\%) $的概率上升到了0.883，可以下决心买了。 三、共轭先验分布在叙述前，我们声明一下符号： $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X})=\frac{h(\boldsymbol{X},\boldsymbol{\beta})}{m(\boldsymbol{X})}=\frac{p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{\displaystyle\int_{\mathcal{B}}p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})\mathrm{d}\boldsymbol{\beta}} $ 1、共轭族定义设 $\displaystyle \boldsymbol{\beta} $是总体分布 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})$的参数向量， $\displaystyle \mathcal{F},\mathcal{P} $表示函数族。如果对任意的 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})\in\mathcal{F} $，存在先验分布函数$\displaystyle \pi(\boldsymbol{\beta})\in\mathcal{P} $，且 $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X}) \in \mathcal{P}$。就是说 $\displaystyle \mathcal{P} $是 $\displaystyle \mathcal{F} $的共轭族、称 $\displaystyle \pi(\boldsymbol{\beta}) $是共轭先验分布。 2、一维随机变量共轭先验分布例子1、方差已知下，一维高斯分布均值的先验分布是高斯分布。为了理解，我们先举一个简单的例子： $\displaystyle x\mid \mu\sim\mathcal{N}(x\mid \mu,\sigma) $，设 $\displaystyle \sigma $已知。有一组样本观测值 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $或者说有数据集 $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$。 我们现在开始分析：1、样本似然函数 $\displaystyle p(\mathcal{D}\mid\mu)=p(\boldsymbol{x}\mid\mu)=\prod_{i=1}^{n}p(x_i\mid\mu)=(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})\right]$2、取 $\displaystyle \mu\sim\mathcal{N}(\mu\mid \bar{\mu},\delta) $为先验分布，其中 $\displaystyle \bar{\mu},\delta $是已知的。接下来我们将看到它是共轭的。3、于是有联合分布：$\displaystyle p(\mathcal{D},\mu)=p(\mathcal{D}\mid \mu)p(\mu)=\frac{1}{(2\pi)^{\frac{n+1}{2}}\sigma^{-\frac{n}{2}}\delta^{-1}}\exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] $4、应用自然语言的贝叶斯定理，我们有后验分布：$$\begin{align}p(\mu\mid\mathcal{D})=\propto&amp; p(\boldsymbol{x}\mid \mu)p(\mu)\propto \exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] \\\propto&amp;\exp\left[-\frac{1}{2}\left((\frac{n}{\sigma^2}+\frac{1}{\delta^2})\mu^2-2(\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2})\mu+\frac{\boldsymbol{x}^\text{T}\boldsymbol{x}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2}\right)\right]\\\propto &amp;\exp\left[-\frac{1}{2}(A\mu^2-2B\mu+C)\right]\\\propto &amp;\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right]\end{align}$$ 其中 $\displaystyle A=\frac{n}{\sigma^2}+\frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2},B=\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2} =\frac{\bar{x}}{\bar{\sigma}^2}+\frac{\bar{\mu}}{\delta^2}$ 在这里我们一般把 $\displaystyle \exp\left[\boldsymbol{w}^\text{T}\boldsymbol{g}(\boldsymbol{x})\right] $称为正态分布的核。于是$$\displaystyle p(\mu\mid\mathcal{D})=p(\mu\mid\boldsymbol{x})=\frac{1}{(2\pi A^{-1})^{\frac{1}{2}}}\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right] $$也就是说后验分布是：$$\mu\mid\mathcal{D}\sim\mathcal{N}(\mu\mid \frac{B}{A},A^{-1}) $$ $\displaystyle \frac{B}{A}=\frac{\delta^2}{\bar{\sigma}^2+\delta^2}\bar{x}+\frac{\bar{\sigma}^2}{\bar{\sigma}^2+\delta^2}\bar{\mu}=\lambda\bar{x}+(1-\lambda)\bar{\mu} $$\displaystyle \frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2} $这就证明了：正态方差已知，它的均值的共轭先验分布是正态分布 3、若干技巧总结1、贝叶斯分析非常依赖于去求后验分布,如果按照定理，分母有一个积分，事实上它是一个数。于是我们经常应用$$\text{posterior}\propto\text{likelihood}\times\text{prior} $$这个式子分析，求得解后，在做归一化处理。就能得到posterior的表达式。2、在高斯分布下，我们经常需要配平方，以及观察随机变量的二次项(二次项的逆就是方差），一次项( 二次项的逆乘以一次项就是均值）。这是一个很重要的技巧。 四、充分统计量1、直观理解：就是不损失信息的统计量就是充分统计量。也就是说 $\displaystyle p_\beta(\boldsymbol{x}\mid T(\boldsymbol{x}))=p(\boldsymbol{x}\mid T(\boldsymbol{x})) $。 2、在这里我们只给出定理：设有样本 $\displaystyle \boldsymbol{x}=[x_1.x_2,…,x_n] $。样本密度 $\displaystyle p(x\mid \beta) $。有一个函数 $\displaystyle T: \boldsymbol{x}\mapsto \mathbb{R}$。 $\displaystyle t=T(\boldsymbol{x}) $它的密度为 $\displaystyle p(t\mid\beta) $。 $\displaystyle \mathcal{P}=\{\pi(\beta)\} $是$\beta$的某个先验分布族。如果对任意的 $\displaystyle \pi(\beta)\in\mathcal{P} $有 $$\displaystyle \pi(\beta\mid T(\boldsymbol{x}))=\pi(\beta\mid\boldsymbol{x}) $$这是 $\displaystyle T(\boldsymbol{x}) $是$\beta$的充分统计量的充要条件。 3、似然函数理解：$$\displaystyle \mathrm{L}(\beta)=p(\boldsymbol{x}\mid \beta)=h(\boldsymbol{x})g(T(\boldsymbol{x})\mid\beta)\propto g(T(\boldsymbol{x})\mid\beta) $$其中$h$与$\beta$无关，因此似然函数与$g(T(\boldsymbol{x})\mid\beta)$成比例，那么按照似然原理，有关$\beta$的推断可以有$T$给出。史称因子分解定理。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>神经网络</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵高斯分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-11-%E7%9F%A9%E9%98%B5%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Each day has enough trouble of its own.摘要：本文主要总结了矩阵高斯分布的若干基本问题，和我自己的一些体会。若有错误，请大家指正。关键词: 矩阵高斯分布,矩阵分布,统计学,概率论 一、标准矩阵高斯分布1、问题表述为了就研究数据集分布，我们将涉及：【矩阵分布问题】，当然矩阵分布是指的它所有元素的联合分布。 研究独立同分布的数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$的分布，我们将其写成数据矩阵： $\displaystyle \bm{X}=\big[\bm{x}_1,\bm{x}_2 \cdots \bm{x}_n\big]^\text{T}$。其中 $\displaystyle \bm{x}\in\mathbb{R}^k$且它的元素是相互独立的一元标准高斯分布： $\displaystyle x_i\sim\mathcal{N}(0,1)$。于是有： $$\begin{align}\mathrm{vec}\big(\bm{X}^\text{T}\big)\sim \mathcal{N}\big(\mathrm{vec}\big(\bm{0}_{n\times k}^\text{T}\big),\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 特别的我们用矩阵简洁的表示为： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$其中1、$\displaystyle \mathrm{vec}\big(\bm{X}_{n\times k}^\text{T}\big)=\big[\bm{x}_1^\text{T},\bm{x}_2^\text{T} \cdots \bm{x}_n^\text{T}\big]^\text{T}$，即 $\displaystyle \bm{X}$转置以后，按列拉成向量。 2、张量积 $\displaystyle \bm{A}\otimes\bm{B}=\big[a_{ij}\bm{B}\big]$。于是 $\displaystyle \bm{\varSigma}_{nk\times nk}=\bm{E}_n\otimes \bm{E}_k$ 3、 $\displaystyle \mathrm{tr}\big(\bm{A}^\text{T}\bm{B}\big)=\mathrm{vec}\big(\bm{A}\big)^\text{T}\mathrm{vec}\big(\bm{B}\big)$。于是 $\displaystyle \sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i=\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)=\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)$4、 $\displaystyle \mathrm{cov}\big[\mathrm{vec}\big(\bm{X}^\text{T}\big)\big]=\bm{E}_n\otimes \bm{E}_k=\bm{E}_{nk}$ 我们来简要说明一下：$$\begin{align}p\bigg(\mathrm{vec}\big(\bm{X}^\text{T}\big)\bigg)&amp;=p(\mathcal{D})=\prod_{i=1}^np(\bm{x}_i)=\prod_{i=1}^n\prod_{j=1}^kp(x_{ij})\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\bm{x}_1 ^\text{T}\bm{x}_1^\text{T}+\cdots+\bm{x}_n ^\text{T}\bm{x}_n^\text{T}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 到目前为止，遗留的问题是 $\displaystyle \bm{E}_n\otimes \bm{E}_k$这个参数做何理解。为何要写成克罗内克积的形式。 2、特征函数下面我们求上述矩阵分布的特征函数：我们定义：$\displaystyle \bm{T}=[\bm{t}_1,\bm{t}_2\cdots \bm{t}_n]^\text{T}$, 且知道 $\displaystyle \varphi_{\bm{x}_i}(\bm{t}_i)=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{E}_k\bm{t}_i\big]=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{t}_i\big]$。由独立随机变量联合分布特征函数等于这些随机变量的特征函数之积，知道$$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\varphi_{\mathrm{vec}\big(\bm{X}^\text{T}\big)}\big(\bm{T}\big)&amp;=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t})=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t}_i)\\&amp;=\exp\big[-\frac{1}{2}\big(\bm{t}_1 ^\text{T}\bm{t}_1+\bm{t}_2 ^\text{T}\bm{t}_2+\cdots+\bm{t}_n ^\text{T}\bm{t}_n\big)\big]\\&amp;=\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{T}\big)\big]\end{align}$$ 二、一般矩阵高斯分布1、分布形式现在我们开始考虑更一般的问题： $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}$，且 $\displaystyle \bm{W}=\bm{A}\bm{A}^\text{T}\,,\bm{V}=\bm{B}\bm{B}^\text{T}$，有： $$\begin{align}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\sim\mathcal{N}\big(\mathrm{vec}\big(\bm{M}^\text{T}\big),\bm{W}\otimes \bm{V}\big)\end{align}$$ $$\begin{align}\bm{Y}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\end{align}$$ 2、矩阵分布的特征函数下面，我们用特征函数来证明这一点：$$\begin{align}\varphi_{\bm{Y}}\big(\bm{T}\big)&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{vec}\big(\bm{T}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\big]\bigg]\\&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}\bm{Y}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{Y}\bm{T}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{Y}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{A}\bm{X}\bm{B}^\text{T}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{B}\bm{B}^\text{T}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big)\big]\\&amp;=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 也就是说矩阵分布： $\displaystyle \bm{X}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)$的特征函数是 $$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 3、一般矩阵高斯分布密度我们知道： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 由 $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}\to \bm{X}=\bm{A}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{B}^{-\text{T}}$、微分形式、变量代换定理有： $\displaystyle \frac{\partial \bm{X}}{\partial \bm{Y}}=\big|\,\bm{A}\,\big|^{-k}\big|\,\bm{B}^\text{T}\big|^{-n}=\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}$代入即可得到 $$\begin{align}p\big(\bm{Y}\big)=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\big)\right]\end{align}$$这样我们就得到了密度：$$\begin{align}\bm{X}&amp;\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\\&amp;=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\big)\right]\end{align}$$ 4、一般矩阵高斯分布性质1、$\displaystyle \bm{x}_{i,:}\sim \mathcal{N}(\bm{\mu}_i,w_{ii}\bm{V})$2、 $\displaystyle \bm{x}_{:,j}\sim\mathcal{N}(\bm{\mu}_j,v_{jj}\bm{W})$3、 $\displaystyle\mathrm{cov}[\bm{x}_{i,:},\bm{x}_{j,:}]=w_{ij}\bm{V}$4、 $\displaystyle\mathrm{cov}[\bm{x}_{:,i},\bm{x}_{:,j}]=v_{ij}\bm{W}$ 这个性质是显而易见的，然后如果你没发现“显然”，请仔细阅读上面的内容。要理解上述内容我们需要补充向量矩阵微分、微分形式、和变量代换定理。 三、评述充分熟悉矩阵微分、微分形式(外微分)、和变量代换定理是我们把握高维世界的基本工具。多加练习，容易掌握。矩阵微分大师：许宝騄。外微分大师：陈省生。可以读读他们的书。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学</tag>
        <tag>矩阵高斯分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布的熵]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-10-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E7%86%B5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、若干引理1、引理1.01、连续随机向量函数考虑一般情况，我们有随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$。现在有函数 $\displaystyle \bm{y}=\bm{g}(\bm{x}):\mathbb{R}^k\mapsto\mathbb{R}^d$。即有：$$\begin{align}\bm{y}=\bm{g}(\bm{x})\end{align}$$若上述方程有唯一解：$$\begin{align}\bm{x}=\bm{h}(\bm{y})\end{align}$$则称函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$是 $\displaystyle \bm{y}=\bm{g}(\bm{x})$的反函数。同时我们有雅可比行列式：$$\begin{align}\bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]\end{align}$$ 2、变量代换引理【定理1.0】对于连续随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$，函数 $\displaystyle \bm{y}=\bm{g}(\bm{x})$满足下列条件：1、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$有唯一反函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$2、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$和 $\displaystyle \bm{x}=\bm{h}(\bm{y})$连续3、 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]$存在而且连续那么$$\begin{align}f(\bm{y})=\left\{\begin{array}{l}f_\bm{x}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|&amp;\text{ 若 }\bm{y}\in G \\\ 0 &amp;\text{ 若 }\bm{y}\notin G \end{array}\right.\end{align}$$其中 $\displaystyle G=\{\bm{y}\mid \bm{y}=\bm{g(\bm{x})},\,\bm{x}\in \mathbb{R}^k\}$。 $\displaystyle \left|\,\bm{J}\right|$是雅可比行列式的绝对值，请勿与行列式符号混淆 证明： 为了区分随机变量与随机变量实例，我们定义 $\displaystyle \bm{\xi}$是随机向量，而 $\displaystyle \bm{x}$是随机向量$\displaystyle \bm{\xi}$的实例；$\displaystyle \bm{\eta}$是随机向量，而 $\displaystyle \bm{y}$是随机向量$\displaystyle \bm{\eta}$的实例当 $\displaystyle \bm{y}\notin G$时， 显然有 $\displaystyle f_\bm{\eta}(\bm{y})=0$。当 $\displaystyle \bm{y}\in G$时，有：$$\begin{align}F_ \bm{\eta}(\bm{y})=P(\bm{\eta}\leqslant \bm{y})=\int_A f_ \bm{\xi}(\bm{x})\mathrm{d}\bm{x}\end{align}$$其中： $\displaystyle A=\bigcap_{j=1}^{d}\{\bm{x}\mid g_j(\bm{x})\leqslant y_j\}$，在上式中换元： $\displaystyle \bm{x}=\bm{h}(\bm{y})$得：$$\begin{align}F_ \bm{\eta}(\bm{y})=\int_C\mathbb{I}_G(\bm{y})\times f_ \bm{\xi}\left[\bm{h}(\bm{y})\right]\cdot\left|\bm{J}\right|\mathrm{d}\bm{y}\end{align}$$其中 $\displaystyle C=\prod_{j=1}^{d}(-\infty,y_j]$， $\displaystyle \mathbb{I}_G(\bm{y})$是 $\displaystyle G$的示性函数。由此当 $\displaystyle \bm{y}\in G$时：$$\begin{align}f_ \bm{\eta}(\bm{y})=f_\bm{\xi}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|\end{align}$$证毕。其中证明中最关键的地方在于： $\displaystyle A\to C$的转变中，函数增减涉及积分方向的问题。这一问题的清晰说明较为繁琐，可以参考《数学分析原理》229页定理10.9以及微分形式的积分。 2、引理2.0定义方阵的幂(可以是分数) $\displaystyle \bm{A}^n=\bm{U}\bm{\Lambda}^n\bm{U}^\text{T}$。其中 $\displaystyle \bm{A}=\bm{U}\bm{\Lambda}\bm{U}^\text{T}$是约当分解或者叫谱分解，简单说就是对角化。 1、马哈拉诺比斯变换引理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{I}_k\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。当然我们也可以通过特征函数的方法对马哈拉诺比斯变换引理加以证明。 二、熵对于连续随机变量有： $\displaystyle \mathrm{H}[\bm{x}]=\mathrm{E}[\mathrm{I}(\bm{x})]=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}$下面我们推导多元高斯分布的熵：$$\begin{align}\mathrm{H}[\bm{x}]&amp;=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x})\ln \left[(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\right]\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x}) \left[\ln \left((2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\right)-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{x}) \left[(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{y})\times\bm{y}^\text{T}\bm{y}\mathrm{d}\bm{y}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\sum_{i=1}^k\mathrm{E}[y_i^2]\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{k}{2}\\&amp;=\ln \left[(2\pi\mathrm{e})^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right]\\&amp;=\frac{k}{2}\left(\ln2\pi+1\right)+\frac{1}{2}\ln\left|\bm{\varSigma}\right|\end{align}$$ 注意：推导中我们使用了马哈拉诺比斯变换引理。 三、评述1、在求解多元高斯分布的熵中，我们使用了变量代换，同时引用了马哈拉诺比斯变换引理。2、深层次的原理涉及到微分形式的积分。同时我们也可以浅层次的理解：使用特征函数导出马哈拉诺比斯变换引理3、好了我们不应止步，我们征途是星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-09-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 本文主要总结了多元高斯分布的若干基本问题，和我自己的一些体会。欢迎大家留言讨论，如有错误，请批评指正。 一、一元高斯分布$$\begin{align}x\sim\mathcal{N}(x\mid \mu,\sigma^2)=\left(2\pi\right)^{-1/2}(\sigma^2)^{-1/2}\exp\left[-\frac{1}{2}(x-\mu)^2\sigma^{-2}\right]\end{align}$$ 1、一元高斯分布特征函数我们有特征函数： $\displaystyle \varphi(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tx}p(x)\mathrm{d}x=\mathrm{e}^{i t\mu-\frac{1}{2}t^2\sigma^2}$。$$\begin{align}\varphi(t)=\exp \left[\mathrm{i} t\mu-\frac{1}{2}t^2\sigma^2\right]\end{align}$$下面我们来证明这一点： 令 $\displaystyle z=\frac {x-\mu}{\sigma}$,于是有 $\displaystyle z\sim\mathcal{N}\left(z\mid 0,1\right)=\left(2\pi\right)^{-1/2}\exp \left[-\frac{1}{2}z^2\right]$：$$\begin{align}\varphi_z(t)&amp;=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\end{align}$$ 1、我们知道虚数 $\displaystyle \mathbb{z}=\mathrm{e}^{\mathrm{i}\theta}=\cos(\theta)+\mathrm{i}\sin(\theta)$, 虚数的模 $\displaystyle \left| \mathbb{z}\right|=1$。令 $\displaystyle A(t)=\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}$于是有：$$\left|\frac{\partial{A}}{\partial{t}}\right|=\left|\mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right|=\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2} $$而且有：$$\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \left|\frac{\partial{A}}{\partial{t}}\right|\mathrm{d}z=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z&lt;\infty$$由于 $\displaystyle \int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z$收敛：故由含参反常积分一致收敛的可微性质知函数 $\displaystyle \left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \frac{\partial{A}}{\partial{t}}\mathrm{d}z$关于 $\displaystyle t\in (-\infty,+\infty)$上一致收敛。所以我们可以在 $\displaystyle \varphi_z(t)$ 的积分号下求导(交换积分与求导顺序) ：$$\varphi’_z(t)=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\mathrm{d}z $$2、现在对上式进行分布积分： $\displaystyle\begin{cases} u=\mathrm{e}^{\mathrm{i}tz}\\v=-\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $，同时 $\displaystyle \begin{cases} u’=\mathrm{i}t\mathrm{e}^{\mathrm{i}tz}\\v’=z\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $ 于是有：$$\begin{align}\varphi’_z(t)&amp;=i\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} u\mathrm{d}v\\&amp;= \left.-i\left(2\pi\right)^{-1/2}\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right| _{x=0}-t\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\\&amp;=-t\varphi_z(t)\end{align}$$ 得到微分方程：$$\begin{cases}\varphi’_z(t)+t\varphi_z(t)=0\\\varphi_z(0)=1\end{cases}$$解得：$$\begin{align}\varphi_z(t)=\mathrm{e}^{-\frac{1}{2}t^2}\end{align}$$又因为： $\displaystyle x=\mu+\sigma z$$$\begin{align}\varphi_x(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\mu+\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\varphi_z(\sigma t)=\mathrm{e}^{\mathrm{i}t\mu-\frac{1}{2}t^2\sigma^2}\end{align}$$ 二、多元高斯分布1、多元高斯分布我们知道一维的特征函数为：$$\begin{align}\varphi(t)=\exp \left[\mathrm{i}t\mu -\frac{1}{2}t^2\sigma^2\right]\end{align}$$ 多元情况： $$\begin{align}\bm{x}\sim\mathcal{N}(\bm{x}\mid\bm{\mu},\bm{\varSigma})=(2\pi)^{-k/2}\left|\bm{\varSigma}\right|^{-1/2}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$ 2、马哈拉诺比斯变换定理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{E}_p\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。 独立随机变量联合分布特征函数等于这些随机变量的特征函数之积。于是我们有$$\begin{align}\varphi_y(t)=\exp \left[-\frac{1}{2}t^2\right]\to\varphi_{\bm{y}}(\bm{t})=\prod_{i=1}^p\exp \left[-\frac{1}{2}t_i^2\right]=\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{t}\right]\end{align}$$ 3、多元高斯分布特征函数接着我们使用特征函数的线性变换性质有：$$\begin{align}\varphi_{\bm{x}}(\bm{t})&amp;=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{x}\right]\bigg]=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\left(\bm{\varSigma}^{1/2}\bm{y}+\bm{\mu}\right)\right]\bigg]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\varphi_{\bm{y}}\bigg(\big[\bm{\varSigma}^{1/2}\big]^\text{T}\bm{t}\bigg)=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}^{1/2}\bm{\varSigma}^{1/2}\bm{t}\right]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]\end{align}$$ 三、多元高斯分布的性质1、高斯随机向量的任意边缘依然是高斯分布有随机向量 $\displaystyle \bm{x}$是 $\displaystyle k$维的，且 $\displaystyle \bm{x}\sim \mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$ 现在我们从 $\displaystyle \{x_i\}_{i=1}^k$中任意选取 $\displaystyle p$个元素，令 $\displaystyle s:p\mapsto k$，则向量 $\displaystyle \tilde{\bm{x}}=[x_{s_1},x_{s_2}\cdots x_{s_p}]^\text{T}$仍然是高斯分布：$$\begin{align}\tilde{\bm{x}}\sim\mathcal{N}\big(\tilde{\bm{\mu}},\tilde{\bm{\varSigma}}\big)\end{align}$$其中： $\displaystyle \tilde{\bm{\mu}}=[\mu_{s_1},\mu_{s_2}\cdots \mu_{s_p}]^\text{T}$， $\displaystyle \tilde{\bm{\varSigma}}=[c_{ij}],i,j\in\{s\mid s{p}\}$即保留 $\displaystyle \bm{\varSigma}$的第 $\displaystyle s_1,s_2\cdots s_p$行和列的 $\displaystyle p$阶矩阵。 证明：我们有 $\displaystyle \bm{x}$的特征函数 $\displaystyle \varphi_{\bm{x}}(\bm{t})=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]$,我们令 $\displaystyle t_i=0,i\in \{s\mid k \lnot s(p)\}$有：$$\begin{align}\varphi_{\tilde{\bm{x}}}(\tilde{\bm{t}})=\exp \left[\mathrm{i}\tilde{\bm{t}}^\text{T}\tilde{\bm{\mu}}-\frac{1}{2}\tilde{\bm{t}}^\text{T}\tilde{\bm{\varSigma}}\tilde{\bm{t}}\right]\end{align}$$故而得证。 2、独立性与相关性等价有$\displaystyle \bm{x}_1\sim \mathcal{N}\big(\bm{\mu}_1,\bm{\varSigma}_1\big)$和 $\displaystyle \bm{x}_2\sim \mathcal{N}\big(\bm{\mu}_2,\bm{\varSigma}_2\big)$，我们有$\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0$，且有$\displaystyle \bm{x}=[\bm{x}_1,\bm{x}_2]^\text{T}$服从： $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu}, \bm{\varSigma}\big)$。其中： $\displaystyle \bm{\mu}=[\bm{\mu}_1,\bm{\mu}_2]^\text{T},\bm{\varSigma}=\begin{bmatrix} \bm{\varSigma}_1 &amp; \bm{0}\\\bm{0}&amp;\bm{\varSigma}_2 \end{bmatrix}$。 证明利用1、特征函数的唯一性定理2、独立随机变量联合分布的特征函数是它们特征函数之积。证明是显然的。$$\begin{align}\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0\end{align}$$ 3、仿射(线性)变换不变性有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$， 仿射变换 $\displaystyle \bm{y}=\bm{A}\bm{x}+\bm{b}$，则 $\displaystyle \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big) $ 证明：由特征函数的仿射变换性质有：$$\begin{align}\varphi_{\bm{y}}\big(\bm{t}\big)&amp;=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}\big]\varphi_{\bm{x}}\big(\bm{A}’\bm{t}\big)=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}+\mathrm{i}\big(\bm{A}^\text{T}\bm{t}\big)^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\big]\\&amp;=\exp\bigg[\mathrm{i}\bm{t}^\text{T}\big[\bm{A}\bm{\mu}+\bm{b}\big]–\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\bigg]\end{align}$$又由特征函数唯一性定理知： $$\begin{align} \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big)\end{align}$$【推论1】：分解：多元高斯分布随机向量都可以经过仿射变换为独立随机变量，且它们是标准高斯随机变量。【推论2】：降维：随机向量的线性组合是高斯分布 $\displaystyle \iff$随机向量服从高斯分布 推论2的性质颇为惊奇，我们来推导一下，以窥细节：证明：【充分性】若有一维高斯分布 ：$$\begin{align}y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)\end{align}$$知其特征函数为：$$\begin{align}\varphi_y(t)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$现在对其观察角度加以变换：令 $\displaystyle t=1$同时把 $\displaystyle \bm{a}$看成任意有：$$\begin{align}\varphi_{\bm{x}}\big(\bm{a}\big)=\mathrm{E}\big[\mathrm{i}\bm{a}^\text{T}\bm{x}\big]=\mathrm{E}\big[\mathrm{i}y\big]=\varphi_y\big(1\big)=\exp\big[\mathrm{i}\bm{a}^\text{T}\bm{x}-\frac{1}{2}\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$【必要性】若有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$，则 $\displaystyle y=\bm{a}^\text{T}\bm{x}$的特征函数为：$$\begin{align}\varphi_y(t)=\varphi_{\bm{x}}\big(t \bm{a}\big)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)$证毕 4、条件分布我们考虑如下分布 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_2\mid \bm{x}_1$依然是高斯分布： $$\begin{align} \bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 证明：1、为了利用独立性与相关性等价的结论，我们使用线性变换来构造两个独立的新随机变量： $$\begin{align}\bm{y}_1&amp;=\bm{x}_1\\\bm{y}_2&amp;=\bm{T}\bm{x}_1+\bm{x}_2\end{align}$$ 欲使 $\displaystyle \bm{y}_1\bot \bm{y}_2$，则：$$\begin{align}\mathrm{cov}\big[\bm{y}_1,\bm{y_2}\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_1- \mathrm{E}\big[\bm{y}_1\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\bm{x}_1+\bm{x}_2- \bm{T}\bm{\mu}_1- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)+\bm{x}_2- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_1- \bm{\mu}_1\big)^\text{T}\Big]\bm{T}^\text{T}+\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_2- \bm{\mu}_2\big)^\text{T}\Big]\\&amp;=\bm{\varSigma}_{11}\bm{T}^\text{T}+\bm{\varSigma}_{12}=\bm{0}\end{align}$$于是有：$$\begin{align}\bm{T}=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\end{align}$$也就是说：$$\begin{align}\bm{y}_2=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1+\bm{x}_2\end{align}$$这个线性变换是：$$\begin{align}\begin{bmatrix} \bm{y}_1 \\\ \bm{y}_2 \end{bmatrix}=\begin{bmatrix} \bm{I} &amp; \bm{0}\\ -\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}&amp;\bm{I}\end{bmatrix}\begin{bmatrix} \bm{x}_1 \\\\\bm{x}_2\end{bmatrix}\end{align}$$2、根据数字特征的性质，我们求出 $\displaystyle \bm{y}_1,\bm{y}_2$的期望和方差，由高斯分布的性质知道经过线性变换后的它们也是服从高斯分布的，从而我们可以确定其分布。容易知道：$\displaystyle \mathrm{E}\big[\bm{y}_1\big]=\bm{\mu}_2$，$\displaystyle \mathrm{cov}\big[\bm{y}_1\big]=\bm{\varSigma}_{11}$，$\displaystyle \mathrm{E}\big[\bm{y}_2\big]=\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1$。下面关键是求：$$\begin{align} \mathrm{cov}\big[\bm{y}_2\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)^\text{T}\Bigg]\\&amp;=\bm{\varSigma}_{22}+\bm{\varSigma}_{21}\bm{T}^\text{T}+\bm{T}\bm{\varSigma}_{12}+\bm{T}\bm{\varSigma}_{11}\bm{T}^\text{T}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{11}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$ 于是有：$$\begin{align}\bm{y}_2\sim\mathcal{N}\bigg(\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1,\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 3、并且我们有： $\displaystyle \bm{J}\big(\bm{y}\to \bm{x}\big)=\bigg|\frac{\partial \bm{y}}{\partial \bm{x}^\text{T}}\bigg|=1$，再有 $\displaystyle \bm{y}_1,\bm{y}_2$独立，于是：$$\begin{align}p\big(\bm{x}_1,\bm{x}_2\big)=p\big(\bm{y}_1,\bm{y}_2\big)\bm{J}\big(\bm{y}\to \bm{x}\big)=p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)\end{align}$$ 现在我们可以求得 $\displaystyle \bm{x}_2\mid \bm{x}_1$的分布：$$\begin{align}p\big(\bm{x}_2\mid \bm{x}_1\big)=\frac{p\big(\bm{x}_1,\bm{x}_2\big)}{p\big(\bm{x}_1\big)}=\frac{p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)}{p\big(\bm{y}_1\big)}=p\big(\bm{y}_2\big)=p_{\bm{y}_2}\big(\bm{x}_2- \bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1\big)\end{align}$$代入到密度函数，经过简单变换有：$$\begin{align}\bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$条件数学期望和协方差矩阵是：$$\begin{align}\bm{\mu}_{2\mid1}&amp;=\mathrm{E}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big]\\\bm{\varSigma}_{2\mid 1}&amp;=\mathrm{cov}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$证毕 当然我们也可以通过概率归一约束，和配平方加以推导。 5、高斯线性模型若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，则有：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}^{-1}=\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}$ $\displaystyle \bm{\mu}_{\bm{x}\mid \bm{y}}=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 证明：要想彻底说明这个问题，我们需要分块矩阵的若干引理：首先我们来证明一个引理 1、【引理1】单位矩阵引理：$$\begin{align} [\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$证明：$$\begin{align}&amp;[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E} \\&amp;\iff\bm{S}[\bm{E}-\bm{S}]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\big[\bm{S}^{-1}-\bm{S}\bm{S}^{-1}\big]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\bm{S}^{-1}[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}^{-1}\bm{S}\\&amp;\iff[\bm{S}^{-1}-\bm{E}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\\&amp;\iff\bm{E}=\bm{E}\end{align}$$证毕 2、【引理2】分块矩阵的逆证明：若分块矩阵的逆矩阵存在，：$$\begin{align}\bm{H}^{-1}=\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}\end{align}$$ 1、 【$\displaystyle \bm{A}$若可以逆】首先我们左乘一个矩阵，消除 $\displaystyle \bm{B} $ $$\begin{align} \left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{C} &amp; \bm{D}\end{array}\right]\end{align}$$ 第二步我们右乘一个矩阵，消除 $\displaystyle \bm{C} $ $$\begin{align}\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E}\end{bmatrix}= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{0} &amp; \bm{D}\end{array}\right]\end{align}$$ 简写$\displaystyle \bm{U}\bm{H}\bm{V}=\bm{W} $，于是 $\displaystyle\bm{H}^{-1}=\bm{V}\bm{W}^{-1}\bm{U} $（这是显然的） $$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}=\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E} \end{bmatrix}\left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} &amp; \bm{0} \\\bm{0} &amp; \bm{D}^{-1}\end{array}\right]\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\end{align}$$简化得：$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp;-[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}\bm{B}\bm{D}^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{M}&amp;- \bm{M} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}\bm{M}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}\bm{M}\bm{B}\bm{D}^{-1}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}$。 2、【同理若 $\displaystyle \bm{B}$可逆】$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1}&amp;-\bm{B}\bm{D}^{-1}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\\-[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}^{-1}\bm{C}&amp; [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{A}^{-1}+\bm{A}^{-1}\bm{B}\bm{M}\bm{C}\bm{A}^{-1}&amp;- \bm{B}\bm{D}^{-1}\bm{M}\\-\bm{M} \bm{D}^{-1}\bm{C}&amp;\bm{M}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}= [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}$ 3、【引理3】分块矩阵行列式：由引理2，消除 $\displaystyle \bm{B}$后的结论知道：$$\begin{align}\det \begin{bmatrix} \bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{bmatrix}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det \begin{bmatrix} \bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]\det[\bm{D}]\end{align}$$ 我们也可以这样（也就是说右乘矩阵消除 $\displaystyle \bm{B}$）： $$\begin{align}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}\det \begin{bmatrix} \bm{E} &amp; -\bm{A}^{-1}\bm{B} \\\bm{0} &amp; \bm{E}\end{bmatrix}=\det \begin{bmatrix} \bm{A} &amp; \bm{0}\\\bm{C}&amp;\bm{D}-\bm{C}\bm{A}^{-1}\bm{B} \end{bmatrix}=\det[\bm{A}]\det[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]\end{align}$$ 也就是说：$$\begin{align}\begin{vmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D}\end{vmatrix}=\big|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}\big|\big|\bm{D}\big|=\big|\bm{A}\big|\big|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\big|\end{align}$$ 我们也有$$\begin{align} \left|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} \right|=\left|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\right|\left|\bm{D}^{-1}\right|\big|\bm{A}\big|\end{align}$$ 4、【引理4】维度变换$\displaystyle [\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}$ $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} =\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1} $ $\displaystyle [\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}=\bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1} $证明：我们挑一个做说明,为了应用【引理1】我们右乘 $\displaystyle \bm{A}$于是有：$$\begin{align} &amp;[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\\ &amp;\iff[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}\bm{A}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\bm{A}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-\bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-[\bm{D}^{-1}\bm{C}^{-1}\bm{B}^{-1}\bm{A}+\bm{E}]^{-1}\\ &amp;\iff\big[\bm{E}-[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]\big]^{-1}-\big[[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}-\bm{E}\big]=\bm{E}\,,\bm{S}=-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}\\ &amp;\iff[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$当然我们也可以直接对比引理2的两个结论得出。证毕 5、【引理5】高斯线性回归模型的联合分布我们知道 $\displaystyle p(\bm{x},\bm{y})=p(\bm{x})p(\bm{y}\mid \bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，现在令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 我们取对数：$$\begin{align}&amp;\ln p(\bm{z})=\ln p(\bm{x})+\ln p(\bm{y}\mid \bm{x})\\&amp;=-\frac{1}{2}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)^\mathrm{T}\bm{\varSigma}_{\bm{x}}^{-1}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)-\frac{1}{2}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)^\mathrm{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)+\\&amp;\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1} \end{bmatrix}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}^{-1}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=\ln\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中我们使用了一些二次型的技巧：简要说明一下，以免显得突兀,其中 $\displaystyle \bm{S}$是对称矩阵,有二次型：$$\begin{align}Q&amp;=\frac{1}{2}(\bm{x}-\bm{\mu})^\text{T}\bm{S}(\bm{x}-\bm{\mu})=\frac{1}{2}\bm{x}^\text{T}\bm{S}\bm{x}-\bm{x}^\text{T}\bm{S}\bm{\mu}+\frac{1}{2}\bm{\mu}^\text{T}\bm{S}\bm{\mu}\\&amp;=\frac{1}{2}\bm{x}^\text{T}\bm{A}\bm{x}-\bm{x}^\text{T}\bm{B}+\frac{1}{2}\bm{\mu}^\text{T}\bm{A}\bm{\mu}\end{align}$$其中：$$\begin{align}\bm{S}&amp;=\bm{A}\\\bm{\mu}&amp;=\bm{A}^{-1}\bm{B}\end{align}$$也就是说我们可以通过观测一次项、二次项的系数来求得参数。 考察二次项：$$\begin{align}&amp;-\frac{1}{2}\bm{x}^\text{T}[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}]\bm{x}-\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}+\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}\bm{x}+\frac{1}{2}\bm{x}^\text{T}\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}\\&amp;=-\frac{1}{2}\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{cc}\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\end{array}\right]\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]\end{align}$$考虑一次项$$\begin{align}\bm{x}^\text{T}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}=\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{c}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}-\bm{A}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\\\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\end{array}\right]\end{align}$$这样再通过分块矩阵求逆，和二次型的特点可以求得：$$\begin{align}\bm{\mu}_{\bm{x}}&amp;=\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\\\bm{\varSigma}_{\bm{z}}&amp;=\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\end{align}$$同时我们通过分块矩阵行列式注意到：$$\begin{align}\big|\bm{\varSigma}_{\bm{z}}\big|=\big|\bm{\varSigma}_{\bm{x}}\big|\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|\end{align}$$以上就是联合分布是高斯分布的推导细节。 【高斯线性回归模型的联合分布】若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 则：$$\begin{align}\bm{z}\sim\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中：$\displaystyle \bm{\varLambda}_{\bm{z}}=\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1} \end{bmatrix}$###### 6、【引理6】精度矩阵与协方差矩阵再看出显然之前我们有必要叙述一精度矩阵的概念: 精度矩阵是协方差矩阵的逆$$\begin{align}\bm{\varLambda}=\bm{\varSigma}^{-1}=\begin{bmatrix} \bm{\varLambda}_{11} &amp; \bm{\varLambda}_{12} \\\bm{\varLambda}_{21}&amp;\bm{\varLambda}_{22}\end{bmatrix}\end{align}$$若 $\displaystyle \bm{\varSigma}_{11},\bm{\varSigma}_{22}$可以逆，根据上述引理我们容易知道：$\displaystyle \bm{\varLambda}_{11}=\big[\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\big]^{-1}$$\displaystyle \bm{\varLambda}_{22}=\big[\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\big]^{-1}$$\displaystyle \bm{\varLambda}_{12}=-\bm{\varLambda}_{11}\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}$$\displaystyle \bm{\varLambda}_{21}=-\bm{\varLambda}_{22}\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}$有了精度矩阵，我们可以重写条件分布定理： 若 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_1\mid \bm{x}_2$依然是高斯分布： $$\begin{align} \bm{x}_1\mid \bm{x}_2\sim\mathcal{N}\bigg(\bm{\mu}_1+\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big],\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\bigg)\end{align}$$且有$$\begin{align}\bm{\mu}_{1\mid2}&amp;=\bm{\mu}_1+\bm{\varSigma}_{11}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\mu}_1- \bm{\varLambda}_{11}^{-1}\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\varLambda}_{11}^{-1}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\\&amp;=\bm{\varSigma}_{1\mid 2}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\end{align}$$ $$\begin{align}\bm{\varSigma}_{1\mid 2}=\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}=\bm{\varLambda}_{11}^{-1}\end{align}$$ 7、最后的战斗有了联合分布我们就可以用应用条件分布定理了：结论是显然的：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}=\big[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}\big]^{-1}=\bm{\varLambda}_{xx}^{-1}$ $$\begin{align}\bm{\mu}_{\bm{x}\mid \bm{y}}&amp;=\bm{\varSigma}_{\bm{x}\mid\bm{y}}\bigg[\bm{\varLambda}_{\bm{x}\bm{x}}\bm{\mu}_{\bm{x}}-\bm{\varLambda}_{\bm{x}\bm{y}}\big[\bm{y}-\bm{A}\bm{\mu}_{x}-\bm{b}\big]\bigg]\\&amp;=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]\end{align}$$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 四、评述1、我们来总结一下：$$\begin{align}\begin{cases}\text{马哈拉诺比斯变换定理}\\\text{一元高斯分布特征函数}\\\text{特征函数性质}\end{cases}\Rightarrow\text{多元高斯分布特征函数}\end{align}$$2、当然也可以使用定义与变量代换定理一步到位。3、有了特征函数，我们就可以证明一系列关键定理4、我们讨论一下逆矩阵计算的问题，定义精度矩阵，重写了条件分布定理5、然后我们显然得出了高斯线性模型的结果。6、当然高斯模型还有非常多性质，我们将继续踏上征途。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00001%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、机器学习若干符号解释我们在表达概念时，通常用集合论，空间之类的术。这个时候，我们注意元素和集合的区别。而在我们表达运算时，我们通常用矩阵的概念，这个时候你要注意维度、列、行的概念。多加练习，你很快就会掌握这个表达。 1、输入空间用矩阵表示数据集$\mathcal{D} : \boldsymbol{X}=\left[\begin{array}{c}\boldsymbol{x}_{1}^\text{T}\\\boldsymbol{x}_{2}^\text{T}\\\vdots\\\boldsymbol{x}_{n}^\text{T}\end{array}\right]=\left[\begin{array}{c}\boldsymbol{x}_{1,:}^\text{T}\\\boldsymbol{x}_{2,:}^\text{T}\\\vdots\\\boldsymbol{x}_{n,:}^\text{T}\end{array}\right]=\left[\begin{array}{ccc}x_{1,1} &amp; \dots &amp; x_{1,k} \\\vdots &amp; \dots &amp; \vdots \\x_{n,1} &amp; \dots &amp; x_{n,k}\end{array}\right]$。$\displaystyle \boldsymbol{x}_i=\boldsymbol{x}_{i,:} $表示用 $\displaystyle \boldsymbol{X} $的第i行转置构造向量 $\displaystyle \underbrace {\boldsymbol{x}_i}_{k\times1} $用matlab举个例子： 1234567891011121314%矩阵与机器学习&gt;&gt; X=[12, 14, 15; 1.5, 5.4 ,6.7;20,3.4,5]X = 12.0000 14.0000 15.0000 1.5000 5.4000 6.7000 20.0000 3.4000 5.0000&gt;&gt; x1=X(1,:)'x1 = 12 14 15&gt;&gt; x1(2,1)ans = 14 所以 $\displaystyle \boldsymbol{x_1} $表示包含k个维度的一次观测(示例)。我们用集合论的方式再叙述一遍有n个样本的数据集或者样本空间 $\displaystyle X=\{\boldsymbol{x}_1,\,\boldsymbol{x}_2,\, …,\,\boldsymbol{x}_i,\,…,\,\boldsymbol{x}_n\} \subseteq \mathcal{X}^n$ ,其中 $\displaystyle \boldsymbol{x}_i $是样本点(样本)，我们把输入的所有可能取值集合 $\displaystyle \mathcal{X} $叫做输入空间,无监督学习下也可以称为样本空间。显然 $\displaystyle X\subseteq\mathcal{X}^n $。 输入空间的矩阵表示和集合表示我们需要多加熟悉、灵活运用。这是我们思考多维问题的基础。 2、输出空间集合$\displaystyle Y=\{y_1\,,y_2\,,..\,,y_i\,,…\,,y_n\} \subseteq \mathcal{Y} $，其中输出空间 $\displaystyle \mathcal{Y} $、 输入样本$\displaystyle Y $、输入样本点 $\displaystyle y_i $。 矩阵表示： $\displaystyle \boldsymbol{y}=[y_1\,,y_2\,,..\,,y_i\,,…\,,y_n]^{\text{T}} $ 在有监督学习中，我们把集合 $\displaystyle \{(\boldsymbol{x}_i,y_i)\mid 1 \leqslant i\leqslant n\}$也叫做训练集$\mathcal{D}$ ，$\displaystyle (\boldsymbol{x}_i,y_i) $表示第i个样本。 符号$\displaystyle P(y\mid \boldsymbol{x})=\mathcal{N}(y\mid \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$与$\displaystyle y\mid \boldsymbol{x}\sim\mathcal{N}( \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$是同一个意思。注意 $\displaystyle \mid $的不同意思。[^1] 统计学中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{\beta}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{\beta}) $ 机器学习中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{w}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{w}) $ 3、假设空间：1、如果真实的世界的关系是 $\displaystyle y=h(\boldsymbol{x})$， 世界充满噪声。所以 $\displaystyle y=h(\boldsymbol{x})+e$。现在我们有一个样本或者数据集 $\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$。我们想通过这个数据集 $\displaystyle \mathcal{D}$估计出 $\displaystyle f(\boldsymbol{x})$来找到 $\displaystyle h(\boldsymbol{x})$。其实我们能找到的 $\displaystyle f$有很多，现在我们把它汇集起来： $\displaystyle \mathcal{H}=\{f_i\}$。我们的模型是 $\displaystyle y=f(\boldsymbol{x})+\varepsilon$。 现在我们换一个说法:1、世界是这样的： $\displaystyle p(y=h(\boldsymbol{x})\mid\boldsymbol{x})$2、我们观察到的世界是这样的：$\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$3、我们假设世界是这样的： $\displaystyle p(y=f(\boldsymbol{x)\mid }\boldsymbol{x},\mathcal{D},M)$[^1]，其中 $\displaystyle M$是模型(算法)。于是$\displaystyle \varepsilon=y-f=y-h+h-f=y-h+h-\mathrm{E}[f]+\mathrm{E}[f]-f$$\displaystyle \mathrm{E}[\varepsilon^2]=\mathrm{Var}[e]+\left(h-\mathrm{E}[f]\right)^2+\mathrm{E}\left[\left(f-\mathrm{E}[f]\right)^2\right]$$$ 平方损失期望=噪声方差+偏误^2+模型方差$$ 4、我们还可以写成：$\displaystyle \mathcal{H} =\{f\mid p(y=f(\boldsymbol{x})\mid\boldsymbol{x}, \mathcal{D})\}=\{f(\boldsymbol{\beta})\mid p(y=f(\boldsymbol{x};\boldsymbol{\beta})\mid \boldsymbol{x},\mathcal{D};\boldsymbol{\beta}),\boldsymbol{\beta}\in \mathbb{R}^k\}$。这里的$\displaystyle \mathcal{H} $是模型 $f$的集合。 2、这里的符号有一个重要的解释：$\displaystyle y $是一个随机变量，它的取值是 $\displaystyle y=y_i $。 $\displaystyle \boldsymbol{x} $表示的是 $\displaystyle n $个 $\displaystyle k $维输入数据。也就是说 $\displaystyle \boldsymbol{x} $也是一个变量，不过是向量的形式。它的取值是 $\displaystyle \boldsymbol{x}=\boldsymbol{x}_i $。 3、换一个程序员比较好理解的说法：$\displaystyle y,\boldsymbol{x} $是一个类。而 $\displaystyle y_i,\boldsymbol{x}_i $是一个实例。所以一个实例 $\displaystyle P(y_i\mid \boldsymbol{x}_i,\mathcal{D})$，又有 $\displaystyle P(y_{n+1}\mid \boldsymbol{x}_{n+1},\mathcal{D})$是一个数或者一个概率。$\displaystyle \hat{y},\hat{y}_i $也是类和实例的区别。 输出的最佳估计： $\displaystyle \hat{y}=\hat{f}(x)=\mathop {\text{argmax}}\limits_{\hat{y}}P(y=\hat{y}\mid \boldsymbol{x},\mathcal{D})$ 4、算法空间$\displaystyle \zeta\in\mathcal{L} $，它是算法的集合。 5、参数空间$\displaystyle \boldsymbol{\beta} \in\mathbb{R}^k $。这里的元素我们将 $\displaystyle \mathbb{R}^k$的k维有序组与向量矩阵$\mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}$等同，以方便表达。 6、概念总结有了这些基本概念，我们就可以建立起机器学习的基本框架。一张图搞定: 机器学习框架 7、指示函数，或者叫示性函数$\displaystyle \mathrm{I}_x(A)=\begin{cases}1&amp;\text{if }x\in A\\0&amp;\text{if }x\notin A\end{cases}$ 8、评论这段概论，大部分是站在频率学派的角度解释的，以后我们还会用贝叶斯学派的观点。 我们注意到符号与文字的转换要非常熟练，这就像英语，如果做到同声翻译的水平，这将有利于快速理解。所以一套好的数学符号是非常关键，好数学符号令人赏心悦目。但是每个人都有不同的风格，这就有点无语，以至于不同的书，符号不一样。TMD这是英语有了方言啊。有些书上的符号真是丑的不堪入目啊。严重影响阅读学习体验。 二、回归模型1、线性回归模型：模型的一些表示方法$ y_i=\boldsymbol{x}_{i}^T\boldsymbol{\beta}+\epsilon_i=\boldsymbol{x}_{i,:\,}^T\boldsymbol{\beta}+\epsilon_i$$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $模型矩阵解释：$$\displaystyle \mathop{\boldsymbol{y}}\limits_{(n\times 1)}=\underbrace{\mathop{\boldsymbol{X}}\limits_{(n\times k)} \mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}}_{n\times k} +\mathop{\boldsymbol{\epsilon}}\limits_{(n\times 1)} $$ 2、梯度下降算法：$\boldsymbol{\beta}: =\boldsymbol{\beta}-\alpha\nabla S$梯度下降算法的本质：可以使用相图的思想加以理解。例如有关系$\displaystyle F(x,y,t)=0$。如果我们画出$$\begin{cases}\dot{x}=-3x+5y\\\dot{y}=-5x-7\sin(y)\end{cases}$$动力系统的相图。那么如果是凸函数。相图上的曲线集就会流向平衡点。如图 相图所谓梯度就是图中的箭头乘以梯度的大小。代表了该点速度最快的方向。而 $\displaystyle\alpha_k$就是给梯度加了一个控制器。所以应该能够理解梯度下降算法了：$$\boldsymbol{\beta}_{k+1}=\boldsymbol{\beta}_{k}-\alpha_k\nabla S(\boldsymbol{\beta}_k)$$所以当系统比较复杂的时候，必然就面临问题。例如这种：$$\begin{cases}\dot{x}=-x+y\\\dot{y}=xy-1\end{cases}$$这个系统就非常复杂了。初始位置不同，我们将走向完全不同的结局。相图2 3、规范方程规范方程的本质可以如下理解： 线性回归几何解释 解决学习平方误差 $\displaystyle S$的最小化问题：$$\mathop {\min }\limits_\boldsymbol{\beta}S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 现在我们用线性空间的概念来加以理解：$\displaystyle \boldsymbol{X}$张成的空间,或者说超平面 $\displaystyle span(\boldsymbol{X})=span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,j},…,\boldsymbol{x}_{:,k})$ 这里的 $\displaystyle\boldsymbol{x}_{:,j}=\left[\begin{array}{c}x_{1,j} \\x_{2,j}\\\vdots\\x_{n,j}\end{array}\right]$。如图我们很容发现要使得 $\displaystyle\boldsymbol{\epsilon} $的欧式距离最短。那么$\displaystyle\boldsymbol{\epsilon} $必然与 $\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$垂直。即有如下方程。$$\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})=\boldsymbol{0}$$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$。所以 $\displaystyle \boldsymbol{y} $的最佳估计量 $\displaystyle \hat{\boldsymbol{y} }$是 $\displaystyle \boldsymbol{y} $在$\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$空间上的投影。 注释：[^1]: 如果 $\displaystyle \zeta$表示算法,可写为$\displaystyle P(y\mid \boldsymbol{x},\mathcal{D},\zeta) $ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>神经网络</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-08-%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-08-三大分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$分布1、定义$$\begin{align}x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0\end{align}$$ 我们有时候也这样书写： $\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\exp(-vx)$。我们称之为等待时机分布，其中 $\displaystyle v$是速度参数。 $\displaystyle n$是次数参数。随机变量的含义是：事件 $\displaystyle A$出现后，再次出现 $\displaystyle n$次需要的时间。 2、$\displaystyle \Gamma$分布的特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 我们来求 $\displaystyle \Gamma$分布的特征函数：$$\begin{align}\displaystyle \varphi(t)&amp;=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\times\mathrm{Ga}(x\mid n,v)\mathrm{d}x\\&amp;=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\mathrm{d}x\\&amp;=\int_0^{\infty}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-(vx-\mathrm{i}t x)}\mathrm{d}x\,,y=vx-\mathrm{i}t x\\&amp;=\frac{v^n}{\Gamma(n)}\int_A \left(v- \mathrm{i}t\right)^{-n}y^{n-1}\mathrm{e}^{-y}\mathrm{d}y\\&amp;=\frac{v^n}{\Gamma(n)}\frac{\Gamma(n)}{\left(v- \mathrm{i}t\right)^n}\\&amp;=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 3、$\displaystyle \Gamma$分布的数字特征期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\left(-\mathrm{i}\right)^2n \frac{v^{n+1}}{\left(v- \mathrm{i}t\right)^{n+2}}\bigg|_{t=0}=\frac{n}{v}$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\frac{n}{v^2}$ 众数$\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{Ga}(x\mid n,v)=\frac{n-1}{v}$其中：$\displaystyle \frac{\partial }{\partial x}\mathrm{Ga}(x\mid n,v)=-C\mathrm{e}^{-vx} \left( x^{n-1}v-x^{n-2}n+x^{n-2} \right)=0\to\mathrm{mode}[x] $ 4、$\displaystyle \Gamma$分布的可加性【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从$\displaystyle \Gamma$分布 $\displaystyle x_i\sim \mathrm{Ga}(x_i\mid m_i,v)$。那么$$\begin{align} \zeta=x_1+x_2+\cdots+x_n\sim\mathrm{Ga}(\zeta\mid m_1+m_2+\cdots+m_n,v)\end{align}$$ 证明：我们有 $\displaystyle \Gamma$分布的特征函数： $\displaystyle \varphi(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^n$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特征函数之积。知道： $$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^{m_1+m_2+\cdots+m_n}\end{align}$$ 于是证明了结论。 二、$\displaystyle \chi^2(x\mid n)$分布1、定义$\displaystyle \chi^2(x\mid n)$分布$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 可以看出： $\displaystyle x\sim\chi^2(x\mid n)=\mathrm{Ga}(x\mid \frac{n}{2},\frac{1}{2}) $。 【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$是相互独立的标准高斯分布，那么随机变量：$$\begin{align}\xi=\chi^2=x_1^2+x_2^2+\cdots+x_n^2\sim\chi^2(\xi\mid n)\end{align}$$ 证明： 【引理】: 若 $\displaystyle x\sim \mathcal{N}(0,1)$,则 $\displaystyle x^2\sim\chi^2(x\mid 1)$ 有$\displaystyle y=x^2$的反函数：$$x=h(y)=\begin{cases} -\sqrt{y} \,,y&lt;0 \\\ \sqrt{y}\,,0\leqslant y&lt;+\infty \end{cases}$$容易知道导数的绝对值 $\displaystyle \left|h’(y)\right|=\frac{1}{2\sqrt{y}}$。由变量代换定理有：$$\begin{align}p(y)=\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}\mathrm{e}^{-\frac{y}{2}}=\frac{2^{-\frac{1}{2}}}{\Gamma \left(\frac{1}{2}\right)}y^{\frac{1}{2}-1}\mathrm{e}^{-y/2}\end{align}$$也就是说：$$\begin{align}p(y)=\chi^2(x\mid 1)=\mathrm{Ga}(x\mid \frac{1}{2},\frac{1}{2})\end{align}$$ 然后应用 $\displaystyle \Gamma$分布的可加性容易证明定理。 2、$\displaystyle \chi^2(x\mid n)$分布的特征函数和数字特征特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(1-2 \mathrm{i}t\right)^{-n/2}\end{align}$$ 期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=-\left(\mathrm{i}\right)^2\left( 1-2\,it \right) ^{-n/2-1}n\bigg|_{t=0}=n$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=-(\mathrm{i})^2 \left( 1-2\,it \right) ^{-n/2-2} \left( n+2 \right) n-n^2=2n$ 众数 $\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{\chi}^2(x\mid n)=n-2$其中：$\displaystyle \frac{\partial }{\partial x}\chi^2(x\mid n)=1/2C\,\mathrm{e}^{-x/2}\left( x^{n/2-2}n-2\,x^{n/2-2}-x^{n/2-1} \right) =0\to\mathrm{mode}[x] $ 3、$\displaystyle \chi^2(x\mid n)$分布的可加性质【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从 $\displaystyle \chi^2$分布 $\displaystyle x_i\sim \chi^2(x_i\mid m_i)$。那么$$\begin{align} x_1+x_2+\cdots+x_n\sim\chi^2(x\mid m_1+m_2+\cdots+m_n)\end{align}$$证明：我们有 $\displaystyle \chi^2$分布的特征函数： $\displaystyle \varphi(t)=\left(1-2 \mathrm{i}t\right)^{-n/2}$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特殊函数之积。知道：$$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)= \left(1-2 \mathrm{i}t\right)^{-\left(m_1+m_2+\cdots+m_n\right)/2}\end{align}$$于是证明了结论。 三、 $\displaystyle t(x\mid n)$分布1、定义$$\begin{align}x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}\end{align}$$ 【定理】若 $\displaystyle x\sim\mathcal{N}(0,1),y\sim\chi^2(n)$,且 $\displaystyle x$和 $\displaystyle y$相互独立,那么：$$\begin{align}\tau=\dfrac{x}{\sqrt{y/n}}\sim t(\tau\mid n)\end{align}$$ 证明：知：$$\begin{align}p(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\,,-\infty&lt;x&lt;+\infty\end{align}$$ 又知：$$\begin{align}p(y)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$于是由变量代换定理有 $\displaystyle z=\sqrt{y/n}$的密度：$$\begin{align}p(z)&amp;=p(nz^2)\times\left|2nz\right|=\frac{2nz}{2^{n/2}\Gamma \left(\frac{n}{2}\right)}\left(nz^2\right)^{n/2-1}\mathrm{e}^{-(nz^2)/2}\\&amp;=\frac{\sqrt{2n}}{\Gamma \left(\frac{n}{2}\right)}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2}\end{align}$$由随机变量商的分布知：$$\begin{align}p(\tau)&amp;=\int_{-\infty}^{+\infty}p_x(\tau z)\times p_z(z)\times \left|z\right|\mathrm{d}z\\&amp;=(n\pi)^{-1/2}\frac{1}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2 \left(1+\tau^2/n\right)}\,,s=(nz^2)/2 \left(1+\tau^2/n\right)\\&amp;=(n\pi)^{-1/2}\frac{\left(1+\frac{\tau^2}{n}\right)^{-(n+1)/2}}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}s^{(n+1)/2-1}\mathrm{e}^{-s}\mathrm{d}s\\&amp;=(n\pi)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+\tau^2/n\right)^{-(n+1)/2}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-\frac{n+1}{2}}\\\end{align}$$ 即有：$$\begin{align}\tau\sim t(\tau\mid n)=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-(n+1)/2}\end{align}$$ 证毕 2、$\displaystyle t(x\mid n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]\end{align}$$ 知：$\displaystyle t(-x\mid n)=t(x\mid n)$。所以：$$\begin{align}\mathrm{E}[x^k]=0,k\in\{2a\mid a\in \mathbb{R}\}\end{align}$$ 下面我们来考虑： $\displaystyle k\in\{2a+1\mid a\in \mathbb{R}\} $$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{-\infty}^{+\infty} x^kt(x\mid n)\mathrm{d}x=\int_{-\infty}^{+\infty} x^k\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{-\infty}^{+\infty} x^k\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\,,y=1+\frac{x^2}{n}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}2\int_{1}^{+\infty}n^{\frac{k}{2}}(y-1)^{\frac{k}{2}}y^{-\frac{n+1}{2}}\frac{1}{2}n^{\frac{1}{2}}(y-1)^{\frac{1}{2}}\mathrm{d}y\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{+\infty}(y-1)^{\frac{k-1}{2}}y^{-\frac{n+1}{2}}\mathrm{d}y\,,z=\frac{1}{y}\\&amp;=-\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{0}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{-\frac{k-1}{2}}z^{\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{\frac{n-k}{2}-1}\mathrm{d}z\\&amp;=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\\\end{align}$$ 即有：$$\begin{align}\mathrm{E}[x^k]=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)},k\in\{2a+1\mid a\in \mathbb{R}\}\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=0\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n)}{n-2}\,,n&gt;2$ 四、 $\displaystyle F(x\mid m,n)$分布1、定义$$\begin{align}x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0\end{align}$$ 【定理】若有随机变量 $\displaystyle x_1\sim \chi^2(m),x_2\sim\chi^2(n)$，且 $\displaystyle x_1$和 $\displaystyle x_2$相互独立，那么：$$\begin{align}\varphi= \frac{x_1/m}{x_2/n}= \frac{\chi^2(m)/m}{\chi^2(n)/n}\sim F(\varphi\mid m,n)\end{align}$$证明：首先我们来分析一下： $\displaystyle x/n$的分布, 我们知道：$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 令 $\displaystyle x=nz$，由变量代换定理知道： $$\begin{align} z\sim \frac{2^{-\frac{n}{2}}n}{\Gamma \left(\frac{n}{2}\right)}(nz)^{n/2-1}\mathrm{e}^{-nz/2}\,,z&gt;0 \end{align}$$于是有 $\displaystyle z_1=\varphi z_2$由随机变量商的分布知：$$\begin{align}p(\varphi)&amp;=\int_{0}^{+\infty}p_{z_1}(\varphi z_2)\times p_{z_2}(z_2)\times \left|z_2\right|\mathrm{d}z_2\\&amp;=C\int_{0}^{+\infty}(\varphi z_2)^{m/2-1}\mathrm{e}^{-m\varphi z_2/2}(z_2)^{n/2-1}\mathrm{e}^{-n z_2/2}z_2\mathrm{d}z_2\\&amp;=C\varphi^{m/2-1}\int_{0}^{+\infty}z_2^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}z_2(n+m\varphi)}\mathrm{d}z_2\,v=z_2(n+m\varphi)\\&amp;=C\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\int_{0}^{+\infty}v^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}v}\mathrm{d}v\\&amp;=C\Gamma\big(\frac{m+n}{2}\big)2^{(m+n)/2}\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\,,C=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}2^{-(m+n)/2}}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\varphi^{m/2-1}\left(n+m\varphi\right)^{-(m+n)/2}\,,\varphi&gt;0\end{align}$$ 2、$\displaystyle F(x\mid m,n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{0}^{+\infty}x^kF(x\mid m,n)\mathrm{d}x\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\int_{0}^{+\infty}x^kx^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=C\int_{0}^{+\infty}x^{k+m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=Cn^{-(m+n)/2}\int_{0}^{+\infty}x^{k+m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2}\mathrm{d}x\,,y=1+\frac{m}{n}x\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{+\infty}(y-1)^{k+m/2-1}y^{-(m+n)/2}\mathrm{d}y\,z=\frac{1}{y}\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{0}(\frac{1}{z}-1)^{k+m/2-1}z^{(m+n)/2}(-z^{-2})\mathrm{d}z\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{0}^{1}(1-z)^{m/2+k-1}z^{n/2-k-1}\mathrm{d}z\\&amp;=\big(\frac{n}{m}\big)^k\frac{\mathrm{B}\left(\frac{m}{2}+k,\frac{n}{2}-k\right)}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\,,2k&lt;n\\&amp;=\big(\frac{n}{m}\big)^k\frac{\Gamma\big(\frac{m}{2}+k\big)\Gamma\big(\frac{n}{2}-k\big)}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\,,2k&lt;n\\\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=\frac{n}{n-2}\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)^2}\,,n&gt;4$ 五、评述我们通过伽马函数引出伽马分布，当然这有点突兀，怎么就突然冒出来了伽马函数？后面还有一个贝塔函数，似乎有一种神秘力量在背后。这两个函数反复出现是有原因的。有机会我们专门会抽空补充一下，让它更加自然。事实上我们可以通过指数分布引入伽马分布。 伽马分布：$\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0$ 卡方分布：$\displaystyle x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0$ t分布：$\displaystyle x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}$ F分布：$\displaystyle x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-08-三大分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>三大分布</tag>
        <tag>伽马分布</tag>
        <tag>卡方分布</tag>
        <tag>t分布</tag>
        <tag>F分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伽马函数]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-07-%E4%BC%BD%E9%A9%AC%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-07-伽马函数/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$函数$$\begin{align}\Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\,,x&gt;0\end{align}$$ 它有如下性质：1、对于 $\displaystyle x\in(0,+\infty)$，有 $\displaystyle \Gamma(x+1)=x \Gamma(x)$成立2、 $\displaystyle \Gamma(n+1)=n!\,,n=1,2,3,…$3、 $\displaystyle \log \Gamma$在 $\displaystyle (0,+\infty)$上是凸的 1、若干引理在证明上诉性质前：我们要证明 $\displaystyle \Gamma(x)$在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数。我们先说明反常积分柯西判别法的两个引理 【引理1】设 $\displaystyle f$定义在 $\displaystyle [a,+\infty)$，在任何有限区间 $\displaystyle [a,v]$上可积，且$$\begin{align}\lim_{x\to +\infty}x^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle p&gt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^{+\infty}f(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\leqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 与无界反常积分类似，也存在类似的瑕积分的判别方法【引理2】设 $\displaystyle f$定义在 $\displaystyle(a,b]$，其中 $\displaystyle a$位瑕点，在任何 $\displaystyle [v,b]\subset(a,b]$上可积，且$$\begin{align}\lim_{x\to a^+}(x-a)^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle 0&lt;p&lt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^b\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^bf(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\geqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 于是做如下分析：$$\begin{align}\Gamma(x)=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\end{align}$$ 根据引理1、2分析 $\displaystyle \Phi(x)，\Psi(x)$。我们容易得出 $\displaystyle \Gamma(x)$函数的定义域是： $\displaystyle (0,+\infty) $ 回忆一下：含参反常积分在 $\displaystyle E$上一致收敛概念： 然后我们归纳如下引理： 魏尔斯特拉斯 $\displaystyle M$判别法，以及含参反常积分一致收敛的可微性质。 【魏尔斯特拉斯 $\displaystyle M$判别法】 设有函数 $\displaystyle g(y)$，使得：$$\begin{align}\left|f(x,y)\right|\leqslant g(y)\,,(x,y)\in E\times[c,+\infty)\end{align}$$若 $\displaystyle \int_c^{+\infty}g(y)\mathrm{d}y$收敛。则 $\displaystyle \int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛 。【含参反常积分一致收敛的连续性质】设 $\displaystyle f(x.y)$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle [a,b]\in[c,+\infty)$上连续。 【含参反常积分一致收敛的可微性质】设 $\displaystyle f(x.y)$与 $\displaystyle \frac{\partial f}{\partial x}$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上收敛， $\displaystyle \int_c^{+\infty}\frac{\partial f}{\partial x}\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle E$上可微，且：$$\begin{align}\frac{\partial F}{\partial x}=\frac{\partial}{\partial x}\int_c^{+\infty}f(x,y)\mathrm{d}y=\int_c^{+\infty}\frac{\partial }{\partial x}f(x,y)\mathrm{d}y\end{align}$$ 2、$\displaystyle \Gamma(x)$是在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数下面我们来说明 $\displaystyle \Gamma(x)$的内闭一致收敛性质，对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有 $$\begin{align}\Gamma(x)&amp;=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\\&amp;\leqslant\int_0^1u^{a-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{b-1}\mathrm{e}^{-u} \mathrm{d}u&lt;\infty\end{align}$$同时考虑到 $\displaystyle u^{x-1}\mathrm{e}^{-u}&gt;0$且是连续函数。由魏尔斯特拉斯 $\displaystyle M$判别法和含参反常积分一致收敛的连续性质知道 $\displaystyle \Gamma(x)$在 $\displaystyle [a,b]\subset (0,+\infty)$一致收敛且在第一域上连续。 同时我们考虑到：$$\begin{align}\int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln u\mathrm{d}u\end{align}$$ 对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有$$\begin{align}&amp;\int_0^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u\right|\mathrm{d}y\\&amp;\leqslant\int_0^1\left|u^{x-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u\\&amp;\leqslant\int_0^1\left|u^{a-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{b-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u&lt;\infty\\\end{align}$$故 $\displaystyle \int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u$在 $\displaystyle [a,b]\subset (0,+\infty)$上一致收敛。于是由含参反常积分一致收敛的可微性质知道 $\displaystyle \Gamma(x)$在任意$\displaystyle [a,b]\subset (0,+\infty)$上可导，也是说在在定义域 $\displaystyle (0,+\infty)$上可导。且有：$$\begin{align}\Gamma^{(n)}(x)=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln^n u\mathrm{d}u\,,x&gt;0\end{align}$$容易知道： $\displaystyle \Gamma’’(x)&gt;0$。于是就证明了： $$\begin{align}\Gamma(x)是在定义域(0,+\infty)内连续且可导的凸函数。\end{align}$$证明中，一些细节并未详细说明，但是这是简单的。所以请注意。 3、其他性质的证明$$\begin{align}\int_0^a u^{x}\mathrm{e}^{-u}\mathrm{d}u&amp;=-u^x\mathrm{e}^{-u}\big|_0^a+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\&amp;=-a^x\mathrm{e}^{-a}+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\\end{align}$$令 $\displaystyle a\to+\infty$有：$$\begin{align}\Gamma(x+1)=x\Gamma(x)\end{align}$$ 若 $\displaystyle x\in \mathbb{Z}^+$有：$$\begin{align}\Gamma(n+1)=n(n-1)\cdots 2\cdot \Gamma(1)=n!\int_0^{+\infty}\mathrm{e}^{-u} \mathrm{d}u=n!\end{align}$$ 4、 $\displaystyle \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}$证明：令 $\displaystyle A=\Gamma\big(\frac{1}{2}\big)=\int_0^\infty u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u$，于是有：$$\begin{align}A&amp;=\int_0^{+\infty} u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u\,,u=t^2\\&amp;=2\int_0^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\\&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\end{align}$$ 同时有：$$\begin{align}A^2&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-x^2}\mathrm{d}x\int_{-\infty}^{+\infty}\mathrm{e}^{-y^2}\mathrm{d}y\\&amp;=\iint_{R^2}\mathrm{e}^{-(x^2+y^2)}\mathrm{d}x \mathrm{d}y\,,x=r\cos(\theta),y=r\sin(\theta)\\&amp;=\int_{0}^{2\pi}\int_{0}^{+\infty}r \mathrm{e}^{-r^2}\mathrm{d}r \mathrm{d}\theta=\frac{1}{2}\int_{0}^{2\pi}\mathrm{d}\theta=\pi\end{align}$$ 于是有：$$\begin{align} \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-07-伽马函数/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习书单]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2016-12-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在列出一些书单。若有错误，请大家指正。关键词: 机器学习,,书单,学习资源 机器学习资源因为大数据的兴起，机器学习大热。这一行业吸引了大量人才涌入。然后现状是学习机器学习需要较高的数学知识，导致人才供应不足。跟不上行业发展需求。较高的工资和技术时尚吸引大量的人们争相学习机器学习、人工智能。故收集网络整理出这些书籍： 网络上建议的入门顺序：1、斯坦福大学公开课 ：机器学习课程2、李航.统计学习方法3、Machine Learning in Action(机器学习实战） 解释一下这个入门：在这之前你可以简单了解一下，周志华：数据挖掘与机器学习，，斯坦福的公开课可以带你入门。《统计学习方法》可以带你了解基本理论和推导。《机器学习实战》可以让你了解一下实操。这个之后有两个路径，看大神Bengio的《deep learning》，或者继续深化机器学习的知识。当然你也可以参加这个：数据分析竞赛kaggle 附照片&lt;img src=http://oiol5pi05.bkt.clouddn.com/Yoshua_Bengio.jpg width=20%&gt;Yoshua BengioFull ProfessorDepartment of Computer Science and Operations ResearchCanada Research Chair in Statistical Learning Algorithms 解决了机器学习入门问题，我们接下来要进阶：1、PRML（Pattern Recognition and Machine Learning）2、ESL（The Elements of Statistical Learning ）3、MLAPP(Machine Learning: a Probabilistic Perspective) 4、Deep learning-author Yoshua Bengio 上面的书单应该是经典的四本书了。个人比较偏爱《MLAPP》，符号比较优美，叙述比较全面。当然你也可以看模式识别的书，例如《统计模式识别（第3版）Statistical Pattern Reco》，那个封面是豹子的书。 &lt;img src=http://oiol5pi05.bkt.clouddn.com/%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB.jpg width=”50%”&gt; 一些理论当然看这些书的时候，你也许会发现，凸优化，图模型，EM，MCMC之类的，你可以通过下面的书深入一下：1、Convex Optimization2、Probabilistic Graphical Models3、The EM Algorithm and Extensions4、Simulation Fifth Edition Sheldon M. Ross 实战编程1、Python for Data Analysis2、SciPy and NumPy3、Machine Learning for Hackers（这本是用R的）4、集体智慧编程（这个书名有点误导人） 网上搜索很容易找到。现在（2017）又出来很多新书，也发生了很多事情。就不一一列举。 好玩的一本书1、Bad Data Handbook 最后这些书我也还没看。。。。。我会把它们看完的！！！！^_^ 1我们还是来点别的吧： 神经网络学习机器学习之前，你应该先了解、学习神经网络。我知道现在媒体关注点在深度学习，机器学习上。网络上的学习路线也多少是从机器学习开始：从线性模型到深度学习。 可以是，你要知道Hinton的深度学习是挖掘了神经网络的潜能，Hinton是要把被抛弃、被侮辱、几起几落的神经网络再度复兴。在被人摒弃的10年中，加拿大多伦多大学的Geoffery Hinton教授依然坚守。2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。 Geoffery Hinton教授&lt;img src=http://oiol5pi05.bkt.clouddn.com/Geoffrey%20Hinton.jpg width=100%&gt; 与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。 他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。所以我觉得从神经网络到深度学习或者说从神经元到深度学习是会对人工智能有更好的认识。而不是机器学习。神经网络有着传奇的历史，如果你了解它，你将为之着迷。 Andrew Ng对神经网络的看法：&lt;img src=http://oiol5pi05.bkt.clouddn.com/Andrew%20Ng%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%81%E5%BA%A6.jpg width=100%&gt; 神经网络的书籍在这里：1、人工神经网络教程 第一版、第二版 韩立群2、神经网络原理3、神经网络设计3、神经网络与机器学习4、Matlab与神经网络，这种书籍就非常多了。 下面是我口号：$$\displaystyle Life=\int_{Birth}^{Death} (Learning+Working) \mathrm{d}t$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行列式]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-24-%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[摘要：本文意在理清行列式的基础问题。若有错误，请大家指正。关键词: 行列式,机器学习 一、定义函数集合$\displaystyle V=\{f\mid f$是矩阵$M_n(K)$上的数量函数$\}$ $\displaystyle V_1=\{f\mid f\in V,$且满足列线性性$\} $ $\displaystyle V_2=\{f\mid f\in V_1,$且满足列反对称性$\}$ $\displaystyle V_3=\{f:f\in V_2,$且满足规范性$\} $ 容易验证， $\displaystyle V,V_1,V_2$关于加法和数乘运算构成函数空间。 二、性质性质1$\displaystyle V$是 $\displaystyle K$上的 $\displaystyle n^2$元函数空间，所以有 $\displaystyle \dim(V)=\infty$ 性质2$\displaystyle \dim(V_1)=n^n$，其一组基底是 $\displaystyle f_{i_1i_2\cdots i_n}(\varepsilon_{j_1}，\varepsilon_{j_2},\cdots,\varepsilon_{j_n})=\delta_{i_1j_1}\delta_{i_2j_2}\cdots\delta_{i_nj_n}$ 性质3$\displaystyle \dim(V_2)=1，V_2=span(\det(\cdot))$，其中 $\displaystyle \det(\cdot)$为行列式函数。 性质4$\displaystyle V_3=\{\det(\cdot)\}$ 三、证明性质2的证明定义 $\displaystyle n^n$维空间 $\displaystyle L=\{(c_{i_1i_2\cdots i_n}):1\leq i_1,i_2,\cdots,i_n \leq n \}$ 定义映射空间 $\displaystyle V_1$到 $\displaystyle L$的线性映射，从而有 $$\begin{align}\dim(V_1)=\dim(L)=n^n\end{align}$$ 1.【H是单射】 若 $\displaystyle f，g \in V_1且H(f)=H(g)=(c_{i_1i_2\cdots i_n})$则有 $$\begin{align}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=c_{i_1i_2\cdots i_n}\end{align}$$又$$\begin{align}f\big(\alpha_1,\alpha_2,\cdots,\alpha_n)= f(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$ $$\begin{align}g\big(\alpha_1,\alpha_2,\cdots,\alpha_n\big)= g\big(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$所以有 $$\begin{align}f=g\end{align}$$ 2.【H是满射】 同时任意给定 $\displaystyle (c_{i_1i_2\cdots i_n})\in L$，定义函数 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}c_{i_1i_2\cdots i_n}\end{align}$$ 可以验证 $\displaystyle f \in V_1$ ，此时有$$\begin{align}H(f)=(c_{i_1i_2\cdots i_n})\end{align}$$所以 $\displaystyle H$是满射。根据同态映射 $\displaystyle H$我们不难找到 $\displaystyle V_2$的一组基底 性质3的证明若 $\displaystyle f\in V_2$，由列反对称性，有 $$\begin{align}f(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n})=\begin{cases}0 &amp;\exists s,t,\to i_s=i_t\\(-1)^{\tau(i_1i_2\cdots i_n)}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)&amp; i_1,i_2,\cdots ,i_n \textit{ pairwise unequal }\end{cases}\end{align}$$ 其中 $\displaystyle \tau(i_1i_2\cdots i_n)$为排列 $\displaystyle i_1.i_2,\cdots,i_n$的逆序数。所以我们有 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{i_1,i_2,\cdots,i_n\textit{ pairwise unequal } }(-1)^{\tau(i_1i_2\cdots i_n)}a_{i_1}a_{i_2}\cdots a_{i_n}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)\end{align}$$从而有 $$\begin{align}f(A)=f(E)\cdot \det(A)\end{align}$$ 容易验证具有该表达式的函数 $\displaystyle f$属于 $\displaystyle V_2$ 性质4的证明若 $\displaystyle f\in V_3$,则有 $$\begin{align}f(A)=f(E)\cdot \det(A)=\det(A)\end{align}$$]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学分析笔记-数的建立]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-23-%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0-%E6%95%B0%E7%9A%84%E5%BB%BA%E7%AB%8B01%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清数的问题。若有错误，请大家指正。关键词: 有理数,实数 这是测试文章： 一、数的建立1、有理数的缝隙数的建立是数学分析的基础。实数的最小上界性质是我们开启现代数学的钥匙。 关于$p=\sqrt{2}$的重要定律： 构造一个数 $\displaystyle z=\frac{2x+2}{x+2}=x-\frac{x^2-2}{x+2}$ 同时令 $\displaystyle A=\{a\mid a^2&lt;2,a\in \mathbb{Q}^+\}$ 又有 $\displaystyle B=\{b\mid b^2&gt;2, b \in \mathbb{Q}^+\}$ 我们有 $\displaystyle z^2-2=\frac{2(x^2-2)}{(p+2)^2}$ 即可证明$\displaystyle A$ 里面没有最大的数$$\begin{align}x \in A,\to z \in A\,, z&gt;x\end{align}$$ $\displaystyle B$ 里面没有最小的数$$\begin{align}\,x \in B\to z \in B\,,0&lt;z&lt;x\end{align}$$ 这说明尽管有理数之间还有有理数，但是有理数系还是有空隙。而实数系填满了这些空隙，这就是实数系能在分析学中能起基础作用的主要原因。 $\displaystyle \ell \mathbb{ABC}$ 二、行列式行列式定义： $\displaystyle det: M_n(F) \to \Bbb{R} $ 行列式表达式$$\begin{align}det(A)=\sum _{\sigma \in S_n}sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }\end{align}$$ 其中$\sigma$是集合$X=\{1,2,\,…\,n\}$上的置换：$\sigma: X \to X$。$S_n$是置换$\sigma$的集合,易知$S_n$是一个对称群。$\tau(\sigma)$是$\sigma$的逆序数。$\displaystyle sgn(\sigma)=\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)=(-1)^{\tau(\sigma)}$是置换的符号函数。于是： $$\displaystyle det(A)=\sum _{\sigma \in S_n}\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)\prod _{i=1}^n A_{\sigma(i)i }=\sum _{\sigma \in S_n}(-1)^{\tau(\sigma)}\prod _{i=1}^n A_{\sigma(i)i }$$ 简记为：$$det(A)=\sum _{\sigma \in S_n} sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }$$ 三、【函数的极限】令$X$和$Y$是度量空间，假设$E \subseteq X$、$\bm{f}$将$E$映入$Y$内、且$\bm{p}$是$E$的极限点。如果 $\forall \epsilon\,,\exists \delta&gt;0$，对于 $\{\bm{x} \in E\mid 0&lt;d_{X}(\bm{x},\bm{p})&lt;\delta\}$中一切点 $\bm{x}$，使得 $\displaystyle d_{Y}(\bm{f}(\bm{x}),\bm{q})&lt;\epsilon,\,\bm{q}\in Y$成立。就说: $$\lim_{\bf{x} \to\bm{p}} \bm{f}(\bm{x})=\bm{q}$$ 卷积$\displaystyle \mathbf{C}=\mathbf{X}*\mathbf{W}$ $\displaystyle \bm{C}=\bm{X}*\bm{W}$ 分段函数$$\displaystyle f(n)= \left\{\begin{matrix} n/2, &amp; \text {if $n$ is even} \\\ 3n+1, &amp; \text{if $n$ is odd} \end{matrix}\right. \\$$ $\displaystyle \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4 \end{bmatrix}$ 我们可以看到 session.run专业我们就可以Variable 1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello).decode('utf-8')) 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
        <tag>数学</tag>
        <tag>现代数学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyhton-notebook主题颜色配置]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2016-05-01-pyhton-notebook%E4%B8%BB%E9%A2%98%E9%A2%9C%E8%89%B2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、安装jupyter-themer插件12345678910111213➜ ~ cd anaconda➜ anaconda sudo pip install jupyter-themerPassword:The directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting jupyter-themer Downloading jupyter-themer-0.3.0.tar.gz (40kB) 100% |████████████████████████████████| 40kB 35kB/sRequirement already satisfied: jupyter in ./lib/python3.6/site-packages (from jupyter-themer)Requirement already satisfied: notebook in ./lib/python3.6/site-packages (from jupyter-themer)Installing collected packages: jupyter-themer Running setup.py install for jupyter-themer ... doneSuccessfully installed jupyter-themer-0.3.0 二、设定主题1、语法介绍：1234567%语法格式如下usage: jupyter-themer [-c COLOR, --color COLOR] [-l LAYOUT, --layout LAYOUT] [-t TYPOGRAPHY, --typography TYPOGRAPHY] [-f CODE_FONT, --font CODE_FONT] [-b BACKGROUND, --background BACKGROUND] [-s OPTION, --show OPTION] 2、显示所有可选颜色主题：12345678910111213141516171819202122232425262728293031323334353637383940414243➜ anaconda jupyter-themer --show color3024-day3024-nightabcdefambiancebase16-darkbase16-lightblackboardcobaltcolorforthdraculaeclipseeleganterlang-darkicecoderlesser-darkliquibytematerialmbomdn-likemidnightmonokaineatneonightparaiso-darkparaiso-lightpastel-on-darkrubybluesetisolarized-darksolarized-lightthe-matrixtomorrow-night-brighttomorrow-night-eightiesttcntwilightvibrant-inkxq-darkxq-lightyetizenburn➜ anaconda 3、设定monokai主题12➜ anaconda jupyter-themer -c monokaiCustom jupyter notebook theme created - refresh any open jupyter notebooks to apply theme. 效果如下 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>notebook</tag>
        <tag>计算环境</tag>
        <tag>工程问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夜晓风]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2015-09-19-%E5%A4%9C%E6%99%93%E9%A3%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2015-09-19-夜晓风/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夜晓风皎月榕树头，夜晓风。 一曲江畔，感似岁月悠悠， 山河在，壮志未酬。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2015-09-19-夜晓风/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[穗夜]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2014-06-19-%E7%A9%97%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2014-06-19-穗夜/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 穗夜夜声碎月光如练，榕树头江畔愁眠。 桥头浪卷已三更，沉沉幕夏雨如注。 引线小白写于2014-6-19 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2014-06-19-穗夜/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>穗夜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再回大学城]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-05-20-%E5%86%8D%E5%9B%9E%E5%A4%A7%E5%AD%A6%E5%9F%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-05-20-再回大学城/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 再回大学城沉浸校园曲风气爽清秋节，叶至桂香来，小湖椰影廊桥，曾记否，谷围晓月，灯影朦胧，勘回首，沉浸校园曲风，且看从容，壮志未酬笑谈中。 引线小白写于2011-5-20 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-05-20-再回大学城/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>大学城</tag>
        <tag>诗歌</tag>
        <tag>小谷围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夏雨急]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-04-30-%E5%A4%8F%E9%9B%A8%E6%80%A5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-04-30-夏雨急/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夏雨急夏雨急，落如珠， 今朝且看风疾。 迎地扫，路人急， 此地空余。。。 小白写于2011-4-30日 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-04-30-夏雨急/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏雨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初夏里]]></title>
    <url>%2F%E4%BC%A4%E6%84%9F%2F2011-04-27-%E5%88%9D%E5%A4%8F%E9%87%8C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/伤感/2011-04-27-初夏里/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 初夏里清风夜，晓月枝头，照斑驳绿影初夏里，世事心愁，思长夜忧忧叹悲歌，一曲无穷，怎泪眼朦胧自惆怅，知与谁同，独望江水流 引线小白写于2011-4-27广州 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/伤感/2011-04-27-初夏里/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>伤感</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>初夏里</tag>
        <tag>引线小白</tag>
        <tag>诗词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[烈日炎炎]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2010-08-04-%E7%83%88%E6%97%A5%E7%82%8E%E7%82%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 烈日炎炎日炎炎，清风绝。惜雨荫少，浪浪绵绵。汗易求冰难接。是个酷热天！ 引线小白写于2010-8-4 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏天</tag>
        <tag>日日炎炎</tag>
      </tags>
  </entry>
</search>
