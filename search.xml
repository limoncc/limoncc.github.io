<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[理解RWKV模型一_大语言模型研究]]></title>
    <url>%2Fpost%2F7a57a5a743894a0e%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/7a57a5a743894a0e/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、引言RWKV模型是一种RNN范式下的大语言模型实现范式。效果是相当不错，关键它的训练和推理要求资源低。非常值得研究。这里给个翻译: 敏感加权键值模型(Receptance Weighted Key Value model)。由于作者还未发表[论文]^1(论文已出(2022-5-22)赶紧研究中)。这里只能借助源码来做一些分析。这篇文章就是笔者对源码分析和阅读论文的一些记录。希望对大家有所帮助。如有错误请大家指正。 二、从线性注意力到RNN2.1、从AFT到RWKV在AFT[^2]中对 $t$权重和对 $t-1$之前的权重是原则是一样的。而在RWKV中，对$t-1$之前权重是随序列衰减的。对 $t$权重是单独赋值了一个 $u$。 AFT 也就是说 $$\begin{align}\bm{o}_t=\sigma(\bm{r}_t) \odot\bm{I}^\text{T}\big[\mathrm{softmax}\big(-w\cdot\mathrm{\bm{arange}}[0:t-1]+\bm{K}\big)\odot \bm{V}\big]\end{align}$$ 2.2、变换到RNN$$\begin{align}o_{t}=\sigma(r_t) \cdot\frac{\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau}\big]\cdot v_\tau+\exp\big[u+k_t\big]\cdot v_t}{\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau}\big]+\exp\big[u+k_t\big]}\end{align}$$ 为了便于分析，我们单独考察分子$$\begin{align}a_t^{out} &amp;=\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau}\big]\cdot v_\tau+\exp\big[u+k_t\big]\cdot v_t\\&amp;=\exp[-w]\sum_{\tau=1}^{t-2} \exp\big[-{w(t-2-\tau)+k_\tau}\big]\cdot v_\tau+\exp[k_{t-1}]v_{t-1}+\exp\big[u+k_t\big]\cdot v_t\end{align}$$ 令 $\displaystyle a_{t-2}=\sum_{\tau=1}^{t-2} \exp\big[-{w(t-2-\tau)+k_\tau}\big]\cdot v_\tau$ 则有$$\begin{align}a_t^{out} &amp;= \exp[-w]a_{t-2}+\exp[k_{t-1}]v_{t-1}+\exp\big[u+k_t\big]\cdot v_t\\&amp;=a_{t-1}+\exp\big[u+k_t\big]\cdot v_t\end{align}$$ 这样我们改为递推RNN的形式就是 1、Initialize $\displaystyle a_0=0,b_0=0$2、Output$\displaystyle\qquad\qquad\begin{array}{|lc}a_t^{out} =a_{t-1}+\exp[u+k_t]\cdot v_t \\b_t^{out} =b_{t-1}+\exp[u+k_t]\\wkv_t= a_t^{out}/b_t^{out}\\o_t=\sigma(r_t)\cdot wkv_t\end{array}\\$3、Update the state$\displaystyle\qquad\qquad\begin{array}{|lc}a_t =\exp[-w]a_{t-1}+\exp[k_t]\cdot v_t \\b_t =\exp[-w]b_{t-1}+\exp[k_t]\\\end{array}\\$3、# end 三、时间混合机制3.1、基本框架3.2、上溢与代码实现RWKV模型V4的代码实现和原理公式是不一样的。融入了一个溢出的技巧，这导致原理公式的脱节代码实现。因为这不太容易看出是等价的。下面文本提供一个证明。这个证明笔者最早发布在(如何评价最新的RWKV论文 (arXiv 2305.13048)？)的知乎问答中。 RWKV模型V4 RWKV模型V4的代码实现和原理公式的等价性证明：下面我们来分析一下如何避免 $\displaystyle \exp[k_t]$向上溢出的技巧。为了简化分析我们近看一个词元(token)的一个维度(通道)计算。要防止 $\displaystyle\displaystyle \exp[k_t]$上溢出，显然这个时候 $\displaystyle k_t$是一大数。这个时候，我们给他减去一个一样大的数 $\displaystyle q$就可以了。注意 $\displaystyle q=k_t+\delta$。 只要确保 $\displaystyle \displaystyle \exp[\delta] $不会溢出即可，这样有： $$\begin{align}o_{t}=\sigma(r_t) \cdot\frac{\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau-q}\big]\cdot v_\tau+ \exp\big[u+k_t-q\big]\cdot v_t}{\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau-q}\big]+ \exp\big[u+k_t-q\big]}\end{align}$$为了便于分析，我们单独考察分子 $$\begin{align}a_t^{out}&amp;=\sum_{\tau=1}^{t-1} \exp\big[-{w(t-1-\tau)+k_\tau-q}\big]\cdot v_\tau+\exp\big[u+k_t-q\big]\cdot v_t\\&amp;=\exp[-w]\sum_{\tau=1}^{t-2} \exp\big[-{w(t-2-\tau)+k_\tau-q}\big]\cdot v_\tau+\exp\big[k_{t-1}-q\big]\cdot v_{t-1}+\exp\big[u+k_t-q\big]\cdot v_t\\&amp;=\exp[-q]\bigg[\exp[-w]\cdot a_{t-2}+\exp\big[k_{t-1}\big]\cdot v_{t-1}\bigg]+ \exp\big[u+k_t-q\big]\cdot v_t\end{align}$$ 为了规避 $\displaystyle \exp[k_{t-1}] $溢出，我们也需要一个 $\displaystyle \exp[-\rho_{t-1}]$ 这样有 $$\begin{align}a_t^{out}&amp;=\exp\big[\rho_{t-1}-q\big]\big[\exp[-w-\rho_{t-1}]\cdot a_{t-2}+\exp\big[k_{t-1}-\rho_{t-1}\big]\cdot v_{t-1}\big]+\exp\big[u+k_t-q\big]\cdot v_t\end{align}$$同时 $\displaystyle \exp[-w-\rho_{t-1}]$也有溢出风险，于是我们可以添加一个 $\displaystyle \exp[\rho_{t-2}] $这样有$$\begin{align}a_t^{out}&amp;=\exp\big[\rho_{t-1}-q\big]\big[\exp[\rho_{t-2}-w-\rho_{t-1}]\cdot \exp[-\rho_{t-2}] a_{t-2}+\exp\big[k_{t-1}-\rho_{t-1}\big]\cdot v_{t-1}\big]+\exp\big[u+k_t-q\big]\cdot v_t\end{align}$$ 我们令 $\displaystyle \tilde{a}_{t-2}=\exp[-\rho_{t-2}] a_{t-2}$，这样发现 $$\begin{align}\tilde{a}_{t-1}&amp;=\exp[-\rho_{t-1}] a_{t-1}\\&amp;=\exp[-\rho_{t-1}] \big(\exp[-w]a_{t-2}+\exp[k_{t-1}]\cdot v_{t-1}\big)\\&amp;=\exp[-w-\rho_{t-1}]a_{t-2}+\exp[k_{t-1}-\rho_{t-1}]\cdot v_{t-1}\\&amp;=\exp[-\rho_{t-1}] \big(\exp[-w]\exp[\rho_{t-2}]\tilde{a}_{t-2}+\exp[k_{t-1}]\cdot v_{t-1}\big)\\&amp;=\exp[\rho_{t-2}-w-\rho_{t-1}]\tilde{a}_{t-2}+\exp[k_{t-1}-\rho_{t-1}]\cdot v_{t-1}\end{align}$$ 这样就有$$\begin{align}a_t^{out}=\exp\big[\rho_{t-1}-q\big]\tilde{a}_{t-1}+\exp\big[u+k_t-q\big]\cdot v_t\end{align}$$ $$\begin{align}\tilde{a}_{t}=\exp[\rho_{t-1}-w-\rho_{t}]\tilde{a}_{t-1}+\exp[k_{t}-\rho_{t}]\cdot v_{t}\end{align}$$ 下面我们开始最后征程，给 $\displaystyle q$和 $\displaystyle \rho_t$给一个恰当具体的值。事实上我们有, 对于 $\displaystyle \tilde{a}_t=\exp[-\rho_t] a_t$$$\begin{align}a_1&amp;=\exp[k_1]v_1\\b_1&amp;=\exp[k_1]\end{align}$$我们可以令 $\displaystyle \rho_1=k_1$ 这样我们就有新的递推公式，未来规避溢出bug，我们先后做了三次修正，此外还有 $\displaystyle \exp[\rho_{t-1}-q]$ $$\begin{align}\unicode{x2776}&amp;\;\exp[-q]\longrightarrow \exp[u+k_t-q]\\\unicode{x2777}&amp;\;\exp[\rho_{t-1}]\longrightarrow \exp[k_{t-1}-\rho_{t-1}]\\\unicode{x2778}&amp;\;\exp[\rho_{t-2}]\longrightarrow \exp[\rho_{t-2}-w-\rho_{t-1}]\\\unicode{x2776}\unicode{x2777}&amp;\;\exp[\rho_{t-1}-q]\end{align}$$ 解决问题的答案就蕴含在解决问题的过程中的。特别的，对于 $\displaystyle x\leqslant =0 $有 $\displaystyle f(x)=\exp[x] \in (0,1]$。考虑到sub-max算子 $$\begin{align}\mathrm{submax}(x,y): = x-max(x,y)\leqslant 0\end{align}$$对于 $\displaystyle \exp[\rho_{t-2}-w-\rho_{t-1}]=\exp\big[\mathrm{submax}(\rho_{t-2}-w,A)\big]$，也就是说 $\displaystyle \rho_{t-1} = \max(\rho_{t-2}-w,A)$。同理我们就有如下这些$$\begin{align}\exp[\rho_{t-2}-w-\rho_{t-1}]=\exp\big[\mathrm{submax}(\rho_{t-2}-w,A)\big]&amp;\to \rho_{t-1} = \max(\rho_{t-2}-w,A)\\\exp[\rho_{t-1}-q]=\exp\big[\mathrm{submax}(\rho_{t-1},B)\big]&amp;\to q=\max(\rho_{t-1},B)\\\exp[k_{t-1}-\rho_{t-1}]=\exp\big[\mathrm{submax}(k_{t-1}-C)\big]&amp;\to\rho_{t-1}=\max(k_{t-1},C)\\\exp[u+k_t-q]=\exp\big[\mathrm{submax}(u+k_t,D)\big]&amp;\toq=\max(u+k_t,D)\end{align}$$易得$$\begin{align} q=\max(\rho_{t-1},u+k_t)\\ \rho_{t-1}=\max(\rho_{t-2}-w,k_{t-1})\end{align}$$ 这样我们就有新的递推RNN，也就证明了理论公式与防溢公式的等价性。 1、Initialize$\displaystyle\qquad\qquad\begin{array}{|lc}\rho_1=k_1\\\tilde{a}_1=\exp[-\rho_1] a_1=v_1\\\tilde{b}_1=\exp[-\rho_1] b_1=1\end{array}\\$2、Output$\displaystyle\qquad\qquad\begin{array}{|lc}q=\max(\rho_{t-1},u+k_t)\\a_t^{out} =\exp\big[\rho_{t-1}-q\big]\tilde{a}_{t-1}+\exp\big[u+k_t-q\big]\cdot v_t \\b_t^{out} =\exp\big[\rho_{t-1}-q\big]\tilde{b}_{t-1}+\exp\big[u+k_t-q\big]\\wkv_t= a_t^{out}/b_t^{out}\\o_t=\sigma(r_t)\cdot wkv_t\end{array}\\$3、Update the state$\displaystyle\qquad\qquad\begin{array}{|lc}\rho_{t}=\max(\rho_{t-1}-w,k_t)\\\tilde{a}_t =\exp[\rho_{t-1}-w-\rho_{t}]\tilde{a}_{t-1}+\exp[k_{t}-\rho_{t}]\cdot v_{t} \\\tilde{b}_t =\exp[\rho_{t-1}-w-\rho_{t}]\tilde{b}_{t-1}+\exp[k_{t}-\rho_{t}]\\\end{array}\\$3、# end 具体代码实现可以参考RWKV_in_150_lines。 四、通道混合机制RWKV使用通道混合机制代替了transforms的FFN，特别的是使用的平方和门控制机制。起着放大和筛选机制。 $$\begin{align}o_t = \sigma(r_t)\odot \big[W_v\cdot \mathrm{ReLU}^2(k_t,0)\big]\end{align}$$ 五、基本架构模型架构基本和transforms一致。整体是就是时间混合、通道混合、残差连接、层归一化的不断堆叠。 RWKV模型架构 [^1]: Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., et al. (n.d.). RWKV: Reinventing RNNs for the Transformer Era.[^2]: Zhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., &amp; Susskind, J. (2021, September 21). An Attention Free Transformer. arXiv. http://arxiv.org/abs/2105.14103. Accessed 30 May 2023 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/7a57a5a743894a0e/如果您需要引用本文，请参考：引线小白. (May. 18, 2023). 《理解RWKV模型一_大语言模型研究》[Blog post]. Retrieved from https://www.limoncc.com/post/7a57a5a743894a0e@online{limoncc-7a57a5a743894a0e,title={理解RWKV模型一_大语言模型研究},author={引线小白},year={2023},month={May},date={18},url={\url{https://www.limoncc.com/post/7a57a5a743894a0e}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>生成模型</tag>
        <tag>RWKV模型</tag>
        <tag>大语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[扩散模型研究一：去噪扩散概率模型DDPM]]></title>
    <url>%2Fpost%2F1c60669bbe56769f%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/1c60669bbe56769f/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、基本介绍扩散模型大放异彩，从原理上搞清楚运作机制非常关键。下面我们定义一些符号：对于观测变量 $\bm{x}$，与VAE对应一个隐变量 $\bm{z}$不同，扩散模型对应一组隐变量 $\displaystyle \mathcal{D}_T^\bm{z}=\{\bm{z}_t\}_{t=1}^T$, 而且我们假设$ \mathcal{D}_T^\bm{z}$有马尔可夫性质。这样我们的模型就不是 $\displaystyle p(\bm{x})=\int p(\bm{x}\mid \bm{z})p(\bm{z})d\bm{z}$而是 $$\begin{align}p(\bm{x})&amp;=\int p(\bm{x}\mid \mathcal{D}_T^\bm{z})d \mathcal{D}_T^\bm{z}\\&amp;=\int p(\bm{x}\mid \bm{z}_1)p(\bm{z}_1\mid \bm{z}_2)\cdots p(\bm{z}_{t-1}\mid \bm{z}_T)p(\bm{z}_T)d \mathcal{D}_T^\bm{z}\\&amp;=\int p(\bm{x}\mid \bm{z}_1)p(\bm{z}_T)\prod_{t=2}^{T}p(\bm{z}_{t-1}\mid \bm{z}_{t})d \mathcal{D}_T^\bm{z}\end{align}$$ 下面我们来解释一下这么做的动机：我们认为上帝是这么来生成一张图片的。先掷骰子(对，没错我们又假设上帝投骰子)，然后慢慢画出图像(先色块，再形状，再细节，就像美术的厚涂法一样)。这对应着解码过程(decode)，反过来就是编码过程(encode)。 $$\begin{align}\text{encode}: &amp;\bm{x}\rightarrow\bm{z}_1\rightarrow\bm{z}_2\rightarrow\cdots\rightarrow\bm{z}_{T-1}\rightarrow\bm{z}_T=\bm{z}\\\text{decode}: &amp;\bm{z}=\bm{z}_{T}\rightarrow\bm{z}_{T-1}\rightarrow\bm{z}_{T-2}\rightarrow\cdots\rightarrow\bm{z}_{1}\rightarrow\bm{x}\end{align}$$ 也就是说有三个关键分布1、编码分布 $\displaystyle p(\bm{z}\mid\bm{x})$2、生成分布(解码分布) $\displaystyle p(\bm{x}\mid\bm{z})$3、先验分布 $\displaystyle p(\bm{z})$ 二、损失函数考虑到编码分布的复杂性，我们一般使用易于计算的分布 $\displaystyle q(\bm{z}\mid \bm{x})$。我们使假设的模型分布尽可能与真实数据靠拢 $$\begin{align}\mathbb{KL}\big[q(\bm{x},\mathcal{D}_T^\bm{z})|p(\bm{x},\mathcal{D}_T^\bm{z})\big]&amp;=\int q(\bm{x},\mathcal{D}_T^\bm{z})\log \frac{q(\bm{x},\mathcal{D}_T^\bm{z})}{p(\bm{x},\mathcal{D}_T^\bm{z})}d \mathcal{D}_T^\bm{z}d\bm{x}\\&amp;=\int q(\mathcal{D}_T^\bm{z}\mid \bm{x})q(\bm{x})\log \frac{q(\mathcal{D}_T^\bm{z}\mid\bm{x})q(\bm{x})}{p(\bm{x},\mathcal{D}_T^\bm{z})}d \mathcal{D}_T^\bm{z}d\bm{x}\\&amp;=\mathbb{E}_{\bm{x}\sim q(\bm{x})}\bigg[\int q(\mathcal{D}_T^\bm{z}\mid \bm{x})\log q(\bm{x})d \mathcal{D}_T^\bm{z}-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x}) \log\frac{p(\bm{x},\mathcal{D}_T^\bm{z})}{q(\mathcal{D}_T^\bm{z}\mid \bm{x})}d \mathcal{D}_T^\bm{z} \bigg]\\&amp;=-\mathbb{H}_{q(\bm{x})}\big[\bm{x}\big]+\mathbb{E}_{\bm{x}\sim q(\bm{x})}\bigg[-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x}) \log\frac{p(\bm{x},\mathcal{D}_T^\bm{z})}{q(\mathcal{D}_T^\bm{z}\mid \bm{x})}d \mathcal{D}_T^\bm{z} \bigg]\\&amp;\leqslant \mathbb{E}_{\bm{x}\sim q(\bm{x})}\bigg[-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x}) \log\frac{p(\bm{x},\mathcal{D}_T^\bm{z})}{q(\mathcal{D}_T^\bm{z}\mid \bm{x})}d \mathcal{D}_T^\bm{z} \bigg]\end{align}$$ 令 $\displaystyle f(\bm{\theta})=-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x}) \log\frac{p(\bm{x},\mathcal{D}_T^\bm{z})}{q(\mathcal{D}_T^\bm{z}\mid \bm{x})}d \mathcal{D}_T^\bm{z} $ 我们有 $$\begin{align}f(\bm{\theta})&amp;=-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x}) \log\frac{p(\bm{x},\mathcal{D}_T^\bm{z})}{q(\mathcal{D}_T^\bm{z}\mid \bm{x})}d \mathcal{D}_T^\bm{z}\\&amp;=-\int q(\mathcal{D}_T^\bm{z}\mid \bm{x})\log \frac{p(\bm{x}\mid \bm{z}_1)p(\bm{z}_T)\prod_{t=2}^{T}p(\bm{z}_{t-1}\mid \bm{z}_t)}{q(\bm{z}_1\mid \bm{x})\prod_{t=2}^{T}q(\bm{z}_t\mid \bm{z}_{t-1})}d \mathcal{D}_T^\bm{z}\\&amp;=-\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\bigg[\log \frac{p(\bm{x}\mid \bm{z}_1)p(\bm{z}_T)\prod_{t=2}^{T}p(\bm{z}_{t-1}\mid \bm{z}_t)}{q(\bm{z}_1\mid \bm{x})\prod_{t=2}^{T}q(\bm{z}_t\mid \bm{z}_{t-1})}\bigg]\end{align}$$ 考虑到$$\begin{align}q(\bm{z}_t\mid \bm{z}_{t-1})=q(\bm{z}_t\mid \bm{z}_{t-1},\bm{x})=\frac{q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})q(\bm{z}_t\mid \bm{x})}{q(\bm{z}_{t-1}\mid \bm{x})}\end{align}$$ 于是有$$\begin{align}f(\bm{\theta})&amp;=-\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\bigg[\log \frac{p(\bm{x}\mid \bm{z}_1)p(\bm{z}_T)\prod_{t=2}^{T}p(\bm{z}_{t-1}\mid \bm{z}_t)}{q(\bm{z}_1\mid \bm{x})\prod_{t=2}^{T}q(\bm{z}_t\mid \bm{z}_{t-1})}\bigg]\\&amp;=-\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\bigg[\log \frac{p(\bm{x}\mid \bm{z}_1)p(\bm{z}_T)\prod_{t=2}^{T}p(\bm{z}_{t-1}\mid \bm{z}_t)q(\bm{z}_{t-1}\mid \bm{x})}{q(\bm{z}_1\mid \bm{x})\prod_{t=2}^{T}q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})q(\bm{z}_t\mid \bm{x})}\bigg]\\&amp;=-\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\bigg[\log p(\bm{x}\mid \bm{z}_1)+\log \frac{p(\bm{z}_T)\cancel{\prod_{t=2}^{T}q(\bm{z}_{t-1}\mid \bm{x})}}{q(\bm{z}_T\mid \bm{x})\cancel{\prod_{t=1}^{T-1}q(\bm{z}_t\mid \bm{x})}}+\sum_{t=2}^{T}\log \frac{p(\bm{z}_{t-1}\mid \bm{z}_t)}{q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})}\bigg]\\&amp;=-\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\bigg[\log p(\bm{x}\mid \bm{z}_1)+\log \frac{p(\bm{z}_T)}{q(\bm{z}_T\mid \bm{x})}+\sum_{t=2}^{T}\log \frac{p(\bm{z}_{t-1}\mid \bm{z}_t)}{q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})}\bigg]\\&amp;=\underbrace {\mathbb{KL}\big[q(\bm{z}_T\mid \bm{x})|p(\bm{z}_T))\big]}_{\mathcal{L}_{T}}+\underbrace{\sum_{t=2}^T\mathbb{KL}\big[q(\bm{z}_{t-1}\mid \bm{z}_{t},\bm{x})|p(\bm{z}_{t-1}\mid \bm{z}_{t})\big]}_{\mathcal{L}_{1:T-1}}+\underbrace{\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\big[\log p(\bm{x}\mid \bm{z}_1)\big]}_{\mathcal{L}_0}\end{align}$$ 所以有损失函数:$$\begin{align}\mathcal{L}= \mathbb{E}_{x\sim p(\bm{x})}\bigg[\mathbb{KL}\big[q(\bm{z}_T\mid \bm{x})|p(\bm{z}_T))\big]+\sum_{t=2}^T\mathbb{KL}\big[q(\bm{z}_{t-1}\mid \bm{z}_{t},\bm{x})|p(\bm{z}_{t-1}\mid \bm{z}_{t})\big]+\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\big[\log p(\bm{x}\mid \bm{z}_1)\big]\bigg]\end{align}$$ 这里需要注意一点，我们是根据马尔可夫假设，推导出了损失函数。如果我们仔细观察损失函数的构成，其实我们很容易发现，马尔可夫假设不是必须的。这也为之后的模型提供了优化空间。 三、模型构建的细节3.1、编码模型的实现我们首先来考察一个损失函数 $\displaystyle \mathcal{L}_T$。所谓编码其实就是一个破坏的过程。我们希望通过往正常图片中逐步的加入噪声，来得到噪声。$$\begin{align}\bm{z}_t=\alpha_t\bm{z}_{t-1}+\beta_t\bm{\epsilon}_t\end{align}$$其中 $\displaystyle \bm{\epsilon}\sim \mathcal{N}(\bm{0},\bm{I})$, $\displaystyle \bm{z}_t\in \{\bm{x}\}\cup\{\bm{z}_t\}_{t=1}^T=\{\bm{z}_t\}_{t=0}^T$最终得到噪声$$\begin{align}p(\bm{z}_T)\sim \mathcal{N}(\bm{0},\bm{I})\end{align}$$ 这样有： $$\begin{align}q(\bm{z}_t\mid \bm{x})=\int q(\mathcal{D}_t^\bm{z}\mid \bm{x})d\mathcal{D}_{-t}= \mathcal{N}(\alpha_t\bm{z}_{t-1},\beta_t^2\bm{I})\end{align}$$对于 $\displaystyle q(\bm{z}_T\mid \bm{x})$, 我们展开到 $\displaystyle \bm{x}$就有 $$\begin{align}\bm{z}_T&amp;=\alpha_T\bm{z}_{T-1}+\beta_T\bm{\epsilon}_T=\alpha_T\big[\alpha_{T-1}\bm{z}_{T-2}+\beta_{T-1}\bm{\epsilon}_{T-1}\big]+\beta_T\bm{\epsilon}_T\\&amp;=\alpha_T\alpha_{T-1}\big[\alpha_{T-2}\bm{z}_{T-3}+\beta_{T-2}\bm{\epsilon}_{T-2}\big]+\alpha_T\beta_{T-1}\bm{\epsilon}_{T-1}+\beta_T\bm{\epsilon}_T\\&amp;=\prod_{t=1}^{T}\alpha_t\bm{x}+\bigg[\sum_{t=1}^{T-1}\beta_t\prod_{\tau=t}^{T}\alpha_{\tau+1}\bm{\epsilon}_t+\beta_T\bm{\epsilon}_T\bigg]\end{align}$$考察均值与方差$$\begin{align}\mathbb{E}[\bm{z}_T]&amp;=\prod_{t=1}^{T}\alpha_t\bm{x}\\\mathbb{Cov}[\bm{z}_T]&amp;=\bigg[\sum_{t=1}^{T-1}\beta_t^2\prod_{\tau=t}^{T}\alpha_{\tau+1}^2+\beta_T^2\bigg]\cdot\bm{I}\end{align}$$ 我们希望最终的 $\displaystyle \bm{z}_T \sim \mathcal{N}(\bm{0},\bm{I})$, 若 $\displaystyle \alpha_t^2+\beta_t^2=1$ 则有 $$\begin{align}\sum_{t=1}^{T-1}\beta_t^2\prod_{\tau=t}^{T}\alpha_{\tau+1}^2+\beta_T^2&amp;=\sum_{t=1}^{T-1}(1-\alpha_t^2)\prod_{\tau=t}^{T}\alpha_{\tau+1}^2+\beta_T^2\\&amp;=\sum_{t=1}^{T-1}\big[\prod_{\tau=t}^{T}\alpha_{\tau+1}^2-\prod_{\tau=t}^{T}\alpha_{\tau}^2\big]+\beta_T^2\\&amp;=\sum_{t=1}^{T-1}\big[\prod_{\tau=t}^{T}\alpha_{\tau+1}^2-\prod_{\tau=t}^{T}\alpha_{\tau}^2\big]+\beta_T^2\\&amp;=-\prod_{t=1}^{T}\alpha_t^2+\alpha_T^2+\beta_T^2\\&amp;=1 -\prod_{t=1}^{T}\alpha_t^2 \end{align}$$ 令 $\displaystyle \bar{\alpha}_T=\prod_{t=1}^{T}\alpha_t$, $\displaystyle \bar{\beta}_T =\sqrt{1-\prod_{t=1}^{T}\alpha_t^2}$, 这样就有： $$\begin{align} q(\bm{z}_T\mid \bm{x})\sim \mathcal{N}(\bar{\alpha}_T\bm{x},\bar{\beta}_T^2)= \mathcal{N}\bigg(\prod_{t=1}^{T}\alpha_t\bm{x},\bigg[1-\prod_{t=1}^{T}\alpha_t^2\bigg]\bm{I}\bigg) \end{align}$$ 我们只要设计合理的 $\displaystyle \alpha_t$使得 $\displaystyle \lim_{T\to +\infty}\prod_{t=1}^T\alpha_t=\bm{0}$即可。这样无需参数，只通过一些必要的高斯假设就实现了我们的目的 $\displaystyle \mathbb{KL}\big[q(\bm{z}_T\mid \bm{x})|p(\bm{z}_T))\big]=0$。故我们损失的第一项是零。 3.2、生成模型的实现3.2.1、$\displaystyle\mathcal{L}_{1:T-1}$考虑逆过程 $\displaystyle q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})$，根据贝叶斯定理有：$$\begin{align}q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x})&amp;=\frac{q(\bm{z}_t\mid \bm{z}_{t-1},\bm{x})q(\bm{z}_{t-1}\mid \bm{x})}{q(\bm{z}_t \mid \bm{x})}\\&amp;=\frac{\mathcal{N}(\alpha_t\bm{z}_{t-1},\beta_t^2\bm{I})\mathcal{N}(\bar{\alpha}_{t-1}\bm{x},\bar{\beta}_{t-1}^2\bm{I})}{\mathcal{N}(\bar{\alpha}_{t}\bm{x},\bar{\beta}_{t}^2\bm{I})}\\&amp; \propto \exp \sum_{i=1}^k\bigg[ -\frac{1}{2}\bigg[ \frac{(z_t-\alpha_tz_{t-1})^2}{\beta_t^2}+\frac{(z_{t-1}-\bar{\alpha}_{t-1}x)^2}{\bar{\beta}_{t-1}^2}-\frac{(z_t-\bar{\alpha}_tx)^2}{\bar{\beta}_t^2}\bigg]\bigg]\\&amp;\propto \exp \sum_{i=1}^k\bigg[-\frac{1}{2}\bigg[\bigg(\frac{\alpha_t^2}{\beta_t^2}+\frac{1}{\bar{\beta}_{t-1}^2}\bigg)z_{t-1}^2-2\bigg(\frac{\alpha_t}{\beta_t^2}z_t+\frac{\bar{\alpha}_{t-1}}{\bar{\beta}_{t-1}^2}x\bigg)z_{t-1}\bigg]\bigg]\end{align}$$根据高斯分布，我们配平方 $\displaystyle ax^2-2bx=a(x-\frac{b}{a})^2+C$可以得到： 均值有 $$\begin{align}\mathbb{E}[\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}]&amp;=\bigg(\frac{\alpha_t}{\beta_t^2}\bm{z}_t+\frac{\bar{\alpha}_{t-1}}{\bar{\beta}_{t-1}^2}\bm{x}\bigg)/\bigg(\frac{\alpha_t^2}{\beta_t^2}+\frac{1}{\bar{\beta}_{t-1}^2}\bigg)=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t\bm{z}_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\bar{\alpha}_{t-1}\bm{x}\end{align}$$ 方差有$$\begin{align}\mathbb{Cov}[\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}]&amp;=1/\bigg(\frac{\alpha_t^2}{\beta_t^2}+\frac{1}{\bar{\beta}_{t-1}^2}\bigg)\bm{I}=1/\bigg(\frac{\alpha_t^2(1-\bar{\alpha}_{t-1}^2)+\beta_t^2}{\beta_t^2(1-\bar{\alpha}_{t-1}^2)}\bigg)\bm{I}=\frac{1-\bar{\alpha}_{t-1}^2}{1-\bar{\alpha}_{t}^2}\beta_t^2=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\beta_t^2\bm{I}\end{align}$$ 也就是说：$$\begin{align}q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}) =\mathcal{N}\Bigg(\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t\bm{z}_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\bar{\alpha}_{t-1}\bm{x},\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\beta_t^2\bm{I}\Bigg)\end{align}$$ 在编码过程(变成噪声)中，我们有 $\displaystyle q(\bm{z}_t\mid \bm{x})\sim \mathcal{N}(\bar{\alpha}_t\bm{x},\bar{\beta}_t^2)= \mathcal{N}\bigg(\prod_{\tau=1}^{t}\alpha_\tau\bm{x},\bigg[1-\prod_{\tau=1}^{t}\alpha_\tau^2\bigg]\bm{I}\bigg)$ 我们改变一下表现形式 $$\begin{align}\bm{z}_t=\bar{\alpha}_t\bm{x}+\bar{\beta}_t\bm{\epsilon}\Rightarrow \bm{x}=\frac{1}{\bar{\alpha}_t}(\bm{z}_t-\bar{\beta}_t\bm{\epsilon})\end{align}$$我们带入上式，进一步考察均值$$\begin{align}\mathbb{E}[\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}]&amp;=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t\bm{z}_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\bar{\alpha}_{t-1}\bm{x}=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t\bm{z}_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\bar{\alpha}_{t-1}\frac{1}{\bar{\alpha}_t}(\bm{z}_t-\bar{\beta}_t\bm{\epsilon})\\&amp;=\bigg[\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_t}\bigg]\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}^2}\frac{\bar{\alpha}_{t-1}\bar{\beta}_t}{\bar{\alpha}_t}\bm{\epsilon}\\&amp;=\bigg[\frac{1-\bar{\alpha}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\frac{1}{\alpha_t}\bigg]\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}\\&amp;=\bigg[\frac{\alpha_t^2+\beta_t^2-\bar{\alpha}_{t-1}^2\alpha_t^2}{\bar{\beta}_{t}^2\alpha_t}\bigg]\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}\\&amp;=\bigg[\frac{1-\bar{\alpha}_{t}^2}{\bar{\beta}_{t}^2\alpha_t}\bigg]\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}\\&amp;=\frac{1}{\alpha_t}\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}\end{align}$$ 我们知道 $\displaystyle \mathbb{KL}\bigg[\mathcal{N}(\mu_1, \sigma_1^2) \big| \mathcal{N}(\mu_2, \sigma_2^2)\bigg]=\frac{(\mu_1-\mu_2)^2+\sigma_1^2}{2 \sigma_2^2}+\log \frac{\sigma_2}{\sigma_1}-\frac{1}{2}$，同时 $\displaystyle p(\bm{z}_{t-1}\mid \bm{z}_{t})=\mathcal{N}\big(\bm{\mu}_{\bm{\theta}}[\bm{z}_t,t],\bm{\Sigma}_{\bm{\theta}}[\bm{z}_t,t]\big)$ 是我们真实的生成分布， 其中 $\displaystyle \bm{\mu}_{\bm{\theta}}[\bm{z}_t,t]$ 是我们要学习的神经网络，用神经网络原因很简单图像数据太复杂。那么还剩下 $\displaystyle \bm{\Sigma}_{\bm{\theta}}[\bm{z}_t,t]$，我们注意到： $$\begin{align}q(\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}) =\mathcal{N}\Bigg(\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\alpha_t\bm{z}_t+\frac{\beta_t^2}{\bar{\beta}_{t}^2}\bar{\alpha}_{t-1}\bm{x},\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\beta_t^2\bm{I}\Bigg)\end{align}$$ 注意到损失函数 $\displaystyle \mathbb{KL}\big[q(\bm{z}_{t-1}\mid \bm{z}_{t},\bm{x})|p(\bm{z}_{t-1}\mid \bm{z}_{t})\big]$，使得最小化的最简单的方法就是令 $\displaystyle \bm{\Sigma}_{\bm{\theta}}[\bm{z}_t,t]=\sigma_t^2 \bm{I}$, $\displaystyle \sigma_t^2=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\beta_t^2$(论文作者是这么干的)。 $\displaystyle \sigma_t^2$ 其实可以取其他的值，在这之后的模型中提出的各种方案。那么有 $$\begin{align}\sum_{t=2}^T\mathbb{E}_{\bm{x}}\bigg[\mathbb{KL}\big[q(\bm{z}_{t-1}\mid \bm{z}_{t},\bm{x})|p(\bm{z}_{t-1}\mid \bm{z}_{t})\big]\bigg]&amp;=\sum_{t=2}^T\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\Bigg[\frac{1}{2\sigma_t^2}\bigg|\frac{1}{\alpha_t}\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}-\bm{\mu}_{\bm{\theta}}[\bm{z}_t,t]\bigg|^2\Bigg]+Const\end{align}$$ 变形神经网络形式如下：$\displaystyle \bm{\mu}_{\bm{\theta}}[\bm{z}_t,t]=\bigg[\frac{1}{\alpha_t}\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}_{\bm{\theta}}[\bm{z}_t,t]\bigg]$，那么可以简化得到： $$\begin{align}\mathcal{L}_{1:T-1}=\sum_{t=2}^T\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\Bigg[\mathbb{KL}\big[q(\bm{z}_{t-1}\mid \bm{z}_{t},\bm{x})|p(\bm{z}_{t-1}\mid \bm{z}_{t})\big]\Bigg]&amp;= \sum_{t=2}^T\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\Bigg[\frac{\beta_t^4}{2\sigma_t^2\alpha_t^2\bar{\beta}_{t}^2}\bigg|\bm{\epsilon}-\bm{\epsilon}_{\bm{\theta}}[\bm{z}_t,t]\bigg|^2\Bigg]+Const\end{align}$$ 3.2.2、$\displaystyle\mathcal{L}_0$接下来我们考察最后一项 $\displaystyle \mathcal{L}_0=\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\big[\log p(\bm{x}\mid \bm{z}_1)\big]$ $$\begin{align}\mathcal{L}_0&amp;=\mathbb{E}_{\bm{x}}\bigg[\mathbb{E}_{q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})}\big[\log p(\bm{x}\mid \bm{z}_1)\big]\bigg]=\mathbb{E}_{\bm{x}}\bigg[\int q(\mathcal{D}_{T}^\bm{z}\mid \bm{x})\log p(\bm{x}\mid \bm{z}_1)d\mathcal{D}_{T}^\bm{z}\bigg]\\&amp;=\mathbb{E}_{\bm{x}}\bigg[\int q(\bm{z}_1\mid \bm{x})\log p(\bm{x}\mid \bm{z}_1)d\bm{z}_1\bigg]\end{align}$$ 由于 $\displaystyle \bm{z}_1=\alpha_1\bm{x}+\beta_1\bm{\epsilon}_1$，$\displaystyle p(\bm{x}\mid \bm{z}_1)=\mathcal{N}\big(\bm{\mu}_{\bm{\theta}}[\bm{z}_1,1],\bm{\Sigma}_{\bm{\theta}}[\bm{z}_1,1]=\sigma_1^2\big)$，有 $\displaystyle \bm{\mu}_{\bm{\theta}}[\bm{z}_1,1]=\bigg[\frac{1}{\alpha_1}\bm{z}_1-\frac{\beta_1^2}{\bar{\beta}_{1}}\frac{1}{\alpha_1}\bm{\epsilon}_{\bm{\theta}}[\bm{z}_1,1]\bigg]$于是有： $$\begin{align}\mathcal{L}_0&amp; =\mathbb{E}_{t,\bm{x},\bm{\epsilon}_1}\Bigg[ \frac{1}{2\sigma_1^2}\bigg|\bm{x}-\bm{\mu}_{\bm{\theta}}[\bm{z}_1,1]\bigg|^2\Bigg]+Const\\&amp; = \mathbb{E}_{t,\bm{x},\bm{\epsilon}_1}\Bigg[\frac{1}{2\sigma_1^2}\bigg|\bm{x}-\frac{1}{\alpha_1}\big[\alpha_1\bm{x}+\beta_1\bm{\epsilon}_1\big]+\frac{\beta_1^2}{\bar{\beta}_{1}}\frac{1}{\alpha_1}\bm{\epsilon}_{\bm{\theta}}[\bm{z}_1,1]\bigg|^2\Bigg]+Const\\&amp; = \mathbb{E}_{t,\bm{x},\bm{\epsilon}_1}\Bigg[\frac{1}{2\sigma_1^2}\bigg|-\frac{\beta_1}{\alpha_1}\bm{\epsilon}_1+\frac{\beta_1^2}{\bar{\beta}_{1}}\frac{1}{\alpha_1}\bm{\epsilon}_{\bm{\theta}}[\bm{z}_1,1]\bigg|^2\Bigg]+Const\\&amp; = \mathbb{E}_{t,\bm{x},\bm{\epsilon}_1}\Bigg[\frac{\beta_1^4}{2\sigma_1^2\alpha_1^2\bar{\beta}_1^2}\bigg|\bm{\epsilon}_1-\bm{\epsilon}_{\bm{\theta}}[\bar{\alpha}_1\bm{x}+\bar{\beta}_1\bm{\epsilon}_1,1]\bigg|^2\Bigg]+Const\end{align}$$ 3.2.3、$\displaystyle\mathcal{L}_{0:T-1}$对于 $\displaystyle\mathcal{L}_{0:T-1}$，令 $\displaystyle \mathcal{L}_{t-1}=\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\bigg[\frac{\beta_t^4}{2\sigma_t^2\alpha_t^2\bar{\beta}_{t}^2}\bigg|\bm{\epsilon}-\bm{\epsilon}_{\bm{\theta}}[\bm{z}_t,t]\bigg|^2\bigg]+Const$, 我们去掉与参数无关的权重，简化得到$\displaystyle\mathcal{L}_{0:T-1}$的统一表达式： $$\begin{align}\mathcal{L}_{t-1}=\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\bigg[\bigg|\bm{\epsilon}-\bm{\epsilon}_{\bm{\theta}}[\bar{\alpha}_t\bm{x}+\bar{\beta}_t\bm{\epsilon},t]\bigg|^2\bigg]+Const\end{align}$$ 于是有损失函数： $$\begin{align} \mathcal{L}=\sum_{t=1}^T\mathbb{E}_{t,\bm{x},\bm{\epsilon}}\Bigg[\frac{\beta_t^4}{2\sigma_t^2\alpha_t^2\bar{\beta}_{t}^2}\bigg|\bm{\epsilon}-\bm{\epsilon}_{\bm{\theta}}[\bm{z}_t,t]\bigg|^2\Bigg]+Const\end{align}$$ 这样，我们有算法：$$\begin{align}\begin{array}\hline \text{Algorithm 1 Training} \\\hline 1: \text{repeat} \\2: \quad \mathbf{x}_0 \sim q\left(\mathbf{x}_0\right) \\3: \quad t \sim \mathrm{Uniform}(\{1, \ldots, T\}) \\4: \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\5: \quad \text{Take gradient descent step on} \\\qquad\qquad \nabla_\theta|\bm{\epsilon}-\bm{\epsilon}_\theta[\bar{\alpha}_t\bm{x}+\bar{\beta}_t\bm{\epsilon},t]|^2 \\6:\text{until converged} \\\hline\end{array}\end{align}$$ 对于采样生成有：$$\begin{align}\begin{array}\hline \text{Algorithm 2 Sampling} \\\hline 1: \bm{z}_T \sim \mathcal{N}(\bm{0}, \bm{I}) \\2: \text{for}\; t=T, \cdots, 1 \;\mathbf{do} \\3: \quad\bm{\xi} \sim \mathcal{N}(\bm{0}, \bm{I}) \text{ if }t&gt;1, \text{ else } \bm{\xi}=\bm{0} \\4: \quad \bm{z}_{t-1}=\frac{1}{\alpha_t}\bm{z}_t-\frac{\beta_t^2}{\bar{\beta}_{t}}\frac{1}{\alpha_t}\bm{\epsilon}_{\bm{\theta}}[\bm{z}_t,t]+\sigma_t \bm{\xi}\\5: \text{end for} \\6: \text{return }\bm{x}=\bm{z}_0\end{array}\end{align}$$ 四、评述1、推理中的一些关键动机，视乎没有加以说明。一些想法视乎是从天而降。 $\displaystyle \alpha_t^2+\beta_t^2=1$似乎还能说的过去。$\displaystyle \sigma_t^2=\frac{\bar{\beta}_{t-1}^2}{\bar{\beta}_{t}^2}\beta_t^2$似乎就动机不足了，毕竟有那么多可以取的值，不一定非要这个。 2、在逆向生成中$\displaystyle \mathbb{E}[\bm{z}_{t-1}\mid \bm{z}_t,\bm{x}]$， 做变形$\displaystyle \bm{z}_t=\bar{\alpha}_t\bm{x}+\bar{\beta}_t\bm{\epsilon}\Rightarrow \bm{x}=\frac{1}{\bar{\alpha}_t}(\bm{z}_t-\bar{\beta}_t\bm{\epsilon})$带入。就是为了消除 $\displaystyle \bm{x}$，从而使得 $\displaystyle \bm{z}_{t}\to\bm{z}_{t-1}$不依赖 $\displaystyle \bm{x}$。而这一步推导DDPM依赖了马尔可夫假设，显然限制了分布空间。如果我们去掉马尔可夫假设，我们可以得到更大自由度。 3、最终的结果是简单，一定还有更加显然的推理方式，论文DDPM的推理总觉得还是太过迂回，与不显然。接下我们将使用宋大神SDE的思想来解读这个模型。 4、DDPM以后有一系列的改进，也将一一解读，敬请关注。 Ho, J., Jain, A., &amp; Abbeel, P. (2020, December 16). Denoising Diffusion Probabilistic Models. arXiv. http://arxiv.org/abs/2006.11239. Accessed 22 March 2023Kingma, D. P., &amp; Welling, M. (2022, December 10). Auto-Encoding Variational Bayes. arXiv. http://arxiv.org/abs/1312.6114. Accessed 18 April 2023 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/1c60669bbe56769f/如果您需要引用本文，请参考：引线小白. (May. 16, 2023). 《扩散模型研究一：去噪扩散概率模型DDPM》[Blog post]. Retrieved from https://www.limoncc.com/post/1c60669bbe56769f@online{limoncc-1c60669bbe56769f,title={扩散模型研究一：去噪扩散概率模型DDPM},author={引线小白},year={2023},month={May},date={16},url={\url{https://www.limoncc.com/post/1c60669bbe56769f}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>生成模型</tag>
        <tag>去噪扩散概率模型</tag>
        <tag>DDPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[戏说rust二_细节篇]]></title>
    <url>%2Fpost%2F54839b6bfa303599%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/54839b6bfa303599/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 上篇说道rust的核心机制 1、内存所有制2、借用规则3、生命周期规则 本篇就来填充一些关键细节，本文的目的就是呈现这些细节，让这些规则变的显然。需要唠叨的是，静态语言的核心是类型系统(Type System)，尤其是rust这种强静态类型，类型系统(Type System)更是核心的核心。rust为了实现编程世界的目标，围绕类型做了很多工作。 一、rust的类型类型是静态语言的需要了解的核心。但通常学习起来都比较繁琐。rust的类型和其他语言有所不同。考虑到rust的内存所有权交易问题。类型的划分既考虑了类型本身的特点又考虑了所有权的交易特性。 1.1、标量类型(Scalar Types )Rust 有四种主要的基本类型：整数、浮点数、布尔值和字符。rust又叫它们标量类型。其他所有类型都是基于这个四大基本类型。 整数又分为有符号整数 (i8, i16, i32, i64, isize)、 无符号整数 (u8, u16, u32, u64, usize)。它们的能力范围如下： 长度 有符号类型=[$-2^{n - 1}$ , $2^{n - 1} - 1$] 无符号类型=[$0$, $2^{n}-1$] 8 位 i8=[-128,127] u8=[0,255] 16 位 i16=[-32768,32767] u16=[0,65535] 32 位 i32 =[-2147483648,2147483647] u32=[0,4294967295] 64 位 i64=[$-9.2\times 10^{18}$,$9.2\times 10^{18}$] u64=[0,$18.4\times 10^{18}$] 128 位 i128=[$-1.7\times 10^{38}$,$1.7\times 10^{38}$] u128=[0,$3.4\times 10^{38}$] 浮点数根据 IEEE-754 标准表示。 f32 类型是单精度浮点数， f64 类型是双精度。这里就不做过多解释了，提供两个玩耍链接： 1、从十进制浮点到 32 位和 64 位十六进制表示形式2、从 32 位十六进制表示形式到十进制浮点数 然后提供几个案例1234567891011fn main() &#123; let a = (0.1_f64 + 0.2 - 0.3).abs(); println!("&#123;a&#125;"); let b = (0.1_f32 + 0.2 - 0.3).abs(); println!("&#123;b&#125;"); let c = (0.123_f32 as f64 - 0.123_f64).abs(); println!("&#123;c&#125;");&#125;// 0.00000000000000005551115123125783// 0// 0.0000000033974647539736225 布尔值 12345678910111213141516fn main() &#123; let a = false; let b = true; let c = a &amp; b; println!("与a &amp; b==&gt;&#123;c&#125;"); let c = a | b; println!("或a | b==&gt;&#123;c&#125;"); let c = !a; println!("非!a==&gt;&#123;c&#125;"); let c = a ^ b; println!("异或a ^ b==&gt;&#123;c&#125;");&#125;// 与a &amp; b==&gt;false// 或a | b==&gt;true// 非!a==&gt;true// 异或a ^ b==&gt;true 字符类型，注意是单引号 1234567fn main() &#123; let a:char = '我'; let b:char = '😍'; let c:char = '你'; println!("&#123;a&#125;&#123;b&#125;&#123;c&#125;")&#125;// 我😍你 最后最重要的一点，标量类型所有权交易都是克隆(Clone)，因为它们都是使用的栈内存。如果还不熟悉这一点请看第一篇《戏说rust一_入门篇》 1.2、复合类型(Compound Types)Rust 有两种原始复合类型：元组(tuple)和数组(array) 1.2.1、元组(tuple)元组(tuple)是由多种类型组合到一起形成的，因此它是复合类型，元组的长度是固定的，元组中元素的顺序也是固定的。1234567fn main() &#123; let a:(i32,f64,u8) = (520,3.14,1); let b = a.2; println!("元组&#123;a:?&#125;的第三位数是&#123;b&#125;")&#125;// 元组(520, 3.14, 1)的第三位数是1 如果下面我们来考察一下元组的所有权交易 123456789fn main() &#123; let a: (i32, f64, u8) = (500, 6.4, 1); let b = a; println!("a的内存地址&#123;:p&#125;", &amp;b); println!("b的内存地址&#123;:p&#125;", &amp;a);&#125;// a的内存地址0x16d6e6920// b的内存地址0x16d6e6910 当元组的元素都是基本类型(四大标量类型)的时候，所有权交易(非借用赋值)都是使用的克隆(Clone)。因为基本类型的所有权交易就是克隆(Clone)，下面来点迷之操作，如果元组的元素类型既是标量类型又是非标量类型会怎么样？ 1234567891011121314151617181920fn main() &#123; let a: (i32, f64, String) = (500, 6.4, "我爱你".to_string()); println!("a==&gt;&#123;:p&#125;", &amp;a); println!("a.0==&gt;&#123;:p&#125;", &amp;a.0); println!("a.1==&gt;&#123;:p&#125;", &amp;a.1); println!("a.2==&gt;&#123;:p&#125;", a.2.as_ptr()); let b = a; println!("a==&gt;&#123;:p&#125;", &amp;b); println!("b.0==&gt;&#123;:p&#125;", &amp;b.0); println!("b.1==&gt;&#123;:p&#125;", &amp;b.1); println!("b.2==&gt;&#123;:p&#125;", b.2.as_ptr());&#125;// a==&gt;0x16cf926d0// a.0==&gt;0x16cf926d8// a.1==&gt;0x16cf926d0// a.2==&gt;0x600000fec060// a==&gt;0x16cf92830// b.0==&gt;0x16cf92838// b.1==&gt;0x16cf92830// b.2==&gt;0x600000fec060 发现rust依然追寻了规则，标量类型的所有权交易是克隆(Clone)，其他类型的所有权交易是转移(move)。所以a前2个元素是标量类型就克隆，第三元素是字符串类型就是转移。 1.2.2、堆内存交易转移(move)和栈内存交易克隆(clone)的本质那么这背后的作用机制到底是什么？我们来看一个报错信息：1234567891011121314151617181920212223242526fn main() &#123; let a: (i32, f64, String) = (500, 6.4, "我爱你".to_string()); println!("a.2==&gt;&#123;:p&#125;", a.2.as_ptr()); let b = a; println!("b.2==&gt;&#123;:p&#125;", b.2.as_ptr()); println!("&#123;a:?&#125;")&#125;error[E0382]: borrow of moved value: `a` --&gt; src/main.rs:10:15 |6 | let a: (i32, f64, String) = (500, 6.4, "我爱你".to_string()); | - move occurs because `a` has type `(i32, f64, String)`, which does not implement the `Copy` trait7 | println!("a.2==&gt;&#123;:p&#125;", a.2.as_ptr());8 | let b = a; | - value moved here9 | println!("b.2==&gt;&#123;:p&#125;", b.2.as_ptr());10 | println!("&#123;a:?&#125;") | ^^^^^ value borrowed here after move | = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)help: consider cloning the value if the performance cost is acceptable |8 | let b = a.clone(); | ++++++++ 其中有一段信息： move occurs because a has type (i32, f64, String), which does not implement the Copy trait转移发生是因为a的类型(i32, f64, String)没有实现复制特征(Copy trait) 这里出现了一个新概念trait，我们暂且不表。顾名思义即可。我们来把这句话说的人性话点。就是说类型没有复制的特征(或者功能，或者方法)，所有权交易就会使用转移move。如果实现了，显然基本类型(四大标量类型)是实现了Copy特征的。于是所有权交易是克隆(Clone)。 回忆一下这个原则：堆内存交易转移(move)和栈内存交易克隆(clone)。其背后就是使用栈内存的类型rust都实现了一种叫Copy的trait，自然就是使用克隆(clone)；而使用堆内存的类型没有这个特征于是就会触发转移(move)。 1.2.3、数组(array)拥有多个值集合的另一种方法是使用数组(array)。与元组不同，数组的每个元素都必须具有相同的类型。与其他一些语言中的数组不同，Rust 中的数组具有固定的长度。 123456789fn main() &#123; let a:[i32;4] =[1,2,3,4]; let b:[i32;4] = a; println!("a的内存地址&#123;:p&#125;",&amp;a); println!("b的内存地址&#123;:p&#125;",&amp;b);&#125;// a的内存地址0x16b8a6910// b的内存地址0x16b8a6920 我来看看元素是非基本类型的情况 123456789101112131415161718fn main() &#123; let a: [String; 2] = std::array::from_fn(|index| format!("第&#123;index&#125;爱你中国")); println!("a的内存地址&#123;:p&#125;", &amp;a); println!("a0的内存地址&#123;:p&#125;", a[0].as_ptr()); println!("a1的内存地址&#123;:p&#125;", a[1].as_ptr()); let b = a; println!("b的内存地址&#123;:p&#125;", &amp;b); println!("&#123;b:?&#125;"); println!("b0的内存地址&#123;:p&#125;", b[0].as_ptr()); println!("b1的内存地址&#123;:p&#125;", b[1].as_ptr());&#125;// a的内存地址0x16ce8a708// a0的内存地址0x6000037891c0// a1的内存地址0x6000037891e0// b的内存地址0x16ce8a810// ["第0爱你中国", "第1爱你中国"]// b0的内存地址0x6000037891c0// b1的内存地址0x6000037891e0 还是那句老话标量类型的所有权交易是克隆(Clone)，其他类型的所有权交易是转移(move)；或者说堆内存交易转移(move)和栈内存交易克隆(clone)；或者说实现了Copy的trait的类型内存交易使用克隆(clone)，否则转移(move)。 注意非标量类型或者说是非基本类型的转移(move)操作是指的内部变量转移(move)操作。它本身的内存地址是改变了的。 1.3、自定义类型：结构体(struct)和枚举(enum)1.3.1、结构体(struct)一个结构体(struct)是这样的：通过关键字 struct 定义名称，内部有字段。 12345678910111213141516fn main() &#123; struct User &#123; name: String, age: u8, email: String, address: String, &#125; let a = User &#123; name: "李四".to_string(), age: 18, email: "xxx@mail.com".to_string(), address: "广州天河区".to_string(), &#125;; println!("&#123;&#125;|&#123;&#125;|&#123;&#125;|&#123;&#125;",a.name,a.age,a.email,a.address)&#125;// 李四|18|xxx@mail.com|广州天河区 这个时候你可能疑惑了？为啥不直接println!。可以的，恭喜你又引入了新概念。1234567891011121314error[E0277]: `User` doesn't implement `Debug` --&gt; src/main.rs:18:15 |18 | println!("&#123;a:?&#125;") | ^^^^^ `User` cannot be formatted using `&#123;:?&#125;` | = help: the trait `Debug` is not implemented for `User` = note: add `#[derive(Debug)]` to `User` or manually `impl Debug for User` = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)help: consider annotating `User` with `#[derive(Debug)]` |6 + #[derive(Debug)]7 | struct User &#123; | 有出现了trait(特征)，我们这里暂时不解释。提示说要格式化打印需要有Debug的trait。我才学struct。什么Debug的trait奇奇怪怪的。接着编译器它说可以再你的结构体User上面加#[derive(Debug)]。又懵圈了。derive是派生的意思，那么 #[derive(Debug)]==&gt;翻译是派生Debug这个trait 那#是什么意思。联想一下css里面似乎表示属性。嗯，那么就是 #[derive(Debug)]==&gt;翻译是属性：派生Debug这个trait 没错rust管这个操作叫做Derived Traits(派生特征)，通过属性#这个操作添加。至于本质我们暂且不表。形式化理解先。这里再说一个rust属性 #[allow(dead_code)]==&gt;翻译是属性：允许未使用(死亡代码) 也就说，rust中没有使用的变量，rust称为dead_code。形式化理解先。我们继续。有时候，抑制好奇心是我们能够前进的动力。 1234567891011121314151617181920212223fn main() &#123; #[allow(dead_code)] #[derive(Debug)] struct User &#123; name: String, age: u8, email: String, address: String, &#125; let a = User &#123; name: "李四".to_string(), age: 18, email: "xxx@mail.com".to_string(), address: "广州天河区".to_string(), &#125;; println!("&#123;a:#?&#125;")&#125;// User &#123;// name: "李四",// age: 18,// email: "xxx@mail.com",// address: "广州天河区",// &#125; 我们来看看结构体的所有权交易情况12345678910111213141516171819202122232425262728293031323334353637fn main() &#123; #[allow(dead_code)] #[derive(Debug)] struct User &#123; name: String, age: u8, email: String, address: String, &#125; let a = User &#123; name: "李四".to_string(), age: 18, email: "xxx@mail.com".to_string(), address: "广州天河区".to_string(), &#125;; println!("a==&gt;&#123;:p&#125;", &amp;a); println!("a.name==&gt;&#123;:p&#125;", a.name.as_ptr()); println!("a.age==&gt;&#123;:p&#125;", &amp;(a.age)); println!("a.email==&gt;&#123;:p&#125;", a.email.as_ptr()); println!("a.address==&gt;&#123;:p&#125;", a.address.as_ptr()); let b = a; println!("b==&gt;&#123;:p&#125;", &amp;b); println!("b.name==&gt;&#123;:p&#125;", b.name.as_ptr()); println!("b.age==&gt;&#123;:p&#125;", &amp;(b.age)); println!("b.email==&gt;&#123;:p&#125;", b.email.as_ptr()); println!("b.address==&gt;&#123;:p&#125;", b.address.as_ptr());&#125;// a==&gt;0x16b8fe580// a.name==&gt;0x600003264060// a.age==&gt;0x16b8fe5c8// a.email==&gt;0x600003264070// a.address==&gt;0x600003264080// b==&gt;0x16b8fe780// b.name==&gt;0x600003264060// b.age==&gt;0x16b8fe7c8// b.email==&gt;0x600003264070// b.address==&gt;0x600003264080 依然还是那句老话标量类型的所有权交易是克隆(Clone)，其他类型的所有权交易是转移(move)；或者说堆内存交易转移(move)和栈内存交易克隆(clone)；或者说实现了Copy的trait的类型内存交易使用克隆(clone)，否则转移(move)。 不过我们需要注意的是，非标量类型或者说是非基本类型的转移(move)操作是指的内部变量转移(move)操作。它本身的内存地址是改变了的。 1.3.2、枚举(enum)枚举(enum)为您提供了一种表示值是一组可能的值之一的方法。和大多数语言定义的差不多。123456789101112131415fn main() &#123; #[allow(dead_code)] #[derive(Debug)] enum Vehicle &#123; Car, Bus, Truck, &#125; let a = Vehicle::Car; let b = Vehicle::Bus; println!("&#123;a:?&#125;"); println!("&#123;b:?&#125;");&#125;// Car// Bus 这里主要关注的一个特殊的枚举Option，当然你如果熟悉scala对这个概念应该不陌生。结合模式匹配，用在None值处理上是比较优雅的。如下 123456789fn main() &#123; let a = Option::from(1); let b = match a &#123; Some(x)=&gt;format!("我是&#123;x&#125;哦。"), None=&gt;format!("我是None哦。") &#125;; println!("&#123;b&#125;")&#125;// 我是1哦。 也就说枚举Option就两个元素Some(x)和None。 1.4、集合类型(collections)1.4.1、向量(Vector)向量(Vector)允许您在单个数据结构中存储多个值，它的长度是可变的。该结构将所有值彼此相邻地放在内存中。向量只能存储相同类型的值。向量(Vector)应该被作为一个整体对待，这与前面的元组、数组、结构体、枚举是不一样的。因为向量(Vector)只能被分配到堆内存上，为何？皆因长度可变。而前述类型并不一定，要看里面的具体元素情况。 下面举几个例子，自行体会： 123456789101112fn main() &#123; let a = std::array::from_fn::&lt;String, 2, _&gt;(|index| format!("第&#123;index&#125;爱你中国")).to_vec(); println!("a==&gt;&#123;:p&#125;", a.as_ptr()); println!("a0==&gt;&#123;:p&#125;", a[0].as_ptr()); let b = a; println!("b==&gt;&#123;:p&#125;", b.as_ptr()); println!("b0==&gt;&#123;:p&#125;", b[0].as_ptr());&#125;// a==&gt;0x600000044270// a0==&gt;0x600000c40060// b==&gt;0x600000044270// b0==&gt;0x600000c40060 123456789101112fn main() &#123; let a = vec![1,2,3,4,5,6,7,8]; println!("a==&gt;&#123;:p&#125;", a.as_ptr()); println!("a0==&gt;&#123;:p&#125;", &amp;(a[0])); let b = a; println!("b==&gt;&#123;:p&#125;", b.as_ptr()); println!("b0==&gt;&#123;:p&#125;", &amp;(b[0]));&#125;// a==&gt;0x60000165d1c0// a0==&gt;0x60000165d1c0// b==&gt;0x60000165d1c0// b0==&gt;0x60000165d1c0 1.4.2、字符串(String)字符串到底是什么，直接上源码。String 是 Vec\&lt;u8> 上的包装器。源码又很多通过rust属性的派生操作，暂且忽略。123456#[derive(PartialEq, PartialOrd, Eq, Ord)]#[stable(feature = "rust1", since = "1.0.0")]#[cfg_attr(not(test), lang = "String")]pub struct String &#123; vec: Vec&lt;u8&gt;,&#125; 对于字符串，关于 UTF-8 的一点是从 Rust 的角度来看，实际上有三种相关的方式来看待字符串：字节(bytes)、标量值(scalar values)和字素簇(grapheme clusters)。 123456789101112use hex;fn main() &#123; let a = "我爱你中国".to_string(); let b = &amp;a.as_bytes(); println!("b长度是&#123;&#125;内容是&#123;b:?&#125;",b.len()); let c = &amp;a.chars(); println!("&#123;c:?&#125;"); println!("&#123;:?&#125;",hex::encode(a.as_bytes()));&#125;// b长度是15内容是[230, 136, 145, 231, 136, 177, 228, 189, 160, 228, 184, 173, 229, 155, 189]// Chars(['我', '爱', '你', '中', '国'])// "e68891e788b1e4bda0e4b8ade59bbd" 内存所有权就不演示了。其他操作去查API 1.4.3、哈希映射(HashMap)最后一个常见的集合是哈希映射。类型HashMap使用哈希函数存储类型 K 的键到类型 V 的值的映射，该函数确定如何将这些键和值放入内存。许多编程语言都支持这种数据结构，但它们通常使用不同的名称，例如哈希、映射、对象、哈希表、字典或关联数组，仅举几例演示如何创建哈希映射。注意_表示任意，这一点和scala里面的意思差不多。 1234567891011121314151617fn main() &#123; use std::collections::HashMap; let mut my_car = HashMap::new(); my_car.insert("宝马车", 1); my_car.insert("比亚迪车", 2); my_car.insert("老爷车", 1); println!("&#123;:?&#125;", my_car); let teams_list = vec![ ("小白".to_string(), 18), ("小黑".to_string(), 22), ("小美".to_string(), 19), ]; let teams_map: HashMap&lt;_, _&gt; = teams_list.into_iter().collect(); println!("&#123;:?&#125;", teams_map)&#125; 哈希映射的所有权和复合类型一致，看元素。 二、rust的抽象类型：泛型(generics)和特征(traits)每种编程语言都有有效处理概念重复的工具。静态语言除了函数，还有泛型(generics)和特征(traits)、类(class)、接口(interface)。由于继承在代码实践中被很多人吐槽。现在一般都用特征(traits)、或者接口(interface)来解决。 2.1、泛型(generics)我们来看看没有泛型(generics)情况，强类型语言无法解决的问题。异质类型无法一起计算。 1234567891011121314151617181920fn main() &#123; fn num_add(a: i32, b: i32) -&gt; i32 &#123; a + b &#125; let c = num_add(1.2,2); println!("&#123;c&#125;");&#125;error[E0308]: mismatched types --&gt; src/main.rs:10:21 |10 | let c = num_add(1.2,2); | ------- ^^^ expected `i32`, found floating-point number | | | arguments to this function are incorrect |note: function defined here --&gt; src/main.rs:7:8 |7 | fn num_add(a: i32, b: i32) -&gt; i32 &#123; | ^^^^^^^ ------ 你要解决这个问题，那么你只能写多个函数1234567891011121314151617181920fn main() &#123; fn num_add_i32(a: i32, b: i32) -&gt; i32 &#123; a + b &#125; fn num_add_i8(a: i8, b: i8) -&gt; i8 &#123; a + b &#125; fn num_add_f32(a: f32, b: i32) -&gt; f32 &#123; a + b as f32 &#125; let c = num_add_i32(1, 2); println!("&#123;c&#125;"); let c = num_add_i8(1, 2); println!("&#123;c&#125;"); let c = num_add_f32(1.2, 2); println!("&#123;c&#125;");&#125;// 3// 3// 3.2 这样实现是不是太繁琐。我们需要能够代表所有i32、i8、f32等一个东东。rust管这个叫泛型(generics)，语法如下：123456789fn main() &#123; fn num_add&lt;T&gt;(a: T, b: T) -&gt; T &#123; a+b &#125; let c = num_add(1.2, 3.3); println!("&#123;c&#125;")&#125;// 4.5 那么如果是f32+i32呢？那就多加一个泛型和几个约束： 123456789101112fn main() &#123; use std::*; fn num_add&lt;'t,'l,T: ops::Add&lt;Output=T&gt; + From&lt;U&gt;, U&gt;(a:T, b: U) -&gt; T &#123; a + b.into() &#125; let c:f64 = num_add(1.1, 3); println!("&#123;c&#125;"); let c:i32 = num_add(520, 3); println!("&#123;c&#125;");&#125;4.1523 又多了点奇怪的语法。我们来分析一下这个函数签名(就是理解为函数的写法就行)1fn num_add\&lt;T: ops::Add\&lt;Output=T\&gt; + From\&lt;U\&gt;, U\&gt;(a: T, b: U) -&gt; T 1、简单理解fn是关键字2、num_add是函数名3、&lt;&gt;称为尖括号。里面当然是写尖括号参数，目前我们知道有生命周期、泛型两个。其中生命周期我们使用‘t: 单引号和小写字母。泛型我们使用T大写字母。4、尖括号的第一个参数1T: ops::Add&lt;Output=T&gt; + From&lt;U&gt; 冒号表示对泛型约束。多个约束使用 + 相连。于是泛型又两个约束：1、要有add的操作；2、要能使得泛型U能转化为泛型T。4、尖括号的第二个参数是泛型U5、()称为圆括号。里面当然是写圆括号参数，也就是我们输入参数，第一个参数a有泛型T的约束，第二参数b有泛型U的约束6、 -&gt; T箭头表示输出类型，就是泛型T。 是不是有点复杂，木有办法要实现自由内存的自由联合确实要实现高水平的治理水平。上例没有生命周期，那么我们加个生命周期，来点复杂的。 1234567891011121314151617181920fn main() &#123; fn longest&lt;'t, 's, T: PartialOrd + ?Sized,U:?Sized&gt;(x: &amp;'t T, y: &amp;'t U) -&gt; &amp;'s T where 't: 's,&amp;'t T: From&lt;&amp;'t U&gt;&#123; if x &gt; y.into() &#123; x &#125; else &#123; y.into() &#125; &#125; let x = "作用域原则".to_string(); let y = "生命周期的灵活性"; let a = &amp;x[..]; let b = y; let c = longest(a, b); println!("&#123;c&#125;"); let c = longest(&amp;8, &amp;6); println!("&#123;c&#125;");&#125;// 生命周期的灵活性// 8 这里解释一下?Sized: Types with a constant size known at compile time.All type parameters have an implicit bound of Sized. The special syntax ?Sized can be used to remove this bound if it’s not appropriate.类型，其大小在编译时已知。 所有类型参数都有一个size的隐式边界。如果这个边界不合适，可以使用特殊的语法?Sized来删除它。 解释一下PartialOrd: Trait for types that form a partial order .The lt, le, gt, and ge methods of this trait can be called using the &lt;, &lt;=, &gt;, and &gt;= operators, respectively.构成偏序类型的特征。 这个trait的lt、le、gt和ge方法可以分别使用&lt;、&lt;=、&gt;和&gt;=操作符调用。 2.2、特征(traits)2.2.1、方法语法(Method Syntax)好了终于讲到特征了，在讲特征之前。我们需要了解一下类型的方法。关键词是impl。 impl仅适用于结构体、枚举、union和trait对象(impl only structs, enums, unions and trait objects)： 我们来实现一个二维坐标象限显示的操作。12345678910111213141516171819202122232425262728293031323334fn main() &#123; #[derive(Debug)] struct Coordinate &#123; x: f64, y: f64, &#125; impl Coordinate &#123; fn show(&amp;self) -&gt; String &#123; format!("===========\n\ 本点在第&#123;&#125;象限\nx是&#123;:.4&#125;\ny是&#123;:.4&#125;\ \n===========", self.quadrant(), self.x, self.y) &#125; fn quadrant(&amp;self) -&gt; i8 &#123; let res = match (self.x &gt; 0.0, self.y &gt; 0.0) &#123; (true, true) =&gt; 1, (false, true) =&gt; 2, (false, false) =&gt; 3, (true, false) =&gt; 4, &#125;; return res; &#125; &#125; let a = Coordinate &#123; x: 1.2, y: 2.0 &#125;; let b = a.show(); println!("&#123;b&#125;"); println!("&#123;a:?&#125;")&#125;// ===========// 本点在第1象限// x是1.2000// y是2.0000// ===========// Coordinate &#123; x: 1.2, y: 2.0 &#125; 对于方法，其它语言中所有定义都在 class 中，但是 Rust 的对象定义和方法定义是分离的，这种数据和使用分离的方式，会给予使用者极高的灵活度。 类型既然可以用泛型抽象，那么方法可不可以抽象。当然可以。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071fn main() &#123; pub trait Show &#123; fn show(&amp;self) -&gt; String; fn quadrant(&amp;self) -&gt; i8; &#125; #[derive(Debug)] struct Coordinate2d &#123; x: f64, y: f64, &#125; #[derive(Debug)] struct Coordinate3d &#123; x: f64, y: f64, z: f64, &#125; impl Show for Coordinate2d &#123; fn show(&amp;self) -&gt; String &#123; format!("====\n本点在第&#123;&#125;象限\nx是&#123;:.4&#125;\ny是&#123;:.4&#125;" , self.quadrant(), self.x, self.y) &#125; fn quadrant(&amp;self) -&gt; i8 &#123; let res = match (self.x &gt; 0.0, self.y &gt; 0.0) &#123; (true, true) =&gt; 1, (false, true) =&gt; 2, (false, false) =&gt; 3, (true, false) =&gt; 4, &#125;; return res; &#125; &#125; impl Show for Coordinate3d &#123; fn show(&amp;self) -&gt; String &#123; format!("====\n本点在第&#123;&#125;象限\nx是&#123;:.4&#125;\ny是&#123;:.4&#125;\nz是&#123;:.4&#125;" , self.quadrant(), self.x, self.y, self.z) &#125; fn quadrant(&amp;self) -&gt; i8 &#123; let res = match (self.x &gt; 0.0, self.y &gt; 0.0, self.z &gt; 0.0) &#123; (true, true, true) =&gt; 1, (false, true, true) =&gt; 2, (false, false, true) =&gt; 3, (true, false, true) =&gt; 4, (true, true, false) =&gt; 5, (false, true, false) =&gt; 6, (false, false, false) =&gt; 7, (true, false, false) =&gt; 8, &#125;; return res; &#125; &#125; fn notify(item: &amp;impl Show) &#123; println!("提示如下&#123;&#125;", item.show()); &#125; let a = Coordinate2d&#123;x:1.2,y:2.4&#125;; let b = Coordinate3d&#123;x:1.2,y:-3.2,z:3.2&#125;; notify(&amp;a); notify(&amp;b);&#125;// 提示如下====// 本点在第1象限// x是1.2000// y是2.4000// 提示如下====// 本点在第4象限// x是1.2000// y是-3.2000// z是3.2000 自行体会一下。 三、评述这里在啰嗦一下特征(traits)和类(class)的区别。数据聚集在一起可以抽象。方法聚集在一起也可以抽象。那么到底哪个是世界的本质。这就涉及集合论和范畴论哪个才是数学的基础的问题。 一开始数学的基础是集合论，但随着数学的发展将事物关系抽象出来，进而作为一切的基础能大大简化分析，而这就是范畴论。我们不在追寻事物本身是什么。我们更关心事物的关系是什么，关系才是本质的东西。而哲学上也基本是这样的一个套路。唯物主义认为人的本质是一切社会关系的总和。也就说我们并不关心人本来是什么，有没眼睛，会不会做30个引体向上，有没有漂亮脸蛋… 这些我们都不关心。我们只关心这个人在社会中的各种关系。也就说无论是阿猫阿狗，还是人工智能，如果它也有这样的关系总和，我们就称它为人。 这反应到编程上，就是大概你应该听说过的鸭子类型：当你看到一只鸟走起来像鸭子，游泳起来鸭子，叫起来也像鸭子，那么这只鸟就被称为鸭子类型。是不是觉得编程语言实际蕴含着人工智能觉醒的基础哈。 所以class将数据和方法集聚，然后使用继承来解决方法抽象的问题是不自然的。它们应该分开。而这就是泛型(generics)和特征(traits)。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/54839b6bfa303599/如果您需要引用本文，请参考：引线小白. (Sep. 15, 2022). 《戏说rust二_细节篇》[Blog post]. Retrieved from https://www.limoncc.com/post/54839b6bfa303599@online{limoncc-54839b6bfa303599,title={戏说rust二_细节篇},author={引线小白},year={2022},month={Sep},date={15},url={\url{https://www.limoncc.com/post/54839b6bfa303599}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>rust</tag>
        <tag>rust语言</tag>
        <tag>工程实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[戏说rust一_入门篇]]></title>
    <url>%2Fpost%2Fafa185b9b060e00e%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/afa185b9b060e00e/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、引言rust大部分教程，包括网上各类文章，和实体书籍都写的极不通俗。可能是为了严谨，大家不敢放开了讲。也可能是各路大神学的太多，太深入了忘记了通俗。本文将用戏说的手法，带领大家快速入门，极大降低入门难度。 阅读本系列的前置条件： 1、会编程，懂编程的基本概念即可。不需要懂什么c++，也不需要懂什么其他复杂概念，懂的先忘记，个人认为：原生式学习比迁移式学习要好。2、rust环境已经准备好了，笔者最讨厌的就是准备各种开发环境了。闹心，还不讨好。。。所以本文不讲这个。3、本文只注重建立基本框架。至于框架的基本内容会给出指引，具体还是去查文档。 二、初见rust人生若只如初见，这句话对于rust而言，第一眼就是难受感觉。我们来看一下 hello word 1234fn main() &#123; let hello: &amp;str = "hello world"; println!("&#123;hello&#125;")&#125; 竟然如此难受，我们就解读一下，让它看上去不那么难受，改善一下初见的感觉，增进好感。 2.1、第一难受的地方fn main() {}第一难受的地方：fn main() {}是静态语言的标配。所谓静态语言你可以理解为必须为变量表注类型。而main函数通常是这类语言编译的入口(因为要编译，语句太多，到底从哪行开始？大家约定从main函数开始)。对和python、js这类不同，rust需要编译成二进制文件，从而不依赖运行环境。 这里好人做到底，照顾基础薄弱的同学，fn是function缩写, 是关键字。也就说一个rust程序必须要有一个main函数来作为解析编译的入口。另外记住rust的风格，或者足够rusty：少即是多，能用2个字符表示函数的意思，rust绝对不会使用更多的字符。 2.2、第二难受的地方let hello: &amp;str = “hello world”;第二难受的地方：let hello: &amp;str = “hello world”; 稍微懂点编程，应该知道这是个赋值语句。嗯哈，需要指出赋值语句这个说法不准确，rust管这个操作叫做变量绑定。我们稍后会解释，这里暂且搁置。这句最难受的莫过于&amp;str，如果说是字符串那为啥不是str。加一个&amp;看着难受不说，也觉得奇奇怪怪，语法丑陋就是了。 言归正传，&amp;在rust里面表示引用(reference)，聪明的读者马上就说了，那let hello: &amp;str = “hello world”;这句话就是 变量hello绑定一个”hello world”的引用字符串 对也不对，rust管&amp;str叫做引用字符串切片，引用字符串是&amp;String。具体它们有什么区别，我们暂且不表。看下面一段程序123456789101112131415fn main() &#123; let hello: String = "世界，你好哇".to_string(); // let hello = String::from("世界，你好哇"); let ref_hello: &amp;String = &amp;hello; // 一个中文一般占用3的char，所以长度是18 let ref_hello_slice: &amp;str = &amp;hello[0..18]; println!("\ hello:&#123;hello&#125;\n\ ref_hello:&#123;ref_hello&#125;\n\ ref_hello_slice:&#123;ref_hello_slice&#125;\ ")&#125;// hello:世界，你好哇// ref_hello:世界，你好哇// ref_hello_slice:世界，你好哇 对于let hello: String = “世界，你好哇”.to_string()顾名思义就好了, 于是let hello: &amp;str = “hello world”;这句话就是 变量hello绑定一个”hello world”的引用字符串切片 好了, 我们大致解释了一下第二个奇怪的地方。当然这也引发的很多问题，为啥rust里面整个字符串好像有点复杂。首先我们要承认人类的语言是复杂的，把它存储在计算机中也是复杂的。我们到底怎么在内存中存储内容？对于这个问题，rust作为系统级的语言提出了很精彩的解决方案。 2.3、第三难受的地方println!(“{hello}”)第三难受的地方：println!(“{hello}”)。靠为什么又出现了一个奇怪的 ! 。rust语法太丑，奇奇怪怪。这个 ! 在rust中叫做宏，啥，听不懂… 又是一些奇奇怪怪的东西。 我们先来看个例子，打印一个字符串的前5个字符的大写, 但是报错了： 12345678910111213fn main() &#123; fn show(text:String,top_num:i8 = 5)&#123; let text_split:Vec&lt;_&gt; = text.split("").collect(); let top_text = &amp;text_split[1..top_num+1].concat().to_uppercase(); println!("&#123;top_text:?&#125;") &#125; show("hello world".to_string())&#125;// error: expected parameter name, found `=`// --&gt; src/main.rs:4:36// |// 4 | fn show(text:String,top_num:i8 = 5)&#123;// | ^ expected parameter name 对没错，rust不支持默认参数，我们可以通过所谓的宏来实现 12345678910111213141516171819 fn main() &#123; fn show(text: String, top_num: usize) &#123; let text_split: Vec&lt;_&gt; = text.split("").collect(); let top_text = &amp;text_split[1..top_num + 1].concat().to_uppercase(); println!("&#123;top_text:?&#125;") &#125; macro_rules! show &#123; ($a: expr) =&gt; &#123; show($a, 5) &#125;; ($a: expr,$b:expr)=&gt;&#123; show($a, $b) &#125; &#125; show!("hello world".to_string()); show!("hello world".to_string(),5);&#125;// "HELLO"// "HELLO" 暂且不理会这些奇奇怪怪的语法，所谓宏，就是做了一点额外的操作，所以println!(“{hello}”)就调用了宏，可以通过查看源码发现这一点： 123456789101112#[macro_export]#[stable(feature = "rust1", since = "1.0.0")]#[cfg_attr(not(test), rustc_diagnostic_item = "println_macro")]#[allow_internal_unstable(print_internals, format_args_nl)]macro_rules! println &#123; () =&gt; &#123; $crate::print!("\n") &#125;; ($($arg:tt)*) =&gt; &#123;&#123; $crate::io::_print($crate::format_args_nl!($($arg)*)); &#125;&#125;;&#125; 先请忽略这其中奇奇怪怪的点，我们当前的目的只是形式化地弄懂hello world。目前看来，要把rust的hello word将清楚就涉及很多概念与语法。 2.4、总结一下要形式化地粗浅地看懂rust的hello word。你需要知道如下概念： 1、静态语言都有一个main函数入口2、&amp;在rust里面表示引用(reference)，我们有字符串类型String，也有引用字符串切片类型&amp;str，还有引用字符串类型是&amp;String。你基本可以理解为引用(&amp;)是rust里面一个很重要的操作。3、! 在rust里面表示调用了宏(macro)，所谓宏，就是做了一点额外的操作。 三、从一个脑洞开始了解rust的核心在克服了初见难受感觉后，我们来复盘一下，赋值在rust里面叫变量绑定。为啥好好的赋值不叫，非要整些幺蛾子。一切皆因资源有限。在现代计算机体系里面，内存是非常宝贵的计算资源。内存不是无限，而我们的世界是无限的。如何用有限资源模拟无限世界这需要技术与艺术。 3.1、一个脑洞：自由内存的自由联合在开始脑洞前，我们需要了解两个基本概念 栈：栈按照顺序存储值并以相反顺序取出值，这也被称作后进先出。想象一下一叠盘子：当增加更多盘子时，把它们放在盘子堆的顶部，当需要盘子时，再从顶部拿走。不能从中间也不能从底部增加或拿走盘子！增加数据叫做进栈，移出数据则叫做出栈。因为上述的实现方式，栈中的所有数据都必须占用已知且固定大小的内存空间，假设数据大小是未知的，那么在取出数据时，你将无法取到你想要的数据。 堆：与栈不同，对于大小未知或者可能变化的数据，我们需要将它存储在堆上。当向堆上放入数据时，需要请求一定大小的内存空间。操作系统在堆的某处找到一块足够大的空位，把它标记为已使用，并返回一个表示该位置地址的指针, 该过程被称为在堆上分配内存，有时简称为 “分配”(allocating)。接着，该指针会被推入栈中，因为指针的大小是已知且固定的，在后续使用过程中，你将通过栈中的指针，来获取数据在堆上的实际内存位置，进而访问该数据。由上可知，堆是一种缺乏组织的数据结构。想象一下去餐馆就座吃饭: 进入餐馆，告知服务员有几个人，然后服务员找到一个够大的空桌子（堆上分配的内存空间）并领你们过去。如果有人来迟了，他们也可以通过桌号（栈上的指针）来找到你们坐在哪。 用有限资源模拟无限世界，这有点像经济学里面的稀缺资源配置问题。回忆一下经济学是如何解决这个问题，第一条就是要产权清晰。第二条就是自由市场：有明晰产权的商品可以自由交易。这样我们就能达到所谓的帕累托最优。 如何才能实现理想社会，马克思说要重建个人所有制，实现自由人的自由联合。这也就是共产主义。换句话说rust要重建内存所有制，实现自由内存的自由联合，来实现编程世界的目标。基于此，rust不仅实现了内存产权清晰，也实现了内存的自由市场交易 坐稳了，我们要开始了，我们将在这个脑洞的指引下叙述内存所有权和几种交易方式 3.2、rust重建内存所有制3.2.1、 基本规则 每一个被使用的内存有一个所有者同一个内存同时只能有一个所有者所有者离开，则内存自由 3.2.2、 堆内存交易[move(转移)]堆内存交易有点像实物交易，下面笔者来一一解释, 注意下面这段代码会报错 1234567891011121314151617181920fn main() &#123; let a = "世界，你好哇".to_string(); /*内存地址：0x600000f651c0*/ let b = a ; println!("&#123;a&#125;")&#125;// error[E0382]: borrow of moved value: `a`// --&gt; src/main.rs:6:15// |// 4 | let a = "世界，你好哇".to_string();// | - move occurs because `a` has type `String`, which does not implement the `Copy` trait// 5 | let b = a ;// | - value moved here// 6 | println!("&#123;a&#125;")// | ^^^ value borrowed here after move// |// = note: this error originates in the macro `$crate::format_args_nl` which comes from the expansion of the macro `println` (in Nightly builds, run with -Z macro-backtrace for more info)// help: consider cloning the value if the performance cost is acceptable// |// 5 | let b = a.clone() ;// | ++++++++ 1、let a = “世界，你好哇”.to_string();会在堆上分配内存来存储字符串。这样这块内存0x600000f651c0的所有权就是归变量a所有。2、let b = a ; 记住rust里面的赋值叫变量绑定，就是说现在内存0x600000f651c0的所有权转移到了b，rust管这个叫做move(转移)3、println!(“{a}”) 这个时候你想打印a。记住我们的规则同一个内存同时只能有一个所有者，这个时候内存0x600000f651c0的所有权是b，a什么都没有，它的所有生命周期结束了。所以也就报错了。4、但是rust编译器有个建议，让你改成let b = a.clone() ;那我们对其更改，同时打印出内存的地址123456789101112fn main() &#123; let a = "世界，你好哇".to_string(); let a_pointer = a.as_ptr(); println!("a的内存地址:&#123;a_pointer:p&#125;"); let b = a.clone() ; /* 在内存中复制了一摸一样的内容 ，所用内存不一样 */ let b_pointer = b.as_ptr(); println!("b的内存地址:&#123;b_pointer:p&#125;"); println!("&#123;a&#125;")&#125;// a的内存地址:0x6000038151c0// b的内存地址:0x6000038151e0// 世界，你好哇 规则依然是适用的，let b = a.clone() ;在内存中复制了一摸一样的内容 ，它们使用的内存是不一样的，这和下面的栈内存交易是一样的。但请记住堆内存的clone交易是有成本的。堆是一种缺乏组织的数据结构，clone是比较耗时的。 3.2.3、 栈内存交易[clone(克隆)]栈内存交易有点像虚拟物品交易，拷贝几乎零成本。对于使用栈内存变量，赋值使用的克隆(clone)。理解这一行为非常重要。因为对于rust基本类型，rust都是使用栈内存交易。 12345678910fn main() &#123; let a = 1; println!("a的内存地址:&#123;:p&#125;", &amp;a); let b = a; println!("b的内存地址:&#123;:p&#125;", &amp;b); println!("&#123;a&#125;")&#125;// a的内存地址:6103771348// b的内存地址:6103771428// 1 好了开始一个迷惑行为12345678910111213141516fn main() &#123; let a = "你好，世界！"; let a_pointer = a.as_ptr(); println!("a的内存地址:&#123;a_pointer:p&#125;"); println!("a的本身内存地址:&#123;:p&#125;",&amp;a); let b = a; let b_pointer = b.as_ptr(); println!("b的本身内存地址:&#123;:p&#125;",&amp;b); println!("b的内存地址:&#123;b_pointer:p&#125;"); println!("&#123;a&#125;")&#125;// a的内存地址:0x1029a9e99// a的本身内存地址:0x16d486810// b的本身内存地址:0x16d4868b0// b的内存地址:0x1029a9e99// 你好，世界！ 注意 let a = “你好，世界！”;中的a是&amp;str的类型。它只是字符串的引用(地址)，而不是字符串本身。a和b是不同的，但是它们指向同一个字符串。这就好比a(0x16d486810)和b(0x16d4868b0)都是某虚拟服务A(0x1029a9e99)的客户，而某虚拟服务才A是资产的所有者。我们可不可以继续增加客户来扩大服务范围呢，当然可以 1234567891011121314151617181920212223fn main() &#123; let a = "你好，世界！"; let a_pointer = a.as_ptr(); println!("a的内存地址:&#123;a_pointer:p&#125;"); println!("a的本身内存地址:&#123;:p&#125;", &amp;a); let b = a; let b_pointer = b.as_ptr(); println!("b的本身内存地址:&#123;:p&#125;", &amp;b); println!("b的内存地址:&#123;b_pointer:p&#125;"); let c = a; let c_pointer = c.as_ptr(); println!("c的本身内存地址:&#123;:p&#125;", &amp;c); println!("c的内存地址:&#123;c_pointer:p&#125;"); println!("&#123;a&#125;");&#125;// a的内存地址:0x102691e69// a的本身内存地址:0x16d79e760// b的本身内存地址:0x16d79e800// b的内存地址:0x102691e69// c的本身内存地址:0x16d79e8a0// c的内存地址:0x102691e69// 你好，世界！ 3.3、自由内存的自由市场: 生命周期与借用3.3.1、自由市场简单介绍上述，我们了解了rust的所有权机制。但是资源是有限的。变量占一个内存，就少一个内存。最终资源将被耗尽。说好的改变世界呢？俗话说好，花无百日红，内存所有权不是一成不变的。rust说要使用就有所有权，没有使用就作废。防止占着茅坑不拉屎的行为发生。看下面代码 12345678910111213141516fn main() &#123; &#123;let b = "hello";&#125;println!("&#123;b&#125;")&#125;// error[E0425]: cannot find value `b` in this scope// --&gt; src/main.rs:6:16// |// 6 | println!("&#123;b&#125;")// | ^// |// help: the binding `b` is available in a different scope in the same function// --&gt; src/main.rs:4:10// |// 4 | &#123;let b = "hello";// | ^ 变量离开了作用域(scope) 也就失去了所有权，占用的内存就自由了。这就是是生命周期的基本原则。 借用(borrow)就是什么意思？就是所有权归你，使用权在我，而且还有两种使用权，不变借用就是你租了我的房子里可以租，不能动里面的东西。可变借用就是你租了我的房子可以重新装修。而借用(borrow)的操作实现就是引用(reference) 俗话说有借有还，再借不难。借用也需要规则，租赁市场需要规则。不能一房两卖，是吧。 总的来说，借用规则如下： 1、同一时刻，你只能拥有要么一个可变引用, 要么任意多个不可变引用2、引用必须总是有效的 123456789101112131415161718fn main() &#123; let mut a = "你好！".to_string(); let c = &amp;a; let b = &amp;mut a; b.push_str("这里是戏说rust的世界。"); println!("&#123;a&#125;"); println!("&#123;c&#125;");&#125;// error[E0502]: cannot borrow `a` as mutable because it is also borrowed as immutable// --&gt; src/main.rs:6:14// |// 5 | let c = &amp;a;// | -- immutable borrow occurs here// 6 | let b = &amp;mut a;// | ^^^^^^ mutable borrow occurs here// ...// 9 | println!("&#123;c&#125;");// | --- immutable borrow later used here 1、let mut a = “你好！”.to_string();创建了一个可变的字符串。2、let c = &a;把a借用给c，是一个不可变借用3、let b = &amp;mut a;把a又借用给b，是一个可变借用4、b.push_str(“这里是戏说rust的世界。”);b修改了内容，也就是重新装修了房子。5、println!(“{a}”);println!(“{c}”);a来看看情况6、println!(“{c}”);c来看看情况这个程序报错了，因为c最后回来看情况，借用的生命周期没有结束，而这个时候a还把房子借了b说你可以装修，这肯定就侵犯了c的权益。也就是说同一时刻，不能同时存在不可变借用和可变借用，也不能同时存在两个可变借用。我们修改一下代码让他运行通过 12345678910fn main() &#123; let mut a = "你好！".to_string(); let c = &amp;a; println!("&#123;c&#125;"); let b = &amp;mut a; b.push_str("这里是戏说rust的世界。"); println!("&#123;a&#125;");&#125;// 你好！// 你好！这里是戏说rust的世界。 3.3.2、自由市场的繁荣自由市场是极度灵活了，它需要能容纳各种交易的发生。所有权规则、借用规则、生命周期规则是内存自由市场的三大支柱，所有权和借用规则是非常清晰的，生命周期只有一个简单的作用域原则，这就打开的想象： 123456789101112131415161718192021222324fn main() &#123; fn longest(x: &amp;String, y: &amp;String) -&gt; &amp;String &#123; if x.len() &gt; y.len() &#123; x &#125; else &#123; y &#125; &#125; let x = "作用域原则".to_string(); let y = "生命周期的灵活性".to_string(); let a = &amp;x; let b = &amp;y; let c = longest(a, b); println!("&#123;c&#125;")&#125;// error[E0106]: missing lifetime specifier// --&gt; src/main.rs:4:43// |// 4 | fn longest(x: &amp;String, y: &amp;String) -&gt; &amp;String &#123;// | ------- ------- ^ expected named lifetime parameter// |// = help: this function's return type contains a borrowed value, but the signature does not say whether it is borrowed from `x` or `y`// help: consider introducing a named lifetime parameter// |// 4 | fn longest&lt;'a&gt;(x: &amp;'a String, y: &amp;'a String) -&gt; &amp;'a String &#123;// | ++++ ++ ++ ++ 上面代码报错缺失生命周期提示。为什么呢？因为我们引入了不确定性：比较两个字符串的大小，然后返回最长的那个。这个结果是不确定的，这也意味着a和b的生命周期具有不确定性，所有编译器这个市场监管者就发出警告了，要我们加上生命周期标注：1234567891011121314fn main() &#123; fn longest&lt;'t&gt;(x: &amp;'t String, y: &amp;'t String) -&gt; &amp;'t String &#123; if x.len() &gt; y.len() &#123; x &#125; else &#123; y &#125; &#125; let x = "作用域原则".to_string(); let y = "生命周期的灵活性".to_string(); let a = &amp;x; let b = &amp;y; let c = longest(a, b); println!("&#123;c&#125;")&#125;// 生命周期的灵活性 代码就正常了。fn longest&lt;’t&gt;(x: &amp;’t String, y: &amp;’t String) -&gt; &amp;’t String {}代码中我们引入一个生命周期t，并赋到了变量的类型标注上。表示x和y以及返回值的生命周期相同。这样就解决的不确定性的问题。 我们来写个更加清晰的写法123456789101112131415161718fn main() &#123; fn longest&lt;'long, 'short&gt;(x: &amp;'long String, y: &amp;'long String) -&gt; &amp;'short String where 'long: 'short &#123; if x.len() &gt; y.len() &#123; x &#125; else &#123; y &#125; &#125; let x = "作用域原则".to_string(); let y = "生命周期的灵活性".to_string(); let a = &amp;x; let b = &amp;y; let c = longest(a, b); println!("&#123;c&#125;")&#125;// 生命周期的灵活性 这个语法定义了两个生命周期long和short，语法是加个 ‘ 。然后我们要求x和y的生命周期大于返回值的生命周期，来消除不确定性。 fn longest&lt;’long, ‘short&gt;(x: &amp;’long String, y: &amp;’long String) -&gt; &amp;’short String where ‘long: ‘short 语法稍微有点奇怪但是，还是安排合理和清晰的。 四、评述rust要实现自由内存的自由联合，来实现编程世界的目标。为此rust做了三件大事 4.1、内存所有制 A、基本规则 每一个被使用的内存有一个所有者同一个内存同时只能有一个所有者所有者离开，则内存自由 B、堆内存交易转移(move)和栈内存交易克隆(clone)做出了不同规定，以节约资源。 并建立交易原则 4.2、借用规则 1、同一时刻，只能拥有要么一个可变引用, 要么任意多个不可变引用2、引用必须总是有效的 4.3、生命周期规则 1、变量离开了作用域(scope) 也就失去了所有权2、生命周期标注来干预生命周期 自此rust凭借三大机制内存所有制、借用规则、生命周期规则建立了内存自由交易的自由市场，开启了编程世界的新篇章。同时rust不会止步，它还进一步安排了各种细节，以优化市场运行。且听我们下回分解。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/afa185b9b060e00e/如果您需要引用本文，请参考：引线小白. (Sep. 8, 2022). 《戏说rust一_入门篇》[Blog post]. Retrieved from https://www.limoncc.com/post/afa185b9b060e00e@online{limoncc-afa185b9b060e00e,title={戏说rust一_入门篇},author={引线小白},year={2022},month={Sep},date={8},url={\url{https://www.limoncc.com/post/afa185b9b060e00e}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>rust</tag>
        <tag>rust语言</tag>
        <tag>工程实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2022的一些思考 述往事思来者]]></title>
    <url>%2Fpost%2F2d743831607dc5bf918db1b4c026a1f8%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/2d743831607dc5bf918db1b4c026a1f8/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、背景时间转眼来到了2022年，从事数据工作也将近5-6年了，工作了也将近12年了，时间可过的真快。太史公曰：古者富贵而名摩灭，不可胜记，唯倜傥非常之人称焉。名摩灭者太多了，古人总想留下一点什么，我也不例外。我本人的发展目标是： 适应生产力社会化智能化趋势，持续锻造数据分析能力，构建持续更新的可计算知识体系。 笔者认为个人的能力运营是以灵感和问题为中心，不断摄取优质知识与能力的新陈代谢活动 (( Knowledge and Ability Metabolism))。重温经典、结合新实践、学习研究一系列的新知识结构/工具方法。以工业级与科学级为目标，形成一个知识与能力数字化的可计算的系统。不断锤炼自身。将灵感、问题清单，知识与笔记系统化，成文成章输出已有经验和做法。实现可计算，使得知识成为储备，可以随时唤醒调用。 笔者的能力分级体系【了解级】：说明你看过，但是说不出来。别人一说，你只会哦哦哦。【知道级】：你能说出来，但一解决实际问题，就发现各种坑，各种不会，各种不顺。【应用级】：你能说出来，而且能动手解决，但是不熟练，别人10分钟，你要一天。【熟练应用级】：你能熟练操作知识，还没开始动手，就在脑海里知道怎么做了，而且很清楚，用什么工具，要费多少时间。【工业级】：你不仅能熟练操作知识，还能发明，发现新知识，新的方法。并应用于实践，解决问题。【科学级】：你不仅能发现新知识，新方法，你还能用数学表达，哲学概括。 经过十多年的发展，已经有些沉淀。太史公曰：此人皆意有所郁结，不得通其道，故述往事、思来者。笔者也曾写诗云 破卷夏雨绵绵听惊雷，再回首已是经年十年舞剑多壮志，长夜不息破卷知 砺剑锋钝剑锈是久藏，事成于行毁于怠灵台离散多困顿，严谋聚时待露白 述往事以思来者，是解决「锋钝剑锈是久藏」的一个办法。也是为这个世界留下一点什么，证明我来过的一个方法。$\displaystyle \text{ Life}=\int_{Birth}^{Death} (Learning+Working)\,dt$ 二、成文成章接来来，笔者会逐渐把这些年的经验和总结写出来，记录在这里，内容不限于 1、数学2、编程3、绘画/设计4、管理5、运营6、大数据7、机器学习8、人工智能9、经济与计量10、数据分析 暂时就列这些吧，尽量每月一篇。有一些是我以前写的，还是会按以前的时间编排。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/2d743831607dc5bf918db1b4c026a1f8/如果您需要引用本文，请参考：引线小白. (Jul. 20, 2022). 《2022的一些思考 述往事思来者》[Blog post]. Retrieved from https://www.limoncc.com/post/2d743831607dc5bf918db1b4c026a1f8@online{limoncc-2d743831607dc5bf918db1b4c026a1f8,title={2022的一些思考 述往事思来者},author={引线小白},year={2022},month={Jul},date={20},url={\url{https://www.limoncc.com/post/2d743831607dc5bf918db1b4c026a1f8}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>工程问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测度论与革命一]]></title>
    <url>%2Fpost%2F52e9ed9ae74c99f3b483237533c8c708%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/52e9ed9ae74c99f3b483237533c8c708/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文主要用革命的比喻，通俗的总结了测度论的基本问题，并且添加了我自己的一些体会。若有错误，请大家指正。关键词: 测度论,科普,革命 一、开篇大家好，这里用黑帮或者革命的故事，通俗讲解集合论、测度论的基本知识。帮助大家理解这门颇为难懂的数学知识。本系列的初衷，是以前在学习时的一些困惑，教科书过于冰冷与严肃，很多概念不知所云，理解学习数学最好的方法之一就是举例子，看数学书本身就是解读故事的过程，故事的精彩取决于你的理解。希望这种看数学书就像看故事书的体验，大家也能有所感受。这就是我的初衷。 初次开篇，写的还不够科普，在之后的岁月我会尽量通俗化，清晰化，故事化，大家敬请期待。也是我博客很久没有更新了，未来推进自身构建可计算知识，我准备开始写些文章，更新不定。本文主要内容，在知乎问答回答过，属于原创，请遵守版权声明。 集合是数学的基石，初等集合论是非常直观的。问题是一旦引入无限的概念，理论就开始复杂起来。在无限的影响下诞生了很多概念。下面我们就来说明一下常用的点集，我们先回顾一下下面点集的定义： 内点：$\displaystyle \exists r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：某种关系下的小弟（包括他自己） 外点：$\displaystyle \forall r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：所有各种关系下的小弟（包括他自己） 边界点：$\displaystyle \forall r &gt;0\,,\mathring{U}(x_0,r)=\{x \in S-\{x_0\}\mid d(x,x_0)&lt;r\}$：所有各种关系（不包括他自己）的小弟 此处应该有配图，稍后补上。 其中$\displaystyle S$是某一集合、 $\displaystyle x_0$是我们的点集里面点、 $\displaystyle d(x,x_0)$表示距离，这个距离是抽象的，并不一定是欧几里得距离。我们将点集的定义翻译为人话，或者一个故事情节。基于此，我们来开始我们的旅程。 二、点集の想像一如果某国 $\displaystyle S$有一个无限人数组成的黑帮 $\displaystyle A=\{x \in R \mid x $是黑帮一成员$\} \subset S$ 说 $ x$是内点：意思是说 $ x$在某种关系下所有小弟（包括他自己）是黑帮的成员。显然 $ x$自己是黑帮成员 说 $ x$是外点：意思是说 $ x$在某种关系下的所有小弟（包括他自己）都不是的黑帮成员，显然x 自己不是黑帮成员 说 $ x$是边界点：意思是说 $ x$所有各种关系下的有一部分小弟（包括他自己）都是的黑帮成员，显然 $ x$自己可能是，也可能不是黑帮成员 说 $ x$是孤立点：意思是说 $ x$在所有关系下的小弟（不包括他自己）都不是的黑帮成员，但是他自己是黑帮成员，所以孤立点这个名词：还是非常形象的。显然孤立点是边界点的成员并且是黑帮成员。 三、点集の想像二1、可见孤立点 $\displaystyle x_0$ 是忠诚的，那么他应该是黑帮最可以信赖的人，因为他只和组织 $A$ 有关系 $\displaystyle x_0\in A$。但是关系似乎不是很好，是个独行侠。是一把利刃。但是不忠诚的话，也有可能是警方卧底。因为他和组织外有关系。 2、内点，很显然他们组织的基础。孤立点是边界点中，是黑帮成员那一部分。 3、边界点是组织要提防的争取的人。 4、外点，显然不是非常重要的。 同理：某国 $\displaystyle S$有一个有限人数组成的黑帮，上述定义也是有意义的。 四、点集の想像三我们来继续考虑 $\displaystyle S$国的一个无限人数组成的黑帮 $\displaystyle A$$\displaystyle x$是极限点： 意思是说 $\displaystyle x$在所有各种关系（不包括他自己）总有一个小弟是黑帮成员。 显然 $ x$ 自己可能是，也可能不是黑帮成员。 $\displaystyle x$是聚点：意思是说 $ x$在所有各种关系下有无数个小弟是黑帮成员。 五、点集の想像四1、我们发现极限点和聚点其实是一个意思。证明请看 bady rudin 2.20定理。还有一些书上有附着点 $ x$的定义，是一个意思。2、显然聚点：是带特殊关系的黑帮成员或者是非黑帮成员。通过组织其他的人，总可以环环相扣找到他聚点。3、如果聚点不是黑帮成员，可以设想他是某个高官。是这个黑帮的后台，但是不是黑帮成员，但是通过组织其他的人，总可以环环相扣找到他。4、如果聚点是黑帮成员，可以想想，他混的不错，是个能人 六、点集の想像五如果某国 $\displaystyle S$有一个有限人数组成的黑帮 B我们可以发现，黑帮 $\displaystyle B$中没有极限点。证明请看baby rudin 2.20定理。 七、点集の想像六显然我们思考有限和无限时，内、边、外和孤是可以的。对于无限，还有聚点的概念。而且有 Weierstrass Theorem魏尔斯特拉斯聚点定理： $\displaystyle \Bbb{R}^n$中每个有界无限子集在 $\displaystyle \Bbb{R}^n$中有聚点。 换句话说， 每个黑帮都有一个强大的高官后台或者内部的能人。这就是神秘无限世界的一瞥。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/52e9ed9ae74c99f3b483237533c8c708/如果您需要引用本文，请参考：引线小白. (Jan. 1, 2019). 《测度论与革命一》[Blog post]. Retrieved from https://www.limoncc.com/post/52e9ed9ae74c99f3b483237533c8c708@online{limoncc-52e9ed9ae74c99f3b483237533c8c708,title={测度论与革命一},author={引线小白},year={2019},month={Jan},date={1},url={\url{https://www.limoncc.com/post/52e9ed9ae74c99f3b483237533c8c708}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>科普</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>科普</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[损失函数大意]]></title>
    <url>%2Fpost%2F301c877f0fd6a2cd%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/301c877f0fd6a2cd/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清损失函数的基础问题。若有错误，请大家指正。关键词: 损失函数,机器学习 [TOC] 1、信息论若干概念与结论我们首先回顾一下信息论的若干基本概念 信息： $\displaystyle \mathrm{I}(\bm{x}_i)=-\ln p(\bm{x}_i) $信息熵： $\displaystyle \mathbb{H}[\bm{x}]=\mathbb{E}[\mathrm{I}(\bm{x})]=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}$条件熵： $\displaystyle \mathbb{H}[\bm{x}\mid\bm{y}]= -\int\int p(\bm{x},\bm{y})\ln p(\bm{x}\mid \bm{y})\mathrm{d}\bm{x}\mathrm{d}\bm{y}$联合熵： $\displaystyle \mathbb{H}[\bm{x}\bm{y}]=-\int\int p(\bm{x},\bm{y})\ln p(\bm{x}, \bm{y})\mathrm{d}\bm{x}\mathrm{d}\bm{y}=\mathbb{H}[\bm{x}]+\mathbb{H}[\bm{y}\mid\bm{x}]\leqslant\mathbb{H}[\bm{x}]+\mathbb{H}[\bm{y}]$交叉熵： $\displaystyle \mathbb{H}[p,q]=-\int p(\bm{x})\ln q(\bm{x}) \mathrm{d}\bm{x}$KL散度： $\displaystyle \mathbb{KL}[p\parallel q]=-\int p(\bm{x})\ln\frac{q(\bm{x})}{p(\bm{x})}\mathrm{d}\bm{x}=-\mathbb{H}[p]+\mathbb{H}[p,q]$互信息：$$\begin{align}\mathbb{MI}[\bm{x},\bm{y}]=\mathbb{KL}\left[p(\bm{x},\bm{y})\parallel p(\bm{x})p(\bm{y})\right]&amp;=-\int\int p(\bm{x},\bm{y})\ln\frac{p(\bm{x})p(\bm{y})}{p(\bm{x},\bm{y})}\mathrm{d}\bm{x}\mathrm{d}\bm{y}\\&amp;=-\int\int p(\bm{x},\bm{y})[\ln p(\bm{x})-\ln p(\bm{x}\mid\bm{y})]\mathrm{d}\bm{x}\mathrm{d}\bm{y}\\&amp;=-\int\int p(\bm{x},\bm{y})\ln p(\bm{x})\mathrm{d}\bm{x}\mathrm{d}\bm{y}+\int\int p(\bm{x},\bm{y})\ln p(\bm{x}\mid\bm{y})\mathrm{d}\bm{x}\mathrm{d}\bm{y}\\\displaystyle&amp;=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}+\int\int p(\bm{x},\bm{y})\ln p(\bm{x}\mid \bm{y})\mathrm{d}\bm{x}\mathrm{d}\bm{y}\\&amp;=\mathbb{H}[\bm{x}]-\mathbb{H}[\bm{x}\mid\bm{y}]=\mathbb{H}[\bm{y}]-\mathbb{H}[\bm{y}\mid\bm{x}]\end{align}$$ 2、损失函数的一些结论最小化 KL 散度其实就是在最小化分布之间的交叉熵，确切的说是最小化 KL 散度个一个上界：经验分布和高斯模型之间的交叉熵。下面我们来证明这一点。我们考虑单变量情形，有数据集 $\displaystyle \{\bm{x}_i,y_i\}_{i=1}^n$ 同时令： $\displaystyle p_\mathcal{D}$表示经验密度， $\displaystyle p_M$表示估计密度。于是有： $$\begin{align}\mathbb{KL}\big[p_\mathcal{D}\parallel p_M\big]&amp;= -\mathbb{E}_{p_\mathcal{D}}\bigg[\ln\frac{p_M}{p_\mathcal{D}}\bigg]=\mathbb{E}_{p_\mathcal{D}}\big[\ln p_M\big]-\mathbb{E}_{p_\mathcal{D}}\big[\ln p_\mathcal{D}\big]=\mathbb{H}[p_\mathcal{D},p_M]-\mathbb{H}[p_\mathcal{D}]\\&amp;\geqslant\mathbb{H}[p_\mathcal{D},p_M]=-\mathbb{E}_{p_\mathcal{D}}\big[\ln p_M\big]=-\frac{1}{n}\ln p_M\big(\mathcal{D}\big)=-\frac{1}{n}\ell\\\end{align}$$ 注意这里 $\displaystyle p_M\big(\mathcal{D}\big)$其实就是似然函数。在频率主义下，我们叫似然函数；在贝叶斯主义下，我们叫数据集分布。在图模型里面，我们叫证据。请自行体会这些名词的含义。 我们来考虑一个具体的例子：均方误差是经验分布和高斯模型之间的交叉熵。我们考虑如下模型 $$\begin{align} p(y\mid \bm{x})=\mathcal{N}(y\mid \bm{w}^\text{T}\bm{x},\sigma^2)=\left(2\pi\right)^{-1/2}(\sigma^2)^{-1/2}\exp\left[-\frac{1}{2}(x-\mu)^2\sigma^{-2}\right]\end{align}$$我们有:$$\begin{align}\mathbb{H}[p_\mathcal{D},p_M]&amp;=-\mathbb{E}_{p_\mathcal{D}}\big[\ln p_M\big]=-\mathbb{E}_{p_\mathcal{D}}\big[\ln\mathcal{N}(y\mid \bm{w}^\text{T}\bm{x},\sigma^2)\big]\\&amp;=\displaystyle -\mathbb{E}_{p_\mathcal{D}}\bigg[-\frac{n}{2}\ln 2\pi-n\ln\sigma- \frac{\bm{\varepsilon}^\text{T}\bm{\varepsilon}}{2\sigma^2}\bigg]\\&amp;\displaystyle\propto \frac{1}{2}\mathbb{E}_{p_\mathcal{D}}\bigg[\bm{\varepsilon}^\text{T}\bm{\varepsilon}\bigg]=\frac{1}{2n}\bm{\varepsilon}^\text{T}\bm{\varepsilon}=\frac{1}{2n}\big|\big|\, \bm{\varepsilon} \,\big|\big|^2\\&amp;\displaystyle=\frac{1}{2n}\sum_{i=1}^n\big|\big|\, y_i-\hat{y}_i \,\big|\big|^2\\&amp;\displaystyle=\frac{1}{2n}\mathrm{RSS}=\mathrm{MSE}\\\end{align}$$ 对于多元独立同分布情况： $$\begin{align}\mathbb{KL}\big[p_\mathcal{D}\parallel p_M\big]&amp;\propto\mathbb{H}[p_\mathcal{D},p_M]=-\mathbb{E}_{p_\mathcal{D}}\big[\ln p_M\big]=-\mathbb{E}_{p_\mathcal{D}}\big[\ln\mathcal{N}\big(\bm{y}\mid \bm{w}^\text{T}\bm{x},\varSigma\big)\big]\\&amp;\propto- \frac{1}{2}\mathbb{E}_{p_\mathcal{D}}\bigg[\sum_{i=1}^n\big|\big|\bm{y}_i-\hat{\bm{y}}_i\big|\big|^2\bigg]\propto \frac{1}{2n}\sum_{i=1}^n\big|\big|\bm{y}_i-\hat{\bm{y}}_i\big|\big|^2=\mathrm{MES}\end{align}$$ 也就是说KL散度、交叉熵、似然函数、均方误差，其实存在等价关系。注意这个等价关系是有条件的。 $\displaystyle -\mathbb{E}_{p_\mathcal{D}}\big[\ln p_M\big]=-\frac{1}{n}\ln p_M\big(\mathcal{D}\big)$显然是要求大数定律保证的，是渐近性质。至于似然函数是不是均方误差需要假设数据分布是符合高斯分布，且数据点是独立。当然如果数据点是独立同分布，大数定律保证下也有高斯分布渐近性质。所以存在如下几个层次的等价关系，每下一个层次约束条件越来越严格。 $$\begin{align}\min \mathbb{KL}\big[p_\mathcal{D}\parallel p_M\big]&amp;\iff \min \mathbb{H}[p_\mathcal{D},p_M]\\\min \mathbb{KL}\big[p_\mathcal{D}\parallel p_M\big]&amp;\iff \min \mathbb{H}[p_\mathcal{D},p_M]\iff \max\ell \\\min \mathbb{KL}\big[p_\mathcal{D}\parallel p_M\big]&amp;\iff \min \mathbb{H}[p_\mathcal{D},p_M]\iff \max\ell\iff\min \mathrm{MSE}\end{align}$$ 3、评述1、在离散型变量的情况下，为了编码真实世界模型$\displaystyle p_\mathcal{D}$的信息。我们使用模型$\displaystyle p_M$编码数据。而$\displaystyle p_M$编码比真实世界模型$\displaystyle p_\mathcal{D}$要多出额外信息量。KL 散度衡量的就是这个额外信息量。 2、贯穿神经网络设计的一个反复出现的主题就是损失函数的梯度必须足够的大和具有足够的预测性，来为学习算法提供一个好的指引。饱和(变得非常平)的函数破坏了这一目标，因为它们把梯度变得非常小。一个显著的例子是神经网络很多输出单元都会包含一个指数函数，这在变量取绝对值非常大时会造成饱和。而这个时候 $\displaystyle \big|\big|\, y_i-\hat{y}_i \,\big|\big|^2$会变得很小。而负对数可以消除输出的指数效果。这就是为什 么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/301c877f0fd6a2cd/如果您需要引用本文，请参考：引线小白. (May. 7, 2017). 《损失函数大意》[Blog post]. Retrieved from https://www.limoncc.com/post/301c877f0fd6a2cd@online{limoncc-301c877f0fd6a2cd,title={损失函数大意},author={引线小白},year={2017},month={May},date={7},url={\url{https://www.limoncc.com/post/301c877f0fd6a2cd}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类分布大意]]></title>
    <url>%2Fpost%2Fb1417de51d2c97b0ef371b6561a65f4b%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/b1417de51d2c97b0ef371b6561a65f4b/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清分类分布的基础问题。若有错误，请大家指正。关键词: multinoulli,Categorical,softmax 一、分类分布的若干形式1.1、分类分布指示形式第一，回顾一下猫猫分布 (multinoulli distribution)或者叫 (Categorical distribution)。因为有英文 cat，我又叫它猫猫分布🐈：$$\begin{align}\mathrm{Cat}(x\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{\mathbb{I}(x=c)},x\in\{1,…,C\}\end{align}$$其中$\displaystyle \bm{\mu}=[\mu_1,\mu_2,…,\mu_C]^\text{T}\,,\sum_{c=1}^{C}\mu_c=\bm{1}^\text{T}\bm{\mu}=1$ 1.2、分类分布0-1编码形式第二、特别的，我们使用0-1编码来表示分类时，我们有：$$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{x_c}\,,\bm{x}=[x_1,x_2,…,x_C]^\text{T}\,,\bm{x}\in\{0,1\}^C\end{align}$$其中 $\displaystyle \sum_{c=1}^{C}x_c=\bm{1}^\text{T}\bm{x}=1$ 1.3、分类分布指数族形式第三、继续使用0-1编码，现在我们使用指数族的思想，对分类分布，也就是这个猫猫加以变形，这形式是我们要经常用到的形式： $$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})&amp;=\prod_{c=1}^{C}\mu_c^{x_c}=\exp\bigg[\sum_{c=1}^C x_c\ln\mu_c\bigg]=\exp\bigg[\bm{x}^\text{T}\ln \bm{\mu}\bigg]\\&amp;=\exp\bigg[\bm{x}^\text{T}\ln\frac{\bm{\mu}}{\mu_C}+\ln \mu_C\bigg]\end{align}$$我们定义一个新的向量 $\displaystyle \bm{\eta}=\ln\frac{\bm{\mu}}{\mu_C}$，于是有：$$\begin{align}\bm{\mu}=\mathcal{S}\big(\bm{\eta}\big)=\mathrm{softmax}\big(\bm{\eta}\big)=\Bigg[\frac{\mathrm{e}^{\eta_c}}{\sum_{c=1}^{C}\mathrm{e}^{\eta_c}}\Bigg]_{1\times C}=\frac{\mathrm{e}^\bm{\eta}}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\end{align}$$亦有$$\begin{align}\bm{x}\sim \mathrm{Cat}\big(\bm{x}\mid \mathcal{S}\big(\bm{\eta}\big)\big)=\exp \big[\bm{x}^\text{T}\bm{\eta}-\ln [\bm{I}^\text{T}\mathrm{e}^\bm{\eta}]\big]\end{align}$$ 二、分类分布、sofxmax和多元logistic回归。2.1、多元logistic回归模型考虑如下广义线性模型连接函数 $\displaystyle g \big(\bm{\mu}\big)=\bm{\eta}=\bm{W}\bm{x}\to \bm{\mu}=\mathcal{S}\big(\bm{W}\bm{x}\big)$。这一模型的形式具有十分重要的意义，首先让我们把诸多特征与与分类建立的了数学模型，其次分类分布的期望是一组归一化的概率，直接代表了我们对一次特征观测应该对应于哪个分类的信心。辅助以决策论，很容易做出推断。$$\begin{align}\bm{y}\sim \mathrm{Cat}\big(\bm{y}\mid \mathcal{S}\big(\bm{W}\bm{x}\big)\big)\end{align}$$我们有对数似然：$$\begin{align}\ell \big(\mathcal{D}\mid \bm{W}\big)&amp;=\ln \prod_{i=1}^N\prod_{c=1}^{C}\mathcal{S}_{ic}^{y_{ic}}=\sum_{i=1}^N \bm{y}_i ^\text{T}\ln \mathcal{S}\big(\bm{W}\bm{x}_i\big)=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{W}\bm{x}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{W}\bm{x}_i\big]\bigg)\bigg]\\&amp;=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{\eta}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]\bigg)\bigg]\end{align}$$ 2.2、多元logistic回归的梯度我们有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\sum_{i=1}^N \Bigg[\bm{y}_i- \frac{\mathrm{diag}\big[\mathrm{e}^{\bm{\eta}_i}\big]\bm{I}}{\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]}\Bigg]=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\end{align}$$有如下微分$$\begin{align}\mathrm{d}\ell=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i=\mathrm{tr}\bigg(\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i\bigg)\end{align}$$注意到 $\displaystyle \bm{\eta}_i=\bm{W}\bm{x}_i\to \mathrm{d}\bm{\eta}_i=\mathrm{d}\bm{W}\bm{x}_i$于是根据数量函数与矩阵微分的地定义有 ：$$\begin{align}&amp;\mathrm{d}\ell=\mathrm{tr}\bigg(\frac{\partial \ell}{\partial \bm{\eta}_i ^\text{T}}\cdot\mathrm{d}\bm{\eta}_i\bigg)=\sum_{i=1}^N\mathrm{tr}\Big( \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\bm{x}_i\Big)=\sum_{i=1}^N\mathrm{tr}\Big( \bm{x}_i\big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\Big)\\&amp;\Longrightarrow \frac{\partial \ell}{\partial \bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}\end{align}$$也就是说我们有梯度$$\begin{align}\nabla_{\bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}=\sum_{i=1}^N \big[\bm{y}_i- \bm{\mu}_i\big]\bm{x}_i^\text{T}\end{align}$$ 2.3、多元logistic回归的海赛矩阵注意到梯度的维数是 $\displaystyle D\times D$，应用 $\displaystyle \frac{\partial \big[a(\bm{x})\bm{f}(\bm{x})\big]}{\partial \bm{x}^\text{T}}=a\frac{\partial \bm{f}}{\partial \bm{x}^\text{T}}+\bm{f}\frac{\partial a}{\partial \bm{x}^\text{T}}$，于是我们注意到有如下结论$$\begin{align}\dot{\mathcal{S}}&amp;=\frac{\partial \mathcal{S}\big(\bm{\eta}\big)}{\partial \bm{\eta}^\text{T}}=\frac{1}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]-\mathrm{e}^\bm{\eta}\left[\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]\bm{I}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}\right]^\text{T}\\&amp;=\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}-\frac{ \mathrm{e}^\bm{\eta}\big[\mathrm{e}^\bm{\eta}\big]^\text{T}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}=\mathrm{diag}\big[\bm{\mu}\big]-\bm{\mu}\bm{\mu}^\text{T}\end{align}$$现在我们更进一步求这个矩阵梯度的微分，同时注意到有( $\displaystyle \mathrm{vec}\big(\bm{A}\bm{X}\bm{B}\big)=\big[\bm{B}^\text{T}\otimes \bm{A}\big]\mathrm{vec}\big(\bm{X}\big)$)，于是我们可以导出矩阵导数：$$\begin{align}&amp;\mathrm{d}\nabla=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{\eta}_i \cdot\bm{x}_i ^\text{T}=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\\\Longrightarrow&amp;\mathrm{vec}\big(\mathrm{d}\nabla\big)=\mathrm{vec}\bigg(\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\bigg)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N \mathrm{vec}\big(\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\big)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N\big[\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\big]\mathrm{d}\mathrm{vec}\big(\bm{W}\big)\\\Longrightarrow&amp;\bm{H}=\frac{\partial \mathrm{vec}\big(\nabla\big)}{\partial \mathrm{vec}^\text{T}\big(\bm{W}\big)}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{align}$$为了方便计算，我们把梯度也向量化，于是我们有$$\begin{align}\begin{cases}\displaystyle\nabla=\mathrm{vec}\big(\nabla_{\bm{W}}\big)=\sum_{i=1}^N\bm{x}_i \otimes\big[\bm{y}_i- \bm{\mu}_i\big]\\\displaystyle\bm{H}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{cases}\end{align}$$牛顿-拉弗迭代法有：$$\begin{align}\mathrm{vec}\big(\bm{W}\big):=\mathrm{vec}\big(\bm{W}\big)-\bm{H}^{-1}\nabla\end{align}$$当然各种优化方法都可以加以应用。 三、评述1、分类分布，有人又叫多项式分布，这里n=1，不过我不太赞成这个说法，使用分类分布更加恰当。当然更萌(๑•ᴗ•๑)一点可以叫猫猫分布。2、分类分布是一种很特殊的分布：对任意一个连续分布，如果我们将其离散化，我们都可以归结为一个分类分布。3、分类问题可以归结为：对特征向量分类，或者说对特征空间进行划分。由于与分类分布的关系使得 softmax函数经常出现，一般把它放在神经网络的最后一层来输出概率。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/b1417de51d2c97b0ef371b6561a65f4b/如果您需要引用本文，请参考：引线小白. (May. 6, 2017). 《分类分布大意》[Blog post]. Retrieved from https://www.limoncc.com/post/b1417de51d2c97b0ef371b6561a65f4b@online{limoncc-b1417de51d2c97b0ef371b6561a65f4b,title={分类分布大意},author={引线小白},year={2017},month={May},date={6},url={\url{https://www.limoncc.com/post/b1417de51d2c97b0ef371b6561a65f4b}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类分布</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动微分与反向传播大意]]></title>
    <url>%2Fpost%2F4be777c8af12fc58%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/4be777c8af12fc58/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 数学上一切都不理解，很大程度上是对符号的不清晰、不清楚、不理解造成的。究其根本是结合数学概念的符号练习太少了。 首先我们回忆一下向量函数对向量导数 1、数量函数对向量导数1、数量函数对列向量的导数定义，结果是个列向量。 $$\begin{align}\big[\nabla f\big]_{i\times 1}=\left[\frac{\partial{f(\bm{x})}}{\partial{\bm{x}}}\right]_{i\times 1}=\frac{\partial{f(\bm{x})}}{\partial{x_i}}\end{align}$$ 2、数量函数对行向量的导数定义，结果是个行向量。 $$\begin{align} \left[\frac{\partial{f(\bm{x})}}{\partial{\bm{x}^\text{T}}}\right]_{1\times j}=\frac{\partial{f(\bm{x})}}{\partial{x_j}}\end{align}$$ 2、向量函数对向量导数1、列向量函数对行向量的导数定义$$\begin{align}\bm{J}=\left[\frac{\partial{\bm{f}(\bm{x})}}{\partial{\bm{x}^\text{T}}}\right]_{i\times j}=\frac{\partial{f_i}}{\partial{x_j}}\end{align}$$ 2、行向量函数对列向量的导数定义 $$\begin{align}\left[\frac{\partial{\bm{f}^\text{T}(\bm{x})}}{\partial{\bm{x}}}\right]_{j\times i}=\frac{\partial{f_i}}{\partial{x_j}}\end{align}$$ 3、链式法则一般情况下我们喜欢列向量函数对行向量导数的表示方法。但是在链式法则中，数量函数对向量导数，一般使用数量函数对行向量导数，也就是说我们认为梯度是行向量，这样就是保持了统一：因为有链式法则 $$\begin{align}h(\bm{t})=f\big(\bm{x}(\bm{t})\big)&amp;\to \mathrm{D}h=\mathrm{D}f \cdot\mathrm{D}\bm{x}\\ \bm{h}(\bm{t})=\bm{f}\big(\bm{x}(\bm{t})\big)&amp;\to \mathrm{D}\bm{h}=\mathrm{D}\bm{f}\cdot\mathrm{D}\bm{x}\end{align}$$链式法则将会保持一致和简单性，即由外向内，从左到右书写。 注意梯度一般我们默认是列向量，这样梯度的导数才是海塞矩阵哈。千万不要混乱了。所以请务必熟悉我讲的以上三点。到底是行对列，还是列对行，然后求出的维度是多少，务必熟练理解定义。 4、自动微分的一个典型例子现在我们来看看神经网络的自动微分为啥要用反向传播，我们先定义点符号，以方便表示：1、$\displaystyle \ell:$ 损失函数2、$\displaystyle \bm{x}\to\bm{z}_1\to\bm{z}_2\to\bm{y}\to\ell$: 一个三层神经网络= 输入层 $\bm{x}$+ 隐藏层 $\bm{z}_1$ +隐藏层 $\bm{z}_2$+ 输出层 $\bm{y}$ 我们先后向求导哈： $$\begin{align}\frac{d\ell}{d\bm{x}^\text{T}}=\frac{d\ell}{d\bm{y}^\text{T}}\cdot\frac{d\bm{y}}{d\bm{z}_2^\text{T}}\cdot\frac{d\bm{z}_2}{d\bm{z}_1^\text{T}}\cdot\frac{d\bm{z}_1}{d\bm{x}^\text{T}}\end{align}$$ 貌似也没什么特别的哈，来我们让它特别起来，首先我们来明确一下维度 $$\begin{align}\bm{x}_{10\times 1}\to\bm{z}1_{6\times 1}\to\bm{z}2_{8\times 1}\to\bm{y}_{2\times 1}\to\ell_{1\times 1}\end{align}$$ 我们给她加上维度 $$\begin{align}\bigg[\frac{d\ell}{d\bm{x}^\text{T}}\bigg]_{1\times 10}=\bigg[\frac{d\ell}{d\bm{y}^\text{T}}\bigg]_{1\times 2}\cdot\bigg[\frac{d\bm{y}}{d\bm{z}_2^\text{T}}\bigg]_{2\times 8}\cdot\bigg[\frac{d\bm{z}_2}{d\bm{z}_1^\text{T}}\bigg]_{8\times 6}\cdot\bigg[\frac{d\bm{z}_1}{d\bm{x}^\text{T}}\bigg]_{6\times 10}\end{align}$$ 我来看一个矩阵乘法的简单事实$$\begin{align}\bm{D}_{n\times s}= \bm{A}_{n\times p}\cdot\bm{B}_{p\times m}\cdot\bm{C}_{m\times s}\end{align}$$1、首先计算 $\displaystyle \bm{A}_{n\times p}\cdot\bm{B}_{p\times m}$。你要计算 $\displaystyle n\times p \times m$次乘法。对吧，想不通自己回忆矩阵数乘定义，并计算一翻。看不出显然要打板子哦。2、其次计算 $\displaystyle \big[\bm{A}_{n\times p}\cdot\bm{B}_{p\times m}\big]_{n\times m}\cdot\bm{C}_{m\times s}$，你要计算 $\displaystyle n\times m\times s$次乘法3、最后计算出 $\displaystyle \bm{D}$，你一共要计算 $$\begin{align}\mathop{\text{cal_cost}}(\bm{D})=n\times p \times m+n\times m\times s\end{align}$$ 如果我们反过来计算呢?$$\begin{align}\big[\bm{D}^\text{T}\big]_{s\times n}= \big[\bm{C}^\text{T}\big]_{s\times m}\cdot\big[\bm{B}^\text{T}\big]_{m\times p}\cdot\big[\bm{A}^\text{T}\big]_{p\times n}\end{align}$$ 计算出 $\displaystyle \bm{D}^\text{T}$，你一共要计算 $$\begin{align}\mathop{\text{cal_cost}}(\bm{D}^\text{T})=s\times m \times p+s\times p\times n\end{align}$$ 现在在看看这个后向求导计算$$\begin{align}\bigg[\frac{d\ell}{d\bm{x}^\text{T}}\bigg]_{1\times 10}=\bigg[\frac{d\ell}{d\bm{y}^\text{T}}\bigg]_{1\times 2}\cdot\bigg[\frac{d\bm{y}}{d\bm{z}_2^\text{T}}\bigg]_{2\times 8}\cdot\bigg[\frac{d\bm{z}_2}{d\bm{z}_1^\text{T}}\bigg]_{8\times 6}\cdot\bigg[\frac{d\bm{z}_1}{d\bm{x}^\text{T}}\bigg]_{6\times 10}\end{align}$$ 计算成本是 $$\begin{align}\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}^\text{T}}\bigg)=1\times 2 \times 8+1\times 8\times 6+1\times 6\times 10 = 124\end{align}$$ 对于前向计算呢 $$\begin{align}\bigg[\frac{d\ell}{d\bm{x}}\bigg]_{10\times 1}=\bigg[\frac{d\bm{z}_1^\text{T}}{d\bm{x}}\bigg]_{10\times 6}\cdot\bigg[\frac{d\bm{z}_2^\text{T}}{d\bm{z}_1}\bigg]_{6\times 8}\cdot\bigg[\frac{d\bm{y}^\text{T}}{d\bm{z}_2}\bigg]_{8\times 2}\cdot\bigg[\frac{d\ell}{d\bm{y}}\bigg]_{2\times 1}\end{align}$$ $$\begin{align}\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}}\bigg)=10\times 6 \times 8+10\times 8\times 2+10\times 2\times 1 = 660\end{align}$$ 实际上我们有，如果看不出显然，请自我反思 $$\begin{align}\forall \dim[\bm{\ell}]\leqslant dim[\bm{x}]\to\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}^\text{T}}\bigg)\leqslant\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}^\text{T}}\bigg)\\\forall \dim[\bm{\ell}]&gt; dim[\bm{x}]\to\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}^\text{T}}\bigg)&gt;\mathop{\text{cal_cost}}\bigg(\frac{d\ell}{d\bm{x}^\text{T}}\bigg)\end{align}$$ 5、自动微分一个普遍的写法特别的如果我们不将 $\displaystyle \ell$视为标量的损失函数，而是一个可以向量优化目标 $\displaystyle \bm{\ell}$。同时我们特别的定义第 $i$层雅可比矩阵 $\displaystyle \bm{J}_i =\frac{d\bm{z}_i^\text{T}}{d\bm{z}_{i-1}}$ 如果是标量损失，最后一层导数 $\displaystyle \bm{v}_{output}:=\displaystyle \bm{J}_n=\frac{d\ell}{d\bm{y}}$ 那么有前向计算公式 $$\begin{align}\frac{d\bm{\ell}^\text{T}}{d\bm{x}} =\prod_{i=1}^{n} \bm{J}_i ^\text{T}=\bm{J}_1 ^\text{T}\cdot\bm{J}_2 ^\text{T}\cdots\bm{J}_{n-1} ^\text{T}\cdot\bm{J}_n ^\text{T}\end{align}$$ 后向计算公式$$\begin{align}\frac{d\bm{\ell}}{d\bm{x}^\text{T}} =\prod_{i=0}^{n-1} \bm{J}_{n-i} =\bm{J}_{n}\cdot\bm{J}_{n-1}\cdots\bm{J}_{2}\cdot\bm{J}_{1}\end{align}$$ 对于标量损失函数前向计算公式 $$\begin{align}\frac{d\ell}{d\bm{x}} =\prod_{i=1}^{n-1} \bm{J}_i ^\text{T}\cdot\bm{v}_{output}==\bm{J}_1 ^\text{T}\cdot\bm{J}_2 ^\text{T}\cdots\bm{J}_{n-1} ^\text{T}\cdot\bm{v}_{output}\end{align}$$ 后向计算公式$$\begin{align}\frac{d\ell}{d\bm{x}^\text{T}}=\bm{v}_{output}^\text{T}\cdot\prod_{i=1}^{n-1} \bm{J}_{n-i} =\bm{v}_{output}^\text{T}\cdot\bm{J}_{n-1}\cdots\bm{J}_{2}\cdot\bm{J}_{1}\end{align}$$ 而对于一个熟练上述概念的人来说，以上所有内容其实应该是显然的，你应该做到在脑海里一闪而过就得出结论。如果不能做到，请打牢固概念基础 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/4be777c8af12fc58/如果您需要引用本文，请参考：引线小白. (Apr. 25, 2017). 《自动微分与反向传播大意》[Blog post]. Retrieved from https://www.limoncc.com/post/4be777c8af12fc58@online{limoncc-4be777c8af12fc58,title={自动微分与反向传播大意},author={引线小白},year={2017},month={Apr},date={25},url={\url{https://www.limoncc.com/post/4be777c8af12fc58}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>自动微分</tag>
        <tag>反向求导</tag>
        <tag>反向传播</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2Fpost%2Fb0b6066e43a4c121a4f8fa0930b6a326%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/b0b6066e43a4c121a4f8fa0930b6a326/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 神经网络神经网络是由简单处理单元构成的大规模并行分布式处理器。 神经元 $$\begin{align}o_j(t)=f\{[\sum_{i=1}^{n}w_{ij}x_i(t-\tau_{ij})]-T_j\}\end{align}$$ 单神经元—感知机模型（Perceptron）模型的建立1943年生理学家W.S.McCulloch和W.A.Pitts提出了形式神经元数学模型，史称M-P模型。开创了神经科学理论研究的新时代！ 1949年心理学家Donald Olding Hebb在《行为构成》（Organization of Behavior）中提出了Hebb算法。而且首次提出了连接主义(connectionism)：大脑活动是靠脑细胞的组合连接实现的。 1958年美国Frank Rosenblatt提出了感知机（Perceptron）这应该是世界上第一个真正优秀的人工神经网络 概念与符号下面我们将说明这个模型，1、 $\displaystyle \boldsymbol{x}=[x_1,…,x_k]^\text{T}$为特征向量， 或者叫输入向量2、 $\displaystyle \boldsymbol{w}=[w_1,…,w_k]^\text{T}$为权值向量，也可以叫突触权值向量 ，模仿神经元的突触。3、 $\displaystyle w_0$为阈值。 $\displaystyle o=f$4、为简洁记令 $\displaystyle x_0=1$,于是有增广特征向量 $\displaystyle \boldsymbol{x}:=[x_0,x_1,…,x_k]^\text{T}$。5、增广权值向量$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$。6、定义输出 $\displaystyle o$，7、激活函数：硬限函数 $\displaystyle \mathrm{hardlims}(x)=\begin{cases} 1, &amp;x\geqslant0 \\\ -1, &amp;x&lt;0\end{cases}$。于是有：$$\begin{align}o=\mathrm{hardlims}\left(\boldsymbol{w}^\text{T}\boldsymbol{x}\right)\end{align}$$ 神经元 感知机学习与训练感知机学习算法：误差修正学习算法对于二分类增广数据集 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$，注意我们用的都是增广数据，$\displaystyle \boldsymbol{x}:=[1,x_1,…,x_k]^\text{T}$，$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$，同时 $\displaystyle y=+1\text{ or }-1$，表示 $\displaystyle c_1 \text{ or } c_2$类。 学习误差$\displaystyle y_t-o_t$于是有： 算法：误差修正学习算法1 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\eta_t(y_t-o_t)\boldsymbol{x}_t\end{array}\\$3 #end while 如果我们定义 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样取得新的数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$，一般称之为二分类规范增广数据集。在这个数据集下，线性可分意味着：存在一个 $\displaystyle \boldsymbol{w}$使得对于任意的 $\displaystyle \boldsymbol{z}_i$，$\displaystyle \boldsymbol{w}\boldsymbol{z}_i&gt;0$成立。 算法：误差修正学习算法2 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+2\eta_t\mathbb{I}(\boldsymbol{w}_t^\text{T} \boldsymbol{z}\leqslant 0)\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：梯度下降算法分析之前，令 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样我们把属于 $\displaystyle c_2$的特征变成了 $\displaystyle -\boldsymbol{x}$。定义目标函数： $\displaystyle J(\boldsymbol{w})=k \left(|\boldsymbol{w}^\text{T}\boldsymbol{z}|-\boldsymbol{w}^\text{T}\boldsymbol{z}\right),k&gt;0$ 通常令 $\displaystyle k=1$易知： $\displaystyle \min J(\boldsymbol{w})=0$时， $\displaystyle \boldsymbol{w}^\text{T}\boldsymbol{z}\geqslant 0$，于是分类正确。问题转化为：$$\begin{align}\min_{\boldsymbol{w}}J(\boldsymbol{w})\end{align}$$我们使用梯度下降算法：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_t\nabla_t\end{align}$$ 我们知道： $\displaystyle \nabla_t=\frac{\partial J}{\partial\boldsymbol{w}_t}=k[\boldsymbol{z}_t\times\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-\boldsymbol{z}_t]$ 代入3式得：$$\begin{align} \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t \end{align}$$当 $\displaystyle k=1$时，就是误差修正学习算法。 算法：梯度下降算法（Gradient Descent） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：最小均方误差算法取任意正数 $\displaystyle \nu_i$，有 $\displaystyle \boldsymbol{\nu}=[\nu_1,…,\nu_k]^\text{T}$。定义误差： $\displaystyle \boldsymbol{\epsilon}=\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}$ $$\begin{align} \mathrm{MES}(\boldsymbol{w}) =\boldsymbol{\epsilon}^\text{T}\boldsymbol{\epsilon} =\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right] \end{align}$$容易知道问题化为$$\begin{align}\min_{\boldsymbol{w}} \mathrm{MES}(\boldsymbol{w})\end{align}$$知道这就最小二乘解： $\displaystyle \hat{\boldsymbol{w}}=\left[\boldsymbol{X}^\text{T}\boldsymbol{X}\right]^{-1}\boldsymbol{X}^\text{T}\boldsymbol{\nu}=\boldsymbol{X}^\dagger \boldsymbol{\nu}$。求解伪逆的计算量大。我们使用梯度下降方法，知道$$\begin{align}\nabla\mathrm{MES}(\boldsymbol{w})=2\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]\end{align}$$ 算法：最小均方误差算法（Least Mean-Square Error） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{X}^\text{T}\boldsymbol{\nu},\boldsymbol{\nu}&gt;0$$2\displaystyle \alpha_0=\alpha$$3\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\alpha_t=\alpha\div t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-2\alpha_t\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}_t-\boldsymbol{\nu}\right]\\\end{array}\\$4 # end while$5\displaystyle \text{ if }\,\boldsymbol{\epsilon}_{end}=\boldsymbol{X}\boldsymbol{w}_{end}-\boldsymbol{\nu}\geqslant 0$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性可分’)}\\\end{array}$$\displaystyle \,\,\,\,\,\text{else }$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性不可分’)}\\\end{array}$6 # end if 感知器收敛定律如果样本集合线性可分，那么感知器存在收敛解。证明：1、我们使用规范增广数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n} $2、假定解向量 $\displaystyle \hat{\boldsymbol{w}}$，则对任意的 $\displaystyle \boldsymbol{z}$有 $\displaystyle \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}&gt;0$。我们注意到对于 $\displaystyle \delta&gt;0$ ，$\displaystyle \delta\cdot\hat{\boldsymbol{w}}$也是解向量。3、我们令 $\displaystyle \boldsymbol{z}_t$是所有错分样本， 有$$\begin{align}\boldsymbol{w}_t\boldsymbol{z}_t\leqslant 0\end{align}$$误差修正学习算法中令 $\displaystyle \eta_t=\frac{1}{2}$于是有：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\end{align}$$另外我们还令规范增广特征向量最大长度，与解向量最小内积$$\begin{align}\beta=\max_t ||\,\boldsymbol{z}_t||\end{align}$$$$\begin{align}\alpha=\min_t \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t&gt;0\end{align}$$4、现在我们把解向量 $\displaystyle \delta\hat{\boldsymbol{w}}$考虑进来于是：$$\begin{align}&amp;\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\\&amp;\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}=\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t\\&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t||^2\end{align}$$5、范数公式 $\displaystyle ||\boldsymbol{x}+\boldsymbol{y}||^2=||\boldsymbol{x}||^2+ 2&lt;\boldsymbol{x},\boldsymbol{y}&gt;+||\boldsymbol{y||^2},\,&lt;\boldsymbol{x},\boldsymbol{y}&gt;=\boldsymbol{x}^\text{T}\boldsymbol{y}$于是我们有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2+2 \boldsymbol{w}_t^\text{T}\boldsymbol{z}_t-2\delta\cdot\hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t+||\boldsymbol{z}_t||^2\end{align}$$6、根据上面的1、2、3的分析，有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-2\delta\cdot\alpha+\beta^2\end{align}$$7、为了简洁令： $\displaystyle \delta=\frac{\beta^2}{\alpha}$于是有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-\beta^2\end{align}$$8、考虑上式 $\displaystyle t=\{0,…,m\}$。并累加得：$$\begin{align}&amp;0\leqslant||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2-m\cdot\beta^2\end{align}$$9、可以看到随着 $\displaystyle m$的不断增加，范数 $\displaystyle ||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\to 0$，于是：$$\begin{align}m_{max}= \frac{||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2}{\beta^2}\end{align}$$10、如果我们令 $\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$,有：$$\begin{align}m_{max}=\frac{\delta^2}{\beta^2}||\,\hat{\boldsymbol{w}}||=\frac{\beta^2}{\alpha^2}||\,\hat{\boldsymbol{w}}||\end{align}$$这时感知机的解收敛于： $\displaystyle \frac{\beta^2}{\alpha}\cdot\hat{\boldsymbol{w}}$。史称感知机固定增量收敛定律。#### 评述1、 历史说明：分类是科学之始，为解决分类问题，人类殚精竭虑。其中模仿神经元的感知器就是其中之一。2、上帝给世界分了两类 $\displaystyle c_1$和 $\displaystyle c_2$。人类观测到了 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$。为了解决问题，人类思考了最简单的方法：劈下一刀，不就两半了。于是有了对称硬限函数。3、有个负号还是很麻烦，于是定义了 $\displaystyle \boldsymbol{z}=y\times \boldsymbol{x}$ 就有了新的数据集$\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$ 劈下一刀，变成了的折一下，然后裁一刀的问题：$$\begin{align} \forall \boldsymbol{z}_i,\boldsymbol{w}^\text{T}\boldsymbol{z}_i&gt;0\end{align}$$4、满足上式的问题，我们叫线性可分。并且根据学习规则和对问题认识，人类很块发现了感知机固定增量收敛定律。5、但是好景不长，人类的智者很快发现 线性不过是人类的YY，非线性才是上帝的YY。人类继续着征程： 我们的征途是星辰大海！ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/b0b6066e43a4c121a4f8fa0930b6a326/如果您需要引用本文，请参考：引线小白. (Apr. 5, 2017). 《感知机》[Blog post]. Retrieved from https://www.limoncc.com/post/b0b6066e43a4c121a4f8fa0930b6a326@online{limoncc-b0b6066e43a4c121a4f8fa0930b6a326,title={感知机},author={引线小白},year={2017},month={Apr},date={5},url={\url{https://www.limoncc.com/post/b0b6066e43a4c121a4f8fa0930b6a326}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>感知机</tag>
        <tag>人工神经网络</tag>
        <tag>人工智能</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法和混合模型三：隐线性模型]]></title>
    <url>%2Fpost%2F387bda4e04291667%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/387bda4e04291667/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 文本接上篇：EM算法和混合模型二：混合模型案例，来谈谈EM算法具体应用：因子分析。 一、因子分析 FA有如下数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^N$， 引入隐变量 $\displaystyle \bm{z}$得到新的数据集 $\displaystyle \mathcal{D}^+=\{\bm{x}_i,\bm{z}_i\}_{i=1}^N$，还有隐变量数据集 $\displaystyle \mathcal{D}^{\bm{z}}=\{\bm{z}_i\}_{i=1}^N$。所谓因子模型就是用潜在变量解释观测变量间协方差大部分的模型，其中潜在变量的维度要小于观测变量。可以做如下分解：$$\begin{align}\bm{x}_i=\bm{Q}\bm{z}_i+\bm{\mu}+\bm{\varepsilon}_i\end{align}$$其中： $\displaystyle \bm{x}_i$是 $\displaystyle D\times 1$维的观察变量，且有 $\displaystyle \bm{x}\sim \mathcal{N}\big(\bm{\mu},\bm{C}\big)$、$\displaystyle \bm{z}_i$是 $\displaystyle K\times 1$维的潜在因子、 $\displaystyle \bm{Q}$是 $\displaystyle D\times K$维的因子负荷、 特定因子 $\displaystyle\bm{\varepsilon}_i\sim \mathcal{N}\big(\bm{0},\bm{\varPsi} \big)$、 特定协方差矩阵 $\displaystyle \bm{\varPsi}=\mathrm{diag}\big[\psi_{11}^2,\cdots,\psi_{DD}^2\big]$。 我们使用概率语言，有条件分布 $\displaystyle p \big(\bm{x}\mid \bm{z},\bm{\theta}\big)=\mathcal{N}\big(\bm{Q}\bm{z}+\bm{\mu},\bm{\varPsi}\big)$、隐变量分布 $\displaystyle p \big(\bm{z}\big)=\mathcal{N}\big(\bm{0},\bm{E}\big)$，为何设置这样的隐变量分布，下文将会解释。$$\begin{align}p \big(\bm{x}_i\big)=\mathcal{N}\big(\bm{x}_i\mid \bm{\mu},\bm{C}\big)\iff \int p \big(\bm{x}_i\mid \bm{z}_i\big)p \big(\bm{z}\big)d \bm{z}_i=\int\mathcal{N}\big(\bm{x}_i\mid\bm{Q}\bm{z}_i+\bm{\mu},\bm{\varPsi} \big)\mathcal{N}\big(\bm{z}_i\mid \bm{0}, \bm{E}\big)d \bm{z}_i\end{align}$$现在，我们将注意力集中到协方差上，由高斯线性模型的结论知：$$\begin{align} \bm{C}=\bm{Q}\bm{Q}^\text{T}+\bm{\varPsi}\end{align}$$于是有：$$\begin{align}\sigma_{dd}^2=\sum_{k=1}^Kq_{dk}^2+\psi_{dd}^2\end{align}$$也就说： 方差=共同因子方差+特定因子方差 当然也有隐变量的条件分布：$$\begin{align}p \big(\bm{z}\mid \bm{x},\bm{\bm{\theta}}\big)=\mathcal{N}\big(\bm{z}\mid \bm{\mu}_{\bm{z}\mid \bm{x}},\bm{C}_{\bm{z}\mid \bm{x}}\big)\end{align}$$其中$\displaystyle \bm{C}_{\bm{z}\mid \bm{x}}=\big[\bm{C}_{\bm{z}}^{-1}+\bm{Q}^\text{T}\bm{C}_{\bm{x}\mid \bm{z}}^{-1}\bm{Q}\big]^{-1}=\big[\bm{E}+\bm{Q}^\text{T}\bm{\varPsi}^{-1}\bm{Q}\big]^{-1}$ $\displaystyle \bm{\mu}_{\bm{z}\mid \bm{x}}=\bm{C}_{\bm{z}\mid \bm{x}}\bigg[\bm{Q}^\text{T}\bm{C}_{\bm{x}\mid \bm{z}}^{-1}\big[\bm{x}-\bm{\mu}\big]+\bm{C}_{\bm{z}}^{-1}\bm{\mu}_{\bm{z}}\bigg]=\bm{C}_{\bm{z}\mid \bm{x}}\bigg[\bm{Q}^\text{T}\bm{\varPsi}^{-1}\big[\bm{x}-\bm{\mu}\big]\bigg]$ 特别的 $\displaystyle \bm{\varPsi}=\sigma^2 \bm{E} $叫概率主成分分析 $\displaystyle \textit{(PPCA)} $。这个名字原因稍后会变得很清晰。 二、混合因子分析接下来我们引入一个分类变量 $\displaystyle y\in\{1,\cdots,C\}$，从而有了混合因子模型，以适应更广范围的数据集: $\displaystyle \mathcal{D}=\mathcal{D}_1\cup \cdots \cup \mathcal{D}_C$，其中 $\displaystyle \mathcal{D}_c=\{\bm{x}_i,\bm{z}_i,y_i\}_{i=1}^{N_c}$。我们分别对每个 $\displaystyle \mathcal{D}_c$使用因子分析。令 $\displaystyle \bm{\theta}=\{\bm{\pi},\bm{\mu},\bm{Q},\bm{\varPsi}\}$$$\begin{align}p \big(\bm{x}\mid \bm{z},y=c,\bm{\theta}\big)&amp;=\mathcal{N}\big(\bm{x}\mid \bm{Q}_c \cdot\bm{z}+\bm{\mu}_c,\bm{\varPsi}\big)\\p \big(y\mid \bm{\theta}\big)&amp;=\mathrm{Cat}\big(y\mid \bm{\pi}\big)\\p \big(\bm{z}\mid \bm{\theta}\big)&amp;=\mathcal{N}\big(\bm{z}\mid \bm{0},\bm{E}\big)\end{align}$$ 混合因子分析 有$\displaystyle \bm{z}$的条件分布$$\begin{align}p \big(\bm{z}_i\mid \bm{x}_i,y=c,\bm{\theta}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{z}},\bm{C}_{\bm{z}}\big)\end{align}$$其中$\displaystyle \bm{C}_{\bm{z}\mid \bm{x}_i,y=c}=\big[\bm{E}+\bm{Q}_c^\text{T}\bm{\varPsi}^{-1}\bm{Q}_c\big]^{-1}$ $\displaystyle \bm{\mu}_{\bm{z}\mid \bm{x}_i,y=c}=\bm{C}_{c}\bigg[\bm{Q}_c^\text{T}\bm{\varPsi}^{-1}\big[\bm{x}_i-\bm{\mu}\big]\bigg]$我们使用 EM算法来求解参数。下面我们开始着手这件事情，以期求得显然。 $$\begin{align}p \big(\mathcal{D}^+\mid \bm{\theta}\big)&amp;=\prod_{i=1}^N p \big(\bm{x}_i,\bm{z}_i,y_i\mid \bm{\theta}\big)=\prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)p \big(\bm{z}_i\mid \bm{\theta}\big)p \big(y_i\mid \bm{\theta}\big)\\&amp;=\prod_{i=1}^N\Bigg[\prod_{c=1}^C\mathcal{N}^{\mathbb{I}(y_i=c)}\big(\bm{x}_i\mid \bm{Q}_c \cdot\bm{z}_i+\bm{\mu}_c,\bm{\varPsi}\big)\cdot\big[\mathcal{N}\big(\bm{z}_i\mid \bm{0},\bm{E}\big)\mathrm{Cat}\big(y_i\mid \bm{\pi}\big)\big]\Bigg]\end{align}$$下面我们来简化一下完全数据集的对数似然函数 $$\begin{align}&amp;\ell \big(\mathcal{D}^+\mid \bm{\theta}\big)=\ln \prod_{i=1}^Np \big(y_i\mid \bm{\theta}\big)+\ln \prod_{i=1}^Np \big(\bm{z}_i\mid \bm{\theta}\big)+\ln \prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)\\&amp;=-\frac{ND}{2}\ln 2\pi+\bm{I}^\text{T}\bm{Y}\ln \bm{\pi}- \frac{1}{2}\mathrm{tr}\big(\bm{Z}^\text{T}\bm{Z}\big)+\ln \prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)\\&amp;=A+\bm{I}^\text{T}\bm{Y}\ln \bm{\pi}+\ln \prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)\end{align}$$ 其中 $\displaystyle A=-\frac{ND}{2}\ln 2\pi- \frac{1}{2}\mathrm{tr}\big(\bm{Z}^\text{T}\bm{Z}\big)$。我们单独分析 $\displaystyle \ln \prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)$，同时令 $\displaystyle \bm{\Lambda}=\bm{\varPsi}^{-1}$$$\begin{align}&amp;\ln \prod_{i=1}^Np \big(\bm{x}_i\mid \bm{z}_i,y_i,\bm{\theta}\big)=\sum_{c=1}^Cy_{ic}\ln\prod_{i=1}^N\mathcal{N}\big(\bm{x}_i\mid \bm{Q}_c \cdot\bm{z}_i+\bm{\mu}_c,\bm{\varPsi}\big)\\&amp;=\frac{1}{2}\sum_{c=1}^C\bigg[-D \times\bm{I}^\text{T}\bm{y}_c\ln 2\pi+\bm{I}^\text{T}\bm{y}_c\ln \big|\bm{\Lambda}\big|- \mathrm{tr}\big(\bm{y}_c\odot \bm{S}_c\bm{\Lambda}\big)\bigg]\\&amp;=\frac{1}{2}\sum_{c=1}^C\bigg[B+\bm{I}^\text{T}\bm{y}_c\ln \big|\bm{\Lambda}\big|- \sum_{i=1}^Ny_{ic}\big[\bm{x}_i-\bm{Q}_c \bm{z}_i-\bm{\mu}_c\big]^\text{T}\bm{\Lambda}\big[\bm{x}_i-\bm{Q}_c \bm{z}_i-\bm{\mu}_c\big]\bigg]\\&amp;=\frac{1}{2}\sum_{c=1}^C\bigg[B+\bm{I}^\text{T}\bm{y}_c\ln \big|\bm{\Lambda}\big|- \mathrm{tr}\big(\bm{y}_c\odot \bm{X}^\text{T}\bm{X}\bm{\Lambda}\big)+2 \mathrm{tr}\big(\bm{y}_c\odot \bm{Z}^\text{T}\bm{X}\bm{\Lambda}\bm{Q}_c\big)- \mathrm{tr}\big(\bm{y}_c\odot \bm{Z}^\text{T}\bm{Z}\bm{Q}_c ^\text{T}\bm{\Lambda}\bm{Q}_c\big)\bigg]\end{align}$$ 其中 $\displaystyle B=-D \times\bm{I}^\text{T}\bm{y}_c\ln 2\pi$、 $\displaystyle \bm{S}_c=\big[\bm{X}-\bm{M}_c\big]^\text{T}\big[\bm{X}-\bm{M}_c\big]$、 $\displaystyle \bm{M}_c=\big[\bm{Q}_c \bm{z}_i+\bm{\mu}_c\big]_{D\times 1}\odot \bm{I}_{N\times 1}$是 $\displaystyle N\times D$维为简洁记，令 $\displaystyle \bm{z}:=[\bm{z};1]$， $\displaystyle \bm{Q}_c:=\big[\bm{Q}_c,\bm{\mu}_c\big]$于是有：$$\begin{align}\ell(\mathcal{D}^+\mid \bm{\theta})&amp;=A+\bm{I}^\text{T}\bm{Y}\ln \bm{\pi}+\frac{1}{2}\sum_{c=1}^C\bigg[B+\bm{I}^\text{T}\bm{y}_c\ln \big|\bm{\Lambda}\big|- \mathrm{tr}\big(\bm{y}_c\odot \bm{X}^\text{T}\bm{X}\bm{\Lambda}\big)+\\&amp;2 \mathrm{tr}\big(\bm{y}_c\odot \bm{Z}^\text{T}\bm{X}\bm{\Lambda}\bm{Q}_c\big)- \mathrm{tr}\big(\bm{y}_c\odot \bm{Z}^\text{T}\bm{Z}\bm{Q}_c ^\text{T}\bm{\Lambda}\bm{Q}_c\big)\bigg]\end{align}$$同时考虑到我们已经更新了 $\displaystyle \bm{z}$和 $\displaystyle \bm{Q}_c$的含义，有$$\begin{align}\mathrm{E}\big[\bm{z}_{ic}\big]=\mathrm{E}\big[\bm{z}_i\mid \bm{x}_i,y=c\big]&amp;=\begin{bmatrix} \bm{\mu}_{\bm{z}\mid \bm{x}_i,y=c}\\\bm{I} \end{bmatrix}\\\mathrm{E}\big[\bm{z}_{ic}\bm{z}_{ic}^\text{T}\big]=\mathrm{E}\big[\bm{z}_i \bm{z}_i ^\text{T}\mid \bm{x}_i,y=c\big]&amp;=\begin{bmatrix} \bm{C}_{\bm{z}\mid \bm{x}_i,y=c} &amp; \mathrm{E}\big[\bm{z}_i\mid \bm{x}_i,y=c\big]\ \mathrm{E}^\text{T}\big[\bm{z}_i\mid \bm{x}_i,y=c\big]&amp; \bm{E} \end{bmatrix}\end{align}$$为简洁记，令$$\begin{align}\mathrm{E}\big[\bm{Z}_c\big]_{N\times (k+1)}&amp;=\bigg[\mathrm{E}\big[\bm{z}_1\mid \bm{x}_1,y=c\big],\cdots,\mathrm{E}\big[\bm{z}_N\mid \bm{x}_N,y=c\big]\bigg]^\text{T}\\\mathrm{E}\big[\bm{Z}_c ^\text{T}\bm{Z}_c\big]_{(k+1)\times (k+1)}&amp;=\sum_{i=1}^N\mathrm{E}\big[\bm{z}_i \bm{z}_i ^\text{T}\mid \bm{x}_i,y=c\big]\end{align}$$现在我们基于隐变量分布$$\begin{align} q \big(\mathcal{D}^{\bm{z}},\mathcal{D}^y\mid \mathcal{D}^{\bm{x}},\bm{\theta}\big) =q \big(\mathcal{D}^y\mid \mathcal{D}^{\bm{x}},\bm{\theta}\big)q \big(\mathcal{D}^{\bm{z}}\mid \mathcal{D}^{\bm{x}},\mathcal{D}^y,\bm{\theta}\big)\end{align}$$来求完全数据集的对数似然函数的期望。我们知道：$\displaystyle \mathrm{E}\big[\mathrm{tr}(\bm{A})\big]=\mathrm{tr}\big(\mathrm{E}\big[\bm{A}\big]\big)$。有：$$\begin{align}&amp;\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\theta}\big)\big]=\int \ell \big(\mathcal{D}^+\mid \bm{\theta}\big)q \big(\mathcal{D}^{\bm{z}},\mathcal{D}^y\mid \mathcal{D}^{\bm{x}},\bm{\theta}\big)d \mathcal{D}^yd \mathcal{D}^{\bm{z}}\\&amp;=\mathrm{E}[A]+\bm{I}^\text{T}\mathrm{E}\big[\bm{Y}\big]\ln \bm{\pi}+\frac{1}{2}\sum_{c=1}^C\bigg[\mathrm{E}[B]+\bm{I}^\text{T}\mathrm{E}\big[\bm{y}_c\big]\ln \big|\bm{\Lambda}\big|- \\&amp;\mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\bm{X}\bm{\Lambda}\big)+2 \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E}^\text{T}\big[\bm{Z}_c\big]\bm{X}\bm{\Lambda}\bm{Q}_c\big)- \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E} \big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bm{Q}_c ^\text{T}\bm{\Lambda}\bm{Q}_c\big)\bigg]\end{align}$$ 对于离散隐变量的混合模型，EM算法的参数 $\displaystyle \bm{\pi}$ 有如下解：\begin{align}\bm{\pi}^{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Y}\big]\bm{I}}{N}=\frac{\bm{N}_c}{N}\end{align}注意这里的 $\displaystyle \bm{y}$已经是 $\displaystyle \textit{one of coding } $的形式。下面我们将注意集中到 $\displaystyle \bm{Q}_c:=\big[\bm{Q}_c,\bm{\mu}_c\big]$$$\begin{align}\mathrm{E}_{\mathcal{D}^+}\propto\sum_{c=1}^C\bigg[2 \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E}^\text{T}\big[\bm{Z}_c\big]\bm{X}\bm{\Lambda}\bm{Q}_c\big)- \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E} \big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bm{Q}_c ^\text{T}\bm{\Lambda}\bm{Q}_c\big)\bigg]\end{align}$$知 $\displaystyle \frac{\partial \mathrm{tr}\big(\bm{B}\bm{X}^\text{T}\bm{A}\bm{X}\big)}{\partial \bm{X}}=\big[\bm{A}+\bm{A}’\big]\bm{X}\bm{B}^\text{T}$有$$\begin{align}&amp;\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{Q}_c}=2\bigg[\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E}^\text{T}\big[\bm{Z}_c\big]\bm{X}\bm{\Lambda}\bigg]^\text{T}-2 \bm{\Lambda}\bm{Q}_c \bigg[\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E} \big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bigg]^\text{T}=\bm{0}\\&amp;\iff \bm{\Lambda}\bm{Q}_c \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot\mathrm{E}\big[\bm{Z}_c^\text{T}\bm{Z}_c\big]= \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot\bm{\Lambda}\bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big]\\&amp;\iff \bm{Q}_c^{EM}=\bigg[\mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big] \bigg]\bigg[ \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot\mathrm{E}\big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bigg]^{-1}\end{align}$$我们将注意力集中到 $\displaystyle \bm{\Lambda}$，同时有约束条件 $\displaystyle \bm{\varPsi}\odot \bm{E}=\bm{\varPsi}$，有： $$\begin{align}\mathrm{E}_{\mathcal{D}^+}\propto &amp;\sum_{c=1}^C\bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{y}_c\mid \bm{X}\big]\ln \big|\bm{\Lambda}\big|-\mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\bm{X}\bm{\Lambda}\big)+\\&amp;2 \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E}^\text{T}\big[\bm{Z}_c\big]\bm{X}\bm{\Lambda}\bm{Q}_c\big)- \mathrm{tr}\big(\mathrm{E}\big[\bm{y}_c\big]\odot \mathrm{E} \big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bm{Q}_c ^\text{T}\bm{\Lambda}\bm{Q}_c\big)\bigg]-\\&amp;\lambda \times\mathrm{tr}\big(\bm{\varPsi}\odot \big[\bm{I}-\bm{E}\big]\big)\end{align}$$又有 $\displaystyle \frac{\partial \mathrm{tr}\big(\bm{A}\bm{X}\bm{B}\big)}{\partial \bm{X}}=\bm{A} ^\text{T}\bm{B}^\text{T}$，于是有：$$\begin{align}&amp;\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\Lambda}}=\bm{I}^\text{T}\mathrm{E}\big[\bm{Y}\big]\bm{I}\cdot\bm{\varPsi}-\sum_{c=1}^C \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\bm{X}+\sum_{c=1}^C \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big]\bm{Q}_c ^\text{T}-\lambda\big|\bm{\Lambda}\big|\bm{\varPsi}\odot\big[\bm{I}-\bm{E}\big]=\bm{0}\\&amp;\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\lambda}}=\mathrm{tr}\big(\bm{\varPsi}\odot \big[\bm{I}-\bm{E}\big]\big)=0\end{align}$$写出相关细节：$$\begin{align}&amp;\bm{\varPsi}=\frac{1}{N}\sum_{c=1}^C\mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bigg[ \bm{X}^\text{T}\bm{X}- \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big]\bm{Q}_c ^\text{T}\bigg]+\frac{1}{N}\lambda\big|\bm{\Lambda}\big|\bm{\varPsi}\odot\big[\bm{I}-\bm{E}\big]\\&amp;\iff \bm{\varPsi}=\frac{1}{N}\bm{A}+\frac{1}{N}\lambda\big|\bm{\Lambda}\big|\bm{\varPsi}\odot\big[\bm{I}-\bm{E}\big]\\&amp;\iff N\bm{\varPsi}-\lambda\big|\bm{\Lambda}\big|\big[\bm{I}-\bm{E}\big]\odot\bm{\varPsi}=\bm{A}\\&amp;\iff \bigg[N \bm{I}-\lambda\big|\bm{\Lambda}\big|\big[\bm{I}-\bm{E}\big]\bigg]\odot\bm{\varPsi}=\bm{A}\\&amp;\iff \bm{\varPsi}=\frac{1}{\bigg[N \bm{I}-\lambda\big|\bm{\Lambda}\big|\big[\bm{I}-\bm{E}\big]\bigg]}\bm{A}\\&amp;\iff \bm{\varPsi}=\bigg[\frac{1}{N} \bm{I}-\frac{1}{\lambda\big|\bm{\Lambda}\big|}\big[\bm{I}-\bm{E}\big]\bigg]\odot \bm{A}\end{align}$$代入约束条件有：$$\begin{align}&amp;\bm{E}\odot\bigg[\frac{1}{N} \bm{I}-\frac{1}{\lambda\big|\bm{\Lambda}\big|}\big[\bm{I}-\bm{E}\big]\bigg]\odot \bm{A}=\bigg[\frac{1}{N} \bm{I}-\frac{1}{\lambda\big|\bm{\Lambda}\big|}\big[\bm{I}-\bm{E}\big]\bigg]\odot \bm{A}\\&amp;\iff \frac{1}{N}\bm{E}\odot\bm{A}=\bigg[\frac{1}{N} \bm{I}-\frac{1}{\lambda\big|\bm{\Lambda}\big|}\big[\bm{I}-\bm{E}\big]\bigg]\odot \bm{A}\\&amp; \iff \frac{1}{N}\bm{E}=\frac{1}{N} \bm{I}-\frac{1}{\lambda\big|\bm{\Lambda}\big|}\big[\bm{I}-\bm{E}\big]\\&amp;\iff \lambda=\frac{N}{\big|\bm{\Lambda}\big|}\end{align}$$于是有：$$\begin{align}\bm{\varPsi}^{EM}=\frac{1}{N}\bm{E}\odot \bm{A}=\frac{1}{N}\sum_{c=1}^C\mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bigg[ \bm{X}^\text{T}\bm{X}- \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big]\bm{Q}_c ^\text{T}\bigg]\odot \bm{E}\end{align}$$混合因子模型的参数如下：$$\begin{align}\begin{cases}\hat{\bm{\pi}}&amp;\displaystyle=\frac{\mathrm{E}^\text{T}\big[\bm{Y}\big]\bm{I}}{N}=\frac{\bm{N}_c}{N} \\\hat{\bm{Q}}_c&amp;\displaystyle=\bigg[\mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big] \bigg]\bigg[ \mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot\mathrm{E}\big[\bm{Z}_c^\text{T}\bm{Z}_c\big]\bigg]^{-1}\\\hat{\bm{\varPsi}}&amp;\displaystyle=\frac{1}{N}\sum_{c=1}^C\mathrm{E}^\text{T}\big[\bm{y}_c\big]\odot \bigg[ \bm{X}^\text{T}\bm{X}- \bm{X}^\text{T}\mathrm{E}\big[\bm{Z}_c\big]\hat{\bm{Q}}_c ^\text{T}\bigg]\odot \bm{E}\end{cases}\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/387bda4e04291667/如果您需要引用本文，请参考：引线小白. (Mar. 15, 2017). 《EM算法和混合模型三：隐线性模型》[Blog post]. Retrieved from https://www.limoncc.com/post/387bda4e04291667@online{limoncc-387bda4e04291667,title={EM算法和混合模型三：隐线性模型},author={引线小白},year={2017},month={Mar},date={15},url={\url{https://www.limoncc.com/post/387bda4e04291667}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>混合模型</tag>
        <tag>PPCA</tag>
        <tag>PCA</tag>
        <tag>混合因子分析</tag>
        <tag>因子分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法和混合模型二：混合模型案例]]></title>
    <url>%2Fpost%2Fcfb848a15208557d%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/cfb848a15208557d/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 文本接上篇：EM算法和混合模型一：EM算法，来谈谈EM算法具体应用：混合模型。 二、一般混合模型问题2.1、问题描述考虑一随机变量的离散混合情况。这句话意思是说：观测到的随机变量可能是由离散可数个分布中的一个产生的： $$\begin{align}\bm{x}\sim \sum_{k=1}^K \pi_k \times p\big(\bm{x}\mid \bm{\theta}_k\big)\end{align}$$例如：$$\begin{align}p(x)=0.35\times\mathcal{N}\big(x\mid 5,25\big)+0.25\times\mathcal{N}\big(x\mid 15,9\big)+0.4\times\mathcal{N}\big(x\mid -10,25\big)\end{align}$$ 2.2、引入隐变量(Latent variables)考虑一般混合模型的数据集 $\displaystyle \mathcal{D}^\bm{x}=\{\bm{x}_i\}_{i=1}^N$，我们加入隐含变量 $\displaystyle \bm{z}\in\{0,1\}^K=\mathcal{C}$、$\displaystyle \bm{I}^\text{T}\bm{z}=1$、 $\displaystyle N_k=\sum_{i=1}^{N}z_{ik}$、 $\displaystyle \bm{N}_k=[N_1,…,N_k,…,N_K]^\text{T}$。这是我们在分类分布 $\displaystyle \mathrm{Cat}$中，惯用的方法： $\displaystyle \textit{1 of K}$编码方法。这样我们有完整数据集：$\displaystyle \mathcal{D}^{+}=\{\bm{x}_i,\bm{z}_i\}_{i=1}^N$，同时我们把 $\displaystyle \mathcal{D}^\bm{x}$称为不完整数据集。 2.3、隐变量先验分布与 $\displaystyle \bm{x}$的条件分布显然：$$\begin{align}p\big(\bm{z}\mid \bm{\pi}\big)=\mathrm{Cat}\big(\bm{z}\mid \bm{\pi}\big)=\prod_{k=1}^K\pi_k^{z_k}\end{align}$$这个分布我们称之为 $\displaystyle \bm{z}$的先验分布，同时我们还有 $\displaystyle \bm{x}$的条件概率分布：$$\begin{align}p \big(\bm{x}\mid \bm{z},\bm{\theta}\big)=\prod_{k=1}^Kp\big(\bm{x}\mid \bm{\theta}_k\big)^{z_k}\end{align}$$ 2.4、隐变量与观测变量的联合分布导出观测变量分布注意到： $\displaystyle \bm{z}\in\{0,1\}^K=\mathcal{C}$。这时的混合模型为：$$\begin{align}p(\bm{x}\mid \bm{\theta},\bm{\pi})&amp;=\sum_{\bm{z}\in \mathcal{C}}p\big(\bm{x},\bm{z}\mid\bm{\theta},\bm{\pi}\big)=\sum_{\bm{z}\in \mathcal{C}}p\big(\bm{z}\mid \bm{\pi}\big)p\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\\&amp;=\sum_{\bm{z}\in \mathcal{C}}\bigg[\mathrm{Cat}\big(\bm{z}\mid \bm{\pi}\big)\times\prod_{k=1}^Kp\big(\bm{x}\mid \bm{\theta}_k\big)^{z_k}\bigg]\\\end{align}$$$$\begin{align}&amp;=\sum_{\bm{z}\in \mathcal{C}}\bigg[\prod_{k=1}^K\pi_k^{z_k}\times\prod_{k=1}^Kp\big(\bm{x}\mid \bm{\theta}_k\big)^{z_k}\bigg]\\&amp;=\sum_{\bm{z}\in \mathcal{C}}\bigg[\prod_{k=1}^K\big[\pi_k^{z_k}p\big(\bm{x}\mid \bm{\theta}_k\big)^{z_k}\big]\bigg]\\&amp;=\sum_{k=1}^K\pi_kp\big(\bm{x}\mid \bm{\theta}_k\big)\end{align}$$这是隐变量的经典变换技巧，请务必熟悉 $\displaystyle D、K、N$与 $\displaystyle d、k、i$的下标翻译。 上述推导中，我们有 隐变量与观测变量的联合分布：$$\begin{align}p\big(\bm{x},\bm{z}\mid\bm{\theta},\bm{\pi}\big)=p\big(\bm{z}\mid \bm{\pi}\big)p\big(\bm{x}\mid \bm{z},\bm{\theta}\big)=\prod_{k=1}^K\big[\pi_k^{z_k}p\big(\bm{x}\mid \bm{\theta}_k\big)^{z_k}\big]\end{align}$$ 2.5、隐变量后验概率分布与期望现在我们来考察后验概率分布 $\displaystyle p\big(\bm{z}\mid \bm{x},\bm{\mu},\bm{\pi}\big)=\frac{p\big(\bm{x},\bm{z}\mid \bm{\mu},\bm{\pi}\big)}{p\big(\bm{x}\mid \bm{\mu},\bm{\pi}\big)}=\frac{p\big(\bm{x},\bm{z}\mid \bm{\mu},\bm{\pi}\big)}{\displaystyle\sum_{\bm{z}\in\mathcal{C}}p\big(\bm{x},\bm{z}\mid \bm{\mu},\bm{\pi}\big)}$ $$\begin{align}q\big(\bm{z}\mid \bm{x},\bm{\mu},\bm{\pi}\big)=\frac{\displaystyle\prod_{k=1}^K\big[\pi_k^{z_k}p\big(\bm{x}\mid \bm{\mu}_k\big)^{z_k}\big]}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}\mid \bm{\mu}_k\big)}\end{align}$$ 注意到 $\displaystyle z_{ik}\in\{0,1\}$,下式$\displaystyle\mathrm{E}[z_{ik}]=\sum_{z_{ik}}\big[\bm{I}^\text{T}\bm{z}_i\times q\big(\bm{z}_i\mid \bm{x},\bm{\theta},\bm{\pi}\big)\big]=\frac{\displaystyle\sum_{\bm{z}}z_{ik}\prod_{k=1}^K\big[\pi_k^{z_{ik}}p\big(\bm{x}_i\mid \bm{\theta}_k\big)^{z_{ik}}\big]}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}=\frac{\displaystyle\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}$ 2.6、完全数据集对数似然函数期望我们将反复使用独立分布数据集的可分解性质。这就是为什么我们可以移动期望符号 $\displaystyle \mathrm{E}$的原因。$$\begin{align}\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\theta},\bm{\pi}\big)\big]&amp;=\mathrm{E}\bigg[\sum_{i=1}^N\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta},\bm{\pi}\big)\bigg]\\&amp;=\sum_{i=1}^N \bigg[\mathrm{E}\big[\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta},\bm{\pi}\big)\big]\bigg]\\&amp;=\sum_{i=1}^N \left[\mathrm{E}\bigg[\ln p \big(\bm{z}_i\mid \bm{\pi}\big)+\ln p \big(\bm{x}_i\mid \bm{z}_i,\bm{\theta}\big)\bigg]\right]\\\end{align}$$将模型代入，于是有：$$\begin{align}\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\theta},\bm{\pi}\big)\big]&amp;=\sum_{i=1}^N\sum_{k=1}^K \mathrm{E}\left[z_{ik}\bigg[\ln \pi_k+\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\bigg]\right]\\&amp;=\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\big[\ln \pi_k+\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\big]\bigg]\\&amp;=\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln \pi_k\bigg]+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\bigg]\\\end{align}$$ 2.7、混合模型EM算法中隐变量的参数估计 令 $\displaystyle \mathrm{E}_{\mathcal{D}^+}=\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\theta},\bm{\pi}\big)\big]$,于是我们有：$$\begin{align}\mathrm{E}_{\mathcal{D}^+}=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\bigg]\end{align}$$现在考虑 $\displaystyle \bm{\pi}$，知道 $\displaystyle \bm{I}^\text{T}\bm{\pi}=1$$$\begin{align}\mathrm{E}_{\mathcal{D}^+}&amp;\propto \bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln \bm{\pi}+\lambda\big[\bm{I}^\text{T}\bm{\pi}-1\big]\end{align}$$于是有 $$\begin{cases}\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\pi}}&amp;\displaystyle=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{\bm{\pi}}+\lambda\bm{I}=\bm{0}\\\\\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \lambda}&amp;\displaystyle=\bm{I}^\text{T}\bm{\pi}-1=0\end{cases}$$分析可得 $\displaystyle \bm{\pi}_{EM}=-\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{\lambda} $ 代入 $\displaystyle \bm{I}^\text{T}\bm{\pi}-1=0$得： $\displaystyle \frac{\bm{I}^\text{T}\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{\lambda}=-1$由于 $\displaystyle \bm{I}^\text{T}\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}=N$，同时令 $\displaystyle \bm{N}_k=\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}$可得：$$\begin{align}\begin{cases}\displaystyle\lambda=-N\\\\\displaystyle\bm{\pi}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{N}=\frac{\bm{N}_k}{N}\end{cases}\end{align}$$其中： 我们有 $\displaystyle \sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]=1$，于是 $\displaystyle \bm{I}^\text{T}\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}=\bm{I^\text{T}I}=N$，在混合模型中 $\displaystyle\bm{\pi}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{N}=\frac{\bm{N}_k}{N}$ 的结论是通用的。 三、伯努利混合模型3.1、伯努利分布混合模型表述考虑一个 $\displaystyle D$ 维二值变量 $\displaystyle \bm{x}$ 的密度是如下密度的混合：$$\begin{align}p(\bm{x}\mid \bm{\mu})=\prod_{d=1}^D\mathrm{Ber}\big(x_d\mid \mu_d\big)=\prod_{d=1}^D\mu_d^{x_d}\big(1-\mu_d\big)^{1-x_d}\end{align}$$也就是有：$$\begin{align}p(\bm{x}\mid \bm{\mu},\bm{\pi})=\sum_{k=1}^K\pi_kp\big(\bm{x}\mid \bm{\mu}_k\big)=\sum_{k=1}^K\Bigg[\pi_k\prod_{d=1}^D\mu_{kd}^{x_d}\big(1-\mu_{kd}\big)^{1-x_d}\Bigg]\end{align}$$ 3.2、伯努利混合模型完全数据集对数似然函数期望$$\begin{align}\mathrm{E}_{\mathcal{D}^+} &amp;=\sum_{i=1}^N\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\bigg[\ln \pi_k+\sum_{d=1}^D\big[x_{id}\ln\mu_{kd}+\big(1-x_{id}\big)\ln\big(1-\mu_{kd}\big)\big]\bigg]\\ &amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \Bigg[\mathrm{E}\big[z_{ik}\big]\bigg[\bm{x}_{i}^\text{T}\ln\bm{\mu}_{k}+\big[\bm{I}-\bm{x}_{i}\big]^\text{T}\ln\big[\bm{I}-\bm{\mu}_{k}\big]\bigg]\Bigg]\\\end{align}$$ 3.3、最大化完全数据集对数似然函数期望：$$\begin{align}\max_{\bm{\pi},\bm{\theta}}\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\mu},\bm{\pi}\big)\big]\end{align}$$考虑 $\displaystyle \bm{\pi}$有$$\begin{align}\bm{\pi}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{N}=\frac{\bm{N}_k}{N}\end{align}$$ 现在我们考虑 $\displaystyle \bm{\mu}_k$，同时注意到 $\displaystyle \sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]=1$ $$\begin{align}\mathrm{E}_{\mathcal{D}^+} &amp;\propto\sum_{i=1}^N\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\bigg[\sum_{d=1}^D\big[x_{id}\ln\mu_{kd}+\big(1-x_{id}\big)\ln\big(1-\mu_{kd}\big)\big]\bigg]\\ &amp;=\sum_{i=1}^N\Bigg[\sum_{k=1}^K \sum_{d=1}^D\mathrm{E}\big[z_{ik}\big]\cdot\ln\mu_{kd}\cdot x_{id}+\sum_{k=1}^K \sum_{d=1}^D\mathrm{E}\big[z_{ik}\big]\cdot\ln\big(1-\mu_{kd}\big)\cdot\big(1-x_{id}\big)\bigg]\\ &amp;=\sum_{i=1}^N\bigg[\mathrm{E}^\text{T}[\bm{z}_i]\cdot\ln[\bm{U}]\cdot \bm{x}_i+\mathrm{E}^\text{T}[\bm{z}_i]\cdot\ln[\bm{I}-\bm{U}]\cdot[\bm{I}-\bm{x}_i]\bigg]\\ &amp;=\mathrm{tr}\bigg(\mathrm{E}\big[\bm{Z}\big]\cdot\ln\big[\bm{U}\big]\cdot \bm{X}^\text{T}\bigg)+\mathrm{tr}\bigg(\mathrm{E}\big[\bm{Z}\big]\cdot\ln\big[\bm{I}-\bm{U}\big]\cdot \big[\bm{I}-\bm{X}\big]^\text{T}\bigg)\end{align}$$有：$$\begin{align}\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{U}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\bm{U}}-\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\big[\bm{I}-\bm{X}\big]}{\bm{I}-\bm{U}}=0\end{align}$$点除和点乘运算，我们写出细节，大家体会一下：$$\begin{align}&amp;\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\bm{U}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\big[\bm{I}_{N\times D}-\bm{X}\big]}{\bm{I}_{K\times D}-\bm{U}}\\\iff&amp;\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\odot \bm{I}-\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\odot \bm{U}=\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}\odot \bm{U}-\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\odot \bm{U}\\\iff&amp;\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}=\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}\odot \bm{U}\\\iff &amp;\bm{U}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_{N\times D}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_N}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}\end{align}$$得：$$\begin{align}\bm{U}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\bm{N}_k}\end{align}$$注意：上面的除法都是点除运算。 $\displaystyle \bm{U}=[\bm{\mu}_1,\bm{\mu}_2,\cdots,\bm{\mu}_k]^\text{T}\,\bm{\mu}\in [0,1]^D$，于是我们有：$$\begin{align}\begin{cases}\displaystyle\mathrm{E}[z_{ik}]=\frac{\displaystyle\pi_kp\big(\bm{x}_i\mid \bm{\mu}_k\big)}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}_i\mid \bm{\mu}_k\big)}\\\displaystyle\bm{\pi}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{N}=\frac{\bm{N}_k}{N}\\\displaystyle\bm{U}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}}{\bm{N}_k}\end{cases}\end{align}$$ 3.4、伯努利混合模型EM算法算法：伯努利混合模型EM算法 1 $\displaystyle \bm{\pi}_0=\bm{a}$2 $\displaystyle \bm{U}_o=\bm{A}$3 $\displaystyle \text{while }\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\pi}_{t+1},\bm{U}_{t+1}\big)=\text{converged}$ :$\displaystyle\quad\begin{array}{|lc}\text{ E step}\\\text{—————-}\\\text{for i in range(1,N+1):}\\\quad\text{for k in range(1,K+1):}\\\quad\quad\mathrm{E}\big[z_{ik}^t\big]=\frac{\displaystyle\pi_k^tp\big(\bm{x}_i\mid \bm{\mu}_k^t\big)}{\displaystyle\sum_{k=1}^K\pi_k^tp\big(\bm{x}_i\mid \bm{\mu}_k^t\big)}\\\quad\text{ end for k}\\\text{ end for i}\\\text{ M step}\\\text{—————-}\\\bm{U}_{t+1}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}_t\big]\bm{X}}{\mathrm{E}^\text{T}\big[\bm{Z}_t\big]\bm{I}}\\\bm{\pi}_{t+1}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}_t\big]\bm{I}}{N}\\\end{array}\\$4 $\displaystyle \bm{\pi}_{end}$5 $\displaystyle \bm{U}_{end}$ end while 3.5、伯努利混合模型简单举例现在我们假定： $\displaystyle \mu_{k1}=\cdots=\mu_{kd}=\cdots=\mu_{kD}=\mu_k$。不完全模型它的故事是这样的：上帝有两个硬币A、B：这两个硬币抛出正面的概率分别是0.75和0.25。上帝每次抛硬币都以一定概率来选择A或B，一旦选出，上帝会抛七次。结果上帝选了5次硬币，同时抛了 $\displaystyle 5\times7$次。现在我们仅仅通过$\displaystyle 5\times7$次中，我们观察到的正反来猜测上帝硬币的概率：我们就有特征同分布、伯努利混合模型的完全数据集：它对数似然函数的期望。 $$\begin{align}\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\mu},\bm{\pi}\big)\big]=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\bm{I}^\text{T}\bm{X}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\mu} +\bm{I}^\text{T}\big[\bm{I}-\bm{X}\big]^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\big[\bm{I}-\bm{\mu}\big]\end{align}$$证明：$$\begin{align}&amp;\mathrm{E}_{\mathcal{D}^+}=\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\mu},\bm{\pi}\big)\big]\\&amp;=\sum_{i=1}^N\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\bigg[\ln \pi_k+\sum_{d=1}^D\big[x_{id}\ln\mu_{kd}+\big(1-x_{id}\big)\ln\big(1-\mu_{kd}\big)\big]\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\sum_{i=1}^N\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\bigg[\sum_{d=1}^D\big[x_{id}\ln\mu_{kd}+\big(1-x_{id}\big)\ln\big(1-\mu_{kd}\big)\big]\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\sum_{i=1}^N\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\bigg[\big[\bm{x}_{i}^\text{T}\bm{I}\ln\mu_{k}+\big(\bm{I}-\bm{x}_{i}\big)^\text{T}\bm{I}\ln\big(1-\mu_{k}\big)\big]\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\sum_{i=1}^N\bigg[\bm{I}^\text{T}\bm{x}_{i}\cdot\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\ln\mu_{k} +\bm{I}^\text{T}\big(\bm{I}-\bm{x}_{i}\big)\cdot\sum_{k=1}^K\mathrm{E}\big[z_{ik}\big]\ln\big(1-\mu_{k}\big)\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\sum_{i=1}^N\bigg[\bm{I}^\text{T}\bm{x}_{i}\mathrm{E}^\text{T}\big[\bm{z}_{i}\big]\ln\bm{\mu} +\bm{I}^\text{T}\big(\bm{I}-\bm{x}_{i}\big)\mathrm{E}^\text{T}\big[\bm{z}_{i}\big]\ln\big(\bm{I}-\bm{\mu}\big)\bigg]\\&amp;=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi} +\bm{I}^\text{T}\bm{X}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\mu} +\bm{I}^\text{T}\big[\bm{I}-\bm{X}\big]^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\big[\bm{I}-\bm{\mu}\big]\\\end{align}$$证毕。 考虑 $\displaystyle \bm{\mu}$，最大化这个期望。于是我们有$$\begin{align}\frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\mu}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}}{\bm{\mu}}-\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\big[\bm{I}-\bm{X}\big]\bm{I}}{\bm{I}-\bm{\mu}}=0\end{align}$$ 我们来详细推导一下，以熟悉点乘、点除运算：$$\begin{align}&amp;\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}_{D\times 1}}{\bm{\mu}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\big[\bm{I}_{N\times D}-\bm{X}\big]\bm{I}_{D\times1}}{\bm{I}_{K\times 1}-\bm{\mu}}\\&amp;\iff\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}\odot \bm{I}-\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}\odot \bm{\mu}=\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_{N\times D}\bm{I}\odot \bm{\mu}-\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}\odot \bm{\mu}\\&amp;\iff\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}=\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_{N\times D}\bm{I}_{D\times1}\odot \bm{\mu}\\&amp;\iff \bm{\mu}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_{N\times D}\bm{I}_{D\times1}}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}_{N\times I}D}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{X}\bm{I}}{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}D}\end{align}$$ 3.6、伯努利混合模型EM算法图示现在我开始开头的那个例子，用图示画出来： EM算法实例 四、高斯混合模型4.1、高斯混合模型问题描述$$\begin{align}\bm{x}\sim \sum_{k=1}^K \bm{\pi}_k \mathcal{N}\big(\bm{x}\mid \bm{\mu}_k,\bm{C}_k\big)\end{align}$$ 4.2、高斯混合模型完全数据集对数似然函数期望$$\begin{align}\mathrm{E}_{\mathcal{D}^+}=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln \mathcal{N}\big(\bm{x}\mid \bm{\mu}_k,\bm{C}_k\big)\bigg]\end{align}$$ 4.3、最大化完全数据集对数似然函数期望现在我们开始适当的变形，以期取得关于 $\displaystyle \bm{\mu}_k$、 $\displaystyle \bm{C}_k$的恰当形式。$$\begin{align}&amp;\mathrm{E}_{\mathcal{D}^+}=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln \mathcal{N}\big(\bm{x}\mid \bm{\mu}_k,\bm{C}_k\big)\bigg]\\&amp;\propto\sum_{k=1}^K\Bigg[\sum_{i=1}^N \mathrm{E}\big[z_{ik}\big]\bigg[-\frac{k}{2}\ln 2\pi+\frac{1}{2}\ln\big|\bm{\Lambda}_k\big|-\frac{1}{2}\big[\bm{x}_i- \bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\big[\bm{x}_i- \bm{\mu}_k\big]\bigg]\Bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\bigg[-k\times\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln 2\pi+\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\sum_{i=1}^N\mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i- \bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\big[\bm{x}_i- \bm{\mu}_k\big]\bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\sum_{i=1}^N\mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i- \bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\big[\bm{x}_i- \bm{\mu}_k\big]\bigg]\\\end{align}$$进而有:$$\begin{align}&amp;\mathrm{E}_{\mathcal{D}^+}=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln \mathcal{N}\big(\bm{x}\mid \bm{\mu}_k,\bm{C}_k\big)\bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\mathrm{tr}\bigg(\sum_{i=1}^N\mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i- \bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\big[\bm{x}_i- \bm{\mu}_k\big]\bigg)\bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|- \mathrm{tr}\bigg(\sum_{i=1}^N\mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i- \bm{\mu}_k\big]\big[\bm{x}_i- \bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\bigg)\bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\mathrm{tr}\bigg(\mathrm{E}^\text{T}\big[\bm{z}_k\big]\odot\big[\bm{X}- \bm{M}_k\big]^\text{T}\big[\bm{X}- \bm{M}_k\big]\bm{\Lambda}_k\bigg)\bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\Bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\mathrm{tr}\bigg(\mathrm{E}^\text{T}\big[\bm{z}_k\big]\odot \bm{S}_{\bm{\mu}_k}\bm{\Lambda}_k\bigg)\Bigg]\end{align}$$ 4.4、高斯混合模型EM算法1、关于 $\displaystyle \mathrm{E}\big[\bm{Z}\big]$$$\begin{align}\mathrm{E}\big[z_{ik}\big]=\frac{\displaystyle\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}=\frac{\displaystyle\pi_k \mathcal{N}\big(\bm{x}_i\mid \bm{\mu}_k,\bm{C}_k\big)}{\displaystyle\sum_{k=1}^K\pi_k\mathcal{N}\big(\bm{x}_i\mid \bm{\mu}_k,\bm{C}_k\big)}\end{align}$$2、关于 $\displaystyle \bm{\pi}$有：$$\begin{align}\bm{\pi}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{\bm{I}^\text{T}\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}=\frac{\bm{N}_k}{N}\end{align}$$3、关于 $\displaystyle \bm{\mu}_k,\bm{C}_k$$$\begin{align}\mathrm{E}_{\mathcal{D}^+}&amp;\propto \frac{1}{2}\sum_{k=1}^K\Bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\sum_{i=1}^N \mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i-\bm{\mu}_k\big]^\text{T}\bm{\Lambda}_k\big[\bm{x}_i- \bm{\mu}_k\big]\Bigg]\\&amp;\propto \frac{1}{2}\sum_{k=1}^K\Bigg[\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big|\bm{\Lambda}_k\big|-\mathrm{tr}\bigg(\mathrm{E}^\text{T}\big[\bm{z}_k\big]\odot \bm{S}_{\bm{\mu}_k}\bm{\Lambda}_k\bigg)\Bigg]\end{align}$$于是我们有：$$\begin{align}\begin{cases}\displaystyle \frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\mu}_k}=\frac{1}{2}\big[\bm{\Lambda}_k+\bm{\Lambda}^\text{T}_k\big]\sum_{i=1}^N\bigg[ \mathrm{E}\big[z_{ik}\big]\big[\bm{x}_i- \bm{\mu}_k\big]\bigg]=\bm{X}^\text{T}\mathrm{E}\big[\bm{z}_k\big]-\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\bm{\mu}_k=0\\\displaystyle \frac{\partial \mathrm{E}_{\mathcal{D}^+}}{\partial \bm{\Lambda}_k}=\frac{1}{2}\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\bm{\Lambda}^{-1}_k- \frac{1}{2}\mathrm{E}^\text{T}\big[\bm{z}_k\big]\odot \bm{S}_{\bm{\mu}_k}=0\end{cases}\end{align}$$简单变形有：$$\begin{align}\begin{cases}\displaystyle \bm{\mu}_k=\frac{\bm{X}^\text{T}\mathrm{E}\big[\bm{z}_k\big]}{\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]}\\\\\displaystyle \bm{C}_k=\frac{\mathrm{E}^\text{T}\big[\bm{z}_k\big]}{\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]}\odot \bm{S}_{\bm{\mu}_k}\end{cases}\end{align}$$ 五、评述1、EM算法的关键在于引入隐变量，然后我们考察它的先验、后验和完全数据集分布，于是我们得到： 不完全数据集对数似然函数等价变换：$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q,p\big]=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\end{align}$$正是因为如此，我们才有了【EM算法收敛定律】:$$\begin{align}\exists \,\bm{\theta}_{end}\to\max\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\iff\max \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]\end{align}$$2、一般混合模型的完全数据集分布：$$\begin{align}\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\theta},\bm{\pi}\big)\big]=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}+\sum_{i=1}^N\sum_{k=1}^K \bigg[\mathrm{E}\big[z_{ik}\big]\ln p\big(\bm{x}_i\mid\bm{\theta}_k\big)\bigg]\end{align}$$且有通用的结论：$$\begin{align}\begin{cases}\displaystyle\mathrm{E}[z_{ik}]=\frac{\displaystyle\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}{\displaystyle\sum_{k=1}^K\pi_kp\big(\bm{x}_i\mid \bm{\theta}_k\big)}\\\\\displaystyle\bm{\pi}_{EM}=\frac{\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}{\bm{I}^\text{T}\mathrm{E}^\text{T}\big[\bm{Z}\big]\bm{I}}=\frac{\bm{N}_k}{N}\end{cases}\end{align}$$3、伯努利混合模型完全数据集对数似然函数期望$$\begin{align}&amp;\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\mu},\bm{\pi}\big)\big]=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}\\ &amp;+\mathrm{tr}\bigg(\mathrm{E}\big[\bm{Z}\big]\cdot\ln\big[\bm{U}\big]\cdot \bm{X}^\text{T}\bigg)+\mathrm{tr}\bigg(\mathrm{E}\big[\bm{Z}\big]\cdot\ln\big[\bm{I}-\bm{U}\big]\cdot \big[\bm{I}-\bm{X}\big]^\text{T}\bigg)\end{align}$$4、高斯混合模型完全数据集对数似然函数期望$$\begin{align}&amp;\mathrm{E}\big[\ell \big(\mathcal{D}^+\mid \bm{\pi},\bm{\mu},\bm{C}\big)\big]=\bm{I}^\text{T}\mathrm{E}\big[\bm{Z}\big]\ln\bm{\pi}\\ &amp;+ \frac{1}{2}\sum_{k=1}^K\Bigg[-k\times\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln 2\pi+\bm{I}^\text{T}\mathrm{E}\big[\bm{z}_k\big]\ln\big[\bm{\Lambda}_k\big]-\mathrm{tr}\bigg(\mathrm{E}^\text{T}\big[\bm{z}_k\big]\odot \bm{S}_{\bm{\mu}_k}\bm{\Lambda}_k\bigg)\Bigg]\end{align}$$也就是说，知道了完全数据集对数似然函数期望，我们就可以递归求解了，而这就是期望最大算法EM5、然后EM要求混合模型已知混合了多少类型： $\displaystyle k$。显然上帝不会轻易告诉我们，我们需要继续征程。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/cfb848a15208557d/如果您需要引用本文，请参考：引线小白. (Mar. 14, 2017). 《EM算法和混合模型二：混合模型案例》[Blog post]. Retrieved from https://www.limoncc.com/post/cfb848a15208557d@online{limoncc-cfb848a15208557d,title={EM算法和混合模型二：混合模型案例},author={引线小白},year={2017},month={Mar},date={14},url={\url{https://www.limoncc.com/post/cfb848a15208557d}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>混合模型</tag>
        <tag>隐变量模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法和混合模型一：EM算法]]></title>
    <url>%2Fpost%2Fbaeffd0a1d876b77%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/baeffd0a1d876b77/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、EM算法1.1、问题表述考虑独立同分布数据集 $\displaystyle \mathcal{D}^\bm{x}=\{\bm{x}_i\}_{i=1}^N$，我们加入隐含变量 $\displaystyle \bm{z}\in \mathcal{C}$。这样我们有完整数据集(complete data)：$\displaystyle \mathcal{D}^{+}=\{\bm{x}_i,\bm{z}_i\}_{i=1}^N$。 $\displaystyle \mathcal{D}^{+}$依然是独立同分布的，同时我们把 $\displaystyle \mathcal{D}^\bm{x}$称为不完整数据集(incomplete data)。于是我们有：$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\sum_{i=1}^N\ln p\big(\bm{x}\mid \bm{\theta}\big)=\sum_{i=1}^N\ln \bigg[\int_{\mathcal{C}} p\big(\bm{x},\bm{z}\mid \bm{\theta}\big)\mathrm{d}\bm{z}\bigg]\end{align}$$我们用数据集表示：$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\ln p\big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\ln \bigg[\int_{\mathcal{C}^N} p\big(\mathcal{D}^\bm{x},\mathcal{D}^\bm{z}\mid \bm{\theta}\big)\mathrm{d}\mathcal{D}^\bm{z}\bigg]\end{align}$$对于离散情况，我们把积分换成求和即可。 1.2、痛点问题：对数里求和的复杂性对于不完全数据集，也就是我们观测到的数据集的对数似然函数 $\displaystyle \ln p\big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)$。它的形式通常是复杂，尤其在想最大化参数时,对数里面居然有求和(积分)：$\displaystyle \ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\sum_{i=1}^N\ln p\big(\bm{x}\mid \bm{\theta}\big)=\sum_{i=1}^N\ln \big[\sum_{\bm{z}\in\mathcal{C}} p\big(\bm{x},\bm{z}\mid \bm{\theta}\big)\big]$这是很很头疼的问题。 1.3、完全数据集求解的简单性隐变量加入后：求解 $\displaystyle \ell\big(\mathcal{D}^+\mid\bm{\theta}\big)$要比求解 $\displaystyle \ell(\mathcal{D}^\bm{x}\mid \bm{\theta})$容易：$$\begin{align}\ell\big(\mathcal{D}^+\mid\bm{\theta}\big)&amp;=\ln\bigg[\prod_{i=1}^N \bigg(p\big(\bm{z}\mid \bm{\pi}\big)p\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\bigg)\bigg]=\ln\bigg[\prod_{i=1}^N p\big(\bm{z}\mid \bm{\pi}\big)\times \prod_{i=1}^Np\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\bigg]\\&amp;=\ln\bigg[\prod_{i=1}^N p\big(\bm{z}\mid \bm{\pi}\big)\bigg]+\ln\bigg[\prod_{i=1}^Np\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\bigg]\\&amp;=\sum_{i=1}^N\ln p\big(\bm{z}\mid \bm{\pi}\big)+\sum_{i=1}^N \ln p\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\end{align}$$对数里的求和自然消失，变成了我们熟悉的形式。 1.4、隐变量后验分布与数据点性质混合模型中，我们会定义隐变量的后验分布：$$\begin{align}p\big(\bm{z}\mid \bm{x},\bm{\theta}\big)=\frac{p\big(\bm{x},\bm{z}\mid \bm{\theta}\big)}{p\big(\bm{x}\mid \bm{\theta}\big)}=\frac{p\big(\bm{x},\bm{z}\mid \bm{\theta}\big)}{\displaystyle\int_{\mathcal{C}}p\big(\bm{x},\bm{z}\mid \bm{\theta}\big)\mathrm{d}\bm{z}}\end{align}$$我们变形一下来说明一个重要引理： 【数据点对数密度分解引理】不完全数据点对数密度可以被分解。$$\begin{align}\ln p\big(\bm{x}_i\mid \bm{\theta}\big)=\mathrm{E}_{q_i}\big[\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q_i,p_i\big]\end{align}$$它的分解是1、隐变量后验分布下，完全数据点对数密度期望2、隐变量后验似然函数分布的交叉熵证明：$$\begin{align}p\big(\bm{x}_i\mid \bm{\theta}\big)=\frac{p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)}{p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}\big)}\iff\ln p\big(\bm{x}_i\mid \bm{\theta}\big)=\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)-\ln p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}\big)\end{align}$$然后针对一个后验分布 $\displaystyle q_i=p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}^{old}\big)$求期望：$$\begin{align}\mathrm{E}_{q_i}\big\{\ln p\big(\bm{x}_i\mid \bm{\theta}\big)\big\}=\mathrm{E}_{q_i}\big\{\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)\big\}-\mathrm{E}_{q_i}\big\{\ln p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}\big)\big\}\end{align}$$于是有：$$\begin{align}\ln p\big(\bm{x}_i\mid \bm{\theta}\big)&amp;=\int_{\mathcal{C}} \big[\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)\times q_i\big]\mathrm{d}\bm{z}_i-\int_{\mathcal{C}} \big[\ln p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}\big)\times q_i\big]\mathrm{d}\bm{z}_i\\&amp;=\mathrm{E}_{q_i}\big[\ln p\big(\bm{x}_i,\bm{z}_i\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q_i,p_i\big]\end{align}$$证毕。这一结论是EM算法中隐变量基本变换技巧，起着至关重要的作用。 1.5、隐变量的若干符号为简洁记，我们定义一下符号，这些符号的定义有助于我们清晰理解EM算法是如何工作的。$\displaystyle q_i=p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}^{old}\big)$， $\displaystyle p_i=p\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}\big)$$\displaystyle q_i^t=q\big(\bm{z}_i\mid \bm{x}_i,\bm{\theta}_t\big)$$\displaystyle q=p\big(\mathcal{D}^\bm{z}\mid \mathcal{D}^\bm{x},\bm{\theta}^{old}\big)$， $\displaystyle p=p\big(\mathcal{D}^\bm{z}\mid \mathcal{D}^\bm{x},\bm{\theta}\big)$$\displaystyle q^t=p\big(\mathcal{D}^\bm{z}\mid \mathcal{D}^\bm{x},\bm{\theta}_t\big)$也就是说隐变量后验分布可以通过参数 $\displaystyle \bm{\theta}_t$不断更新。事实上我们可以得到一个隐变量后验分布序列，并且收敛：$$\begin{align}q_i^1\to q_i^2\to \cdots\to q_i^t\to q_i^{t+1}\to\cdots\to q_i^{end}\end{align}$$ 1.6、不完全数据集对数似然函数等价变换【不完全数据集对数似然函数等价变换引理】不完全数据集对数似然函数可以被分解。$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q,p\big]=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\end{align}$$它的分解是1、隐变量后验分布下，完全数据集对数似然函数的期望2、隐变量后验似然函数分布的交叉熵 证明：我们参考【数据点对数密度分解引理】和隐变量符号有：$$\begin{align}p \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\frac{p \big(\mathcal{D}^+\mid \bm{\theta}\big)}{p \big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)}\iff\ln p \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\ln p \big(\mathcal{D}^+\mid \bm{\theta}\big)-\ln p \big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)\end{align}$$ 针对隐变量数据集的一个新后验分布 $\displaystyle q=q\big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}^{old}\big)$求期望有：$$\begin{align}\mathrm{E}_{ q}\left[\ln p \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\right]=\mathrm{E}_{ q\big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)}\left[\ln p \big(\mathcal{D}^+\mid \bm{\theta}\big)\right]-\mathrm{E}_{ q\big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)} \left[\ln p \big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)\right]\end{align}$$于是有：$$\begin{align}\ln p \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)&amp;=\mathrm{E}_{q}\left[\ln p \big(\mathcal{D}^+\mid \bm{\theta}\big)\right]-\mathrm{E}_{q} \left[\ln p \big(\mathcal{D}^z\mid \mathcal{D}^\bm{x}, \bm{\theta}\big)\right]\\&amp;=\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}^+\mid \bm{\bm{\theta}}\big)\big]+\mathrm{H}\big[q,p\big]\\&amp;=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\\\end{align}$$注意，我们使用了乘法公式、熵的性质从而我们把数据点的规律，上升到数据集。这说明了独立同分布的可以分解性。 1.7、EM算法收敛定律【EM算法收敛定律】存在收敛参数，使得不完全数据集对数似然函数的最大化与完全数据集对数似然函数期望的最大化等价。$$\begin{align}\exists \,\bm{\theta}_{end}\to\max\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\iff\max \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]\end{align}$$ 证明1、下面我们引入一个辅助函数(auxiliary function): $\displaystyle \mathcal{Q}\big(\bm{\theta},q\big)\triangleq\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]$有$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)&amp;=\mathcal{Q}\big(\bm{\theta},q\big)+\mathrm{KL}\big[q\parallel p\big]\\&amp;\geqslant\mathcal{Q}\big(\bm{\theta},q\big)=\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]\\&amp;\geqslant\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]\end{align}$$现在我们略微改变一下辅助函数(auxiliary function) $\displaystyle \mathcal{Q}$的观察视角 ：$$\begin{align}\mathcal{Q}\big({\bm{\theta},\bm{\theta}}_t\big)=\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q^t\big]\end{align}$$ 同时有 $\displaystyle \ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\geqslant\mathcal{Q}\big({\bm{\theta},\bm{\theta}}_t\big)\geqslant\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]$，也就是说 $\displaystyle \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]$是 $\displaystyle \ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)$的下界。 2、下面我们将要构造出 $\displaystyle \bm{\theta}_{end}$，同时证明最大化等价。令 $\displaystyle \bm{\theta}_{t+1}=\mathop{\mathrm{argmax}}_{\bm{\theta}} \mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)$ 则： $\displaystyle \mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}_{t+1}\big)-\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]\geqslant0$，同时$\displaystyle \ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\geqslant\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]$于是有$$\begin{align}&amp;\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}_{t+1}\big)-\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\\&amp;\geqslant\mathcal{Q}\big({\bm{\theta}_{t+1},\bm{\theta}}_t\big)-\mathcal{Q}\big({\bm{\theta},\bm{\theta}}_t\big)\\&amp;\geqslant\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}_{t+1}\big)-\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}_t\big)\big]\geqslant0\end{align}$$通过迭代 $\displaystyle \bm{\theta}$， 使得 $\displaystyle \ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}_{t+1}\big)\geqslant\ell\big(\mathcal{D}^\bm{x}\mid \bm{\theta}_{t}\big)$，$\displaystyle t$步收敛后从而实现 $\displaystyle \max_{\bm{\theta}_{t+1}}\ell(\mathcal{D}\mid \bm{\theta}_{t+1})$这就证明了结论。 1.8、EM算法我们总结前面的结论，写出EM算法。 算法：EM算法 1 $\displaystyle \bm{\theta}_0=\bm{a}$2 $\displaystyle p\big(\mathcal{D}^\bm{z}\mid \bm{\pi}\big)=\prod_{i=1}^Np \big(\bm{z}\mid \bm{\pi}\big)$3 $\displaystyle \text{while }\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}_{t+1}\big)=\text{converged}$ :$\displaystyle\quad\begin{array}{|lc}\text{ E step}\\\text{—————-}\\q\big(\mathcal{D}^\bm{z}\mid \mathcal{D}^\bm{x},\bm{\theta}_t\big)=\frac{p\big(\mathcal{D}^\bm{x},\mathcal{D}^\bm{z}\mid \bm{\theta}_t\big)}{\displaystyle\int_{\mathcal{C}^N}p\big(\mathcal{D}^\bm{x},\mathcal{D}^\bm{z}\mid \bm{\theta}_t\big)\mathrm{d}\mathcal{D}^\bm{z}}\\p\big(\mathcal{D}^\bm{x},\mathcal{D}^\bm{z}\mid \bm{\theta}\big)=p\big(\mathcal{D}^\bm{x}\mid \mathcal{D}^\bm{z},\bm{\theta}\big)p\big(\mathcal{D}^\bm{z}\mid \bm{\pi}\big)=\prod_{i=1}^Np\big(\bm{x}\mid \bm{z},\bm{\theta}\big)\times \prod_{i=1}^Np\big(\bm{z}\mid \bm{\pi}\big)\\\mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]=\int q\big(\mathcal{D}^\bm{z}\mid \mathcal{D}^\bm{x},\bm{\theta}_t\big)\ln p\big(\mathcal{D}^\bm{x},\mathcal{D}^\bm{z}\mid \bm{\theta}\big)\mathrm{d}\mathcal{D}^\bm{z}\\\text{ M step}\\\text{—————-}\\\bm{\theta}_{t+1}=\mathop{\mathrm{argmax}}_{\bm{\theta}} \mathrm{E}_{q^t}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\\\end{array}\\$4 $\displaystyle \bm{\theta}_{end}$ #end while 二、评述1、EM算法的关键在于引入隐变量，然后我们考察它的先验、后验和完全数据集分布，于是我们得到： 不完全数据集对数似然函数等价变换：$$\begin{align}\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q,p\big]=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\end{align}$$正是因为如此，我们才有了【EM算法收敛定律】:$$\begin{align}\exists \,\bm{\theta}_{end}\to\max\ell \big(\mathcal{D}^\bm{x}\mid \bm{\theta}\big)\iff\max \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \bm{\theta}\big)\big]\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/baeffd0a1d876b77/如果您需要引用本文，请参考：引线小白. (Mar. 13, 2017). 《EM算法和混合模型一：EM算法》[Blog post]. Retrieved from https://www.limoncc.com/post/baeffd0a1d876b77@online{limoncc-baeffd0a1d876b77,title={EM算法和混合模型一：EM算法},author={引线小白},year={2017},month={Mar},date={13},url={\url{https://www.limoncc.com/post/baeffd0a1d876b77}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>EM算法</tag>
        <tag>混合模型</tag>
        <tag>隐变量模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场模型]]></title>
    <url>%2Fpost%2F875b887a5e7e39e56eea929a72336d4e%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/875b887a5e7e39e56eea929a72336d4e/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清机器学习的基础问题。若有错误，请大家指正。关键词: 无向图,哈默斯利-克利福德定理,条件随机场 [TOC] 一、哈默斯利-克利福德定理Hammersley-Clifford定理由于没有与无向图相关的拓扑排序，我们不能使用链式法则来表示 $ p \big(\bm{x}\big)$。因此，我们将概率密度与每个节点关联起来替换方法是，将势函数或因子与图中每个极大团联系起来。我们将团 $ c$势函数记为 $ \psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)$。势函数可以是其参数的任何非负函数。然后定义联合分布与团势函数乘积成比例。令人惊讶的是，可以通过这样的方式来表示任何可以由无向图模型表达条件独立性质的正分布。我们更正式地陈述这个结果如下 【 Hammersley-Clifford 定理】一个正分布 $ p \big(\bm{x}\big)&gt;0$ 能满足一个无向图模型 $ \mathcal{G}$的条件独立性质，当且仅当 $ p$能够被每个极大团因子的乘积表示。 $$\begin{align}\forall x\to p(x)&gt;0\Rightarrow \mathcal{I}(p)=\mathcal{I}(\mathcal{G})\iff p \big(\bm{x}\mid \bm{\theta}\big)=\frac{1}{Z(\bm{\theta})}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 其中 $ \mathcal{C}$是图 $ \mathcal{G}$ 所有极大团的集合。 $ Z(\bm{\theta})$是配分函数：$$\begin{align}Z(\bm{\theta})=\sum_{\bm{x}}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 请注意，配分函数确保了整体分布总和为 $ 1$。 二、条件随机场Conditional random ﬁelds (CRFs)2.1、一般定义条件随机场conditional random ﬁeld(CRF) (Lafferty et al . 2001年)，又称判别随机场 (Kumar and Hebert 2003)，是 MRF 的一个版本，其中所有团势以输入特征为条件: $$\begin{align}p \big(\bm{z}\mid \bm{X},\bm{w}\big)=\frac{1}{Z \big(\bm{X},\bm{w}\big)}\prod_{c\in \mathcal{C}} \psi_c \big(\bm{z}_c\mid \bm{X},\bm{w}\big)\end{align}$$ CRF 可以被认为是逻辑回归的结构化输出扩展，在此模型中，我们对输入特征的输出标签之间的关联进行建模。我们通常会假设一个对数线性表示的势能: $$\begin{align}\psi_c \big(\bm{z}_c\mid \bm{X},\bm{w}\big)=\exp \big[\bm{w}_c ^\text{T} \bm{\phi} \big(\bm{X},\bm{z}_c\big)\big]\end{align}$$其中 $ \bm{\phi} \big(\bm{X},\bm{z}_c\big)$是一个由全局输入 $ \bm{X}$和局部标签 $ \bm{z}_c$所衍生的特征向量。 为了方便描述，我们形式化的定义隐变量数据集 $\displaystyle \mathcal{D}_T^z=\{z_t\}_{t=1}^T$，观测变量数据集$\displaystyle \mathcal{D}_T^{\bm{x}}=\{\bm{x}_t\}_1^T=:\bm{X}=[\bm{x}_1,\cdots,\bm{x}_T]^T$, 完全数据集$\displaystyle \mathcal{D}^+_T=\{\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\}$， 条件数据集 $\displaystyle \mathcal{D}^{\cdot|\cdot}_T=\{\mathcal{D}_T^\bm{x}\mid\mathcal{D}_T^z\}$ 。于是有 $$\begin{align}p\big(\bm{z}\mid\bm{X}\big)&amp;=p\big(\mathcal{D}^{\cdot|\cdot}_T\big)=p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}\big)=\frac{1}{Z \big(\bm{X}\big)}\prod_{c\in \mathcal{C}} \psi_c \big(\bm{z}_c\mid \bm{X},\bm{w}\big)\\&amp;=\frac{1}{Z \big(\bm{X}\big)}\prod_{c\in \mathcal{C}}\exp \big[\bm{w}_c ^\text{T} \bm{\phi} \big(\bm{X},\bm{z}_c\big)\big]\\\end{align}$$ 其中$\displaystyle Z(\bm{X})=\sum_{\bm{z}\in \mathcal{Z}}\prod_{c\in \mathcal{C}}\exp \big[\bm{w}_c ^\text{T} \bm{\phi} \big(\bm{X},\bm{z}_c\big)\big]$ 2.2、边势能和节点势能为了让符号与原理更加清晰，我们来明确定义一下势能函数 $\displaystyle \exp \big[\bm{w}_c ^\text{T} \bm{\phi} \big(\bm{X},\bm{z}_c\big)\big]$：注意到条件随机场的极大团 $\displaystyle \{z_{t-1},z_t\}_{2}^T,\{z_t,\bm{X}\}_1^T$，我们分别定义边势能，和节点势能。或者叫转移分数，和发射分数。同时为简单记我们直接不同参数 $\displaystyle \lambda,\mu $融入到相应函数中， 进一步也可以把指数符号 $\displaystyle \exp$融入到相应函数中，于是有 $$\begin{align}\exp \big[\bm{w}_c ^\text{T} \bm{\phi} \big(\bm{X},\bm{z}_c\big)\big]&amp;=\exp\big[[\lambda,\mu][s(z_{t-1},z_t,\bm{X}),k(z_t,\bm{X})]^T\big]\\&amp;=\exp\big[\lambda s(z_{t-1},z_t,\bm{X})+ \mu k(z_t,\bm{X}) \big]\\&amp; :\Rightarrow\exp\big[s(z_{t-1},z_t,\bm{X})+k(z_t,\bm{X}) \big]\\&amp; :\Rightarrow s\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\end{align}$$ 图: 条件随机场模型 代入，我们得到一个更加简洁，清晰的条件数据分布， $$\begin{align}p\big(\bm{z}\mid\bm{X}\big)&amp;=p\big(\mathcal{D}^{\cdot|\cdot}_T\big)=p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}\big)=\frac{1}{Z\big(\bm{X}\big)}\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)\prod_{t=1}^Tk\big(z_t,\bm{X}\big)\\&amp;=\frac{1}{Z\big(\bm{X}\big)}k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\\\end{align}$$ 其中$\displaystyle Z(\bm{X})=\sum_{z_{1},\cdots,z_T\in \mathcal{Z}}\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)\prod_{t=1}^Tk\big(z_t,\bm{X}\big)$ 为了简洁记，我们来定义一些符号。考虑隐变量有 $\displaystyle C$个状态 $\displaystyle z_t\in\{1,\cdots,C\}=\mathcal{Z}$，观测变量有 $\displaystyle O$个状态 $\displaystyle \bm{x}_t\in\{1,\cdots,O\}=\mathcal{X}$. 如果是文本，就是有多少个词。注意这里形式化的使用 $ o$来表示向量 $ \bm{x}_t$的状态，也就是说我们并没有定义向量是什么，因为这样能节约符号，和叙述上不必要的麻烦。继续定义若干参数：$\displaystyle \pi[v]=k\big(z_1=v,\bm{X}\big)$$\displaystyle A_t[i,j]=s\big(z_{t-1}=i,z_t=j,\bm{X})$$\displaystyle \rho_t[j]=k\big(z_t=j,\bm{X})$令参数集合为：$$\begin{align}\bm{\theta}=\{\bm{\pi},\bm{A}\}\end{align}$$ 注意在实际应用中，我们会加 $\displaystyle \log$，避免数据上溢，和下溢：$\displaystyle s\big(z_{t-1},z_t,\bm{X}\big) = \exp\big[\ln A_t[i,j]\big]$、$\displaystyle k\big(z_t,\bm{X}\big) = \exp\big[\ln \rho_t[j]\big]$。同时 $\displaystyle \pi[v]$中的 $v$一般指的是标签 $\displaystyle $ 三、前向向后算法3.1、直接计算我们要解决如何高效求条件数据集分布的问题，下面我们要分析一下这个基本问题。 $$\begin{align}p\big(\mathcal{D}^{\cdot|\cdot}_T,\bm{\theta} \big)&amp;=p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}, \bm{\theta} \big)=\frac{1}{Z\big(\bm{X}\big)}k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\\&amp;=\frac{1}{Z\big(\bm{X}\big)}\pi[v]\prod_{t=2}^TA_t[i,j]\rho_t[j]\end{align}$$ 我们取对数有 $$\begin{align}\ln p\big(\mathcal{D}^{\cdot|\cdot}_T\mid \bm{\theta} \big)&amp;=\ln p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x},\bm{\theta} \big)=\ln\bigg[\frac{1}{Z\big(\bm{X}\big)}\pi[v]\prod_{t=2}^TA_t[i,j]\rho_t[j]\bigg]\\&amp;=\underbrace{\mathop{\ln \pi[v]+\sum_{t=2}^T\big[\ln A_t[i,j]+\ln \rho_t[j]\big]}}_{\ln\text{true path}} - \underbrace{\mathop{\ln Z(\bm{X})}}_{\ln\text{total path}}\end{align}$$ 在给定条件数据集 $\displaystyle \mathcal{D}^{\cdot|\cdot}_T=\{\mathcal{D}_T^\bm{x}\mid\mathcal{D}_T^z\}=\{\bm{x}_1,\cdots,\bm{x}_T\mid z_1,\cdots,z_T\}$，条件数据集分布的计算分为两部分： $$\begin{align}\ln\text{true_path} = \ln \pi[v]+\sum_{t=2}^T\big[\ln A_t[i,j]+\ln \rho_t[j]\big]\end{align}$$计算复杂度是 $\displaystyle O(2T)$。为什么取名 true path和 total path，如下图： 图: 条件随机场直接计算复杂度 难点在于第二部的计算 $$\begin{align}\ln\text{total_path}&amp;= \ln Z(\bm{X})= \ln \sum_{i,j\in \mathcal{Z}}\bigg[ \pi[v]\prod_{t=2}^TA_t[i,j]\rho_t[j]\bigg]\\&amp;=\ln \pi[v]+\ln\bigg[\underbrace{\mathop{\sum_{i,j\in \mathcal{Z}}\prod_{t=2}^T}}_{O(2TC^T)}A_t[i,j]\rho_t[j]\bigg]\end{align}$$ 这个方法的计算复杂度是 $\displaystyle O\big({2TC}^T\big)$, 例如 $\displaystyle C=5,T=10$，那么就要约计算 $\displaystyle 1.95\times10^8$已经非常夸张了。为了更快的计算条件分布，我们需要充分利用条件随机场的马尔可夫的性质。 3.2、前向计算我们单独考虑 $\displaystyle \text{total_path}$： $\displaystyle Z(\bm{X})=\sum_{z_{1},\cdots,z_T\in \mathcal{Z}}\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)\prod_{t=1}^Tk\big(z_t,\bm{X}\big)$。 我们从序列的第 $\displaystyle 1$步看到第 $\displaystyle t$步，并令 $\displaystyle t$步状态为 $\displaystyle j$。这种操作也称为 sum-out。于是可以观测到 $$\begin{align}\alpha_t[j] &amp;= \mathop{Z}^{\rightharpoonup}\big(\bm{X},z_t=j\big)\\ &amp;=\sum_{z_1,\cdots,z_{t-1}\in \mathcal{Z}}k\big(z_1,\bm{X}\big)\prod_{\tau=2}^{t-1}\bigg[s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big)\times s\big(z_{t-1},z_t=j,\bm{X}\big)k\big(z_t=j,\bm{X}\big)\bigg]\\ &amp;=\sum_{z_{t-1}=i\in \mathcal{Z}}\sum_{z_1,\cdots,z_{t-2}\in \mathcal{Z}}k\big(z_1,\bm{X}\big)\prod_{\tau=2}^{t-2}\bigg[s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big) \times s\big(z_{t-2},z_{t-1}=i,\bm{X}\big)k\big(z_{t-1}=i,\bm{X}\big)\\&amp;\times s\big(z_{t-1}=i,z_t=j,\bm{X}\big)k\big(z_t=j,\bm{X}\big)\bigg]\\ &amp;=\sum_{z_{t-1}=i\in \mathcal{Z}}\Bigg[\sum_{z_1,\cdots,z_{t-2}\in \mathcal{Z}}\bigg[k\big(z_1,\bm{X}\big)\prod_{\tau=2}^{t-2}s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big)\bigg]\\&amp;\times s\big(z_{t-2},z_{t-1}=i,\bm{X}\big)k\big(z_{t-1}=i,\bm{X}\big)\times s\big(z_{t-1}=i,z_t=j,\bm{X}\big)\Bigg]\\&amp;k\big(z_t=j,\bm{X}\big)\\ &amp;=\sum_{z_{t-1}=i\in \mathcal{Z}}\alpha_{t-1}[i]A_t[i,j]\rho_t[j]\end{align}$$ $$\begin{align} \alpha_t[j]=\rho_t[j]\sum_{z_{t-1}=i\in \mathcal{Z}}\bigg[\alpha_{t-1}[i]\cdot A_t[i,j]\bigg]\end{align}$$ 易知道 $\displaystyle \alpha_1[i]=\pi[i]$，我们把它写成向量形式 $$\begin{align}\bm{\alpha}_t = \bm{\rho}_t\odot\big[\bm{A}^T\bm{\alpha}_{t-1}\big]\end{align}$$ 这种操作也叫 sum-product的操作，即是我们把求和算子转化了乘积算子 $ \sum\to \prod$ 于是 $\displaystyle \text{total_path}$就是 $$\begin{align}\ln \text{total_path}= \ln Z\big(\bm{X}\big)= \ln \sum_{z_T=i\in \mathcal{Z}} \mathop{Z}^{\rightharpoonup}\big(\bm{X},z_T=i\big)= \ln \sum_{i\in \mathcal{Z}}\alpha_{T}[i]= \ln \bm{I}^T\bm{\alpha}_T\end{align}$$ 计算复杂变成了 $\displaystyle O\big(TS^2\big)$ 3.3、前向后向算法我们也可以反向从序列的第 $\displaystyle T$步看到第 $\displaystyle t$步，可以观测到 $$\begin{align}\beta_t[i]&amp;=\mathop{Z}^{\leftharpoonup}\big(\bm{X},z_t=i\big)\\&amp;=\sum_{z_{t+1},\cdots,z_T\in \mathcal{Z}}s\big(z_{t}=i,z_{t+1},\bm{X}\big)k\big(z_{t+1},\bm{X}\big)\prod_{\tau=t+2}^{T}s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big)\\&amp;=\sum_{z_{t+1}=j\in \mathcal{Z}}s\big(z_{t}=i,z_{t+1}=j,\bm{X}\big)k\big(z_{t+1}=j,\bm{X}\big)\\&amp;\sum_{z_{t+2},\cdots,z_T\in \mathcal{Z}}s\big(z_{t+1}=j,z_{t+2},\bm{X}\big)k\big(z_{t+2},\bm{X}\big)\prod_{\tau=t+3}^{T}s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big)\\&amp;=\sum_{z_{t+1}=j\in \mathcal{Z}}s\big(z_{t}=i,z_{t+1}=j,\bm{X}\big)k\big(z_{t+1}=j,\bm{X}\big)\beta_{t+1}[j]\\&amp;=\sum_{z_{t+1}=j\in \mathcal{Z}}A_{t+1}[i,j]\rho_{t+1}[j]\beta_{t+1}[j]\end{align}$$即是 $$\begin{align}\beta_t[i]=\sum_{z_{t+1}=j\in \mathcal{Z}}A_{t+1}[i,j]\rho_{t+1}[j]\beta_{t+1}[j]\end{align}$$ 写成矩阵形式有 $$\begin{align}\bm{\beta}_t=\bm{A}\big[\bm{\rho}_{t+1}\odot \bm{\beta}_{t+1}\big]\end{align}$$ 于是 $\displaystyle \text{total_path}$就是 $$\begin{align}\ln \text{total_path}&amp;= \ln Z\big(\bm{X}\big)= \ln \sum_{z_t=i\in \mathcal{Z}} \mathop{Z}^{\rightharpoonup}\big(\bm{X},z_t=i\big)\mathop{Z}^{\leftharpoonup}\big(\bm{X},z_t=i\big)\\&amp;= \ln \sum_{i\in \mathcal{Z}}\alpha_{t}[i]\beta_t[i]= \ln \bm{I}^T\big[\bm{\alpha}_t\odot\bm{\beta}_t\big]\end{align}$$ 这就是前向后向算法。 图: 条件随机场信念传播 3.4、一些概率与期望的计算利用前向后向算法，很容实现一些计算： 1、条件概率$$\begin{align}p\big(z_t=i\mid\bm{X}\big)&amp;=p\big(z_t=i\mid \mathcal{D}_T^\bm{x}\big)=\frac{\bm{\alpha}_t\odot\bm{\beta}_t}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\\p\big(z_{t-1}=j,z_t=i\mid\bm{X}\big)&amp;=p\big(z_t=i\mid \mathcal{D}_T^\bm{x}\big)=\frac{\bm{A}^T\bm{\alpha}_{t-1}\odot\bm{\beta}_t}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\end{align}$$ 2、期望值 $$\begin{align}\mathrm{E}_{p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}\big)}\bigg[s\big(z_{t-1},z_t,\bm{X}\big)\bigg]&amp;=\sum_{z_{t-1},z_t\in \mathcal{D}}s\big(z_{t-1},z_t,\bm{X}\big)\frac{\alpha_{t-1}[z_{t-1}]A[z_{t-1},z_t]\beta_t[z_t]}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\\\mathrm{E}_{p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}\big)}\bigg[k\big(z_t,\bm{X}\big)\bigg]&amp;=\sum_{z_t\in \mathcal{D}}k\big(z_t,\bm{X}\big)\frac{\alpha_{t}[z_{t}]\beta_t[z_t]}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\end{align}$$ 若有观测数据集经验分布 $\displaystyle \tilde{p}\big(\bm{X}\big)= \tilde{p}\big(\mathcal{D}_T^\bm{x}\big)$ $$\begin{align}\mathrm{E}_{p\big(\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\big)}\bigg[s\big(z_{t-1},z_t,\bm{X}\big)\bigg]&amp;=\sum_{\bm{X}\in \mathcal{X}}\tilde{p}\big(\bm{X}\big)\sum_{z_{t-1},z_t\in \mathcal{D}}s\big(z_{t-1},z_t,\bm{X}\big)\frac{\alpha_{t-1}[z_{t-1}]A[z_{t-1},z_t]\beta_t[z_t]}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\\\mathrm{E}_{p\big(\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\big)}\bigg[k\big(z_t,\bm{X}\big)\bigg]&amp;=\sum_{\bm{X}\in \mathcal{X}}\tilde{p}\big(\bm{X}\big)\sum_{z_t\in \mathcal{D}}k\big(z_t,\bm{X}\big)\frac{\alpha_{t}[z_{t}]\beta_t[z_t]}{\bm{I}^T\bm{\alpha}_t\cdot\bm{\beta}_t}\end{align}$$ 四、维特比算法4.1、问题描述维比特算法要解决的是求隐变量最可能状态序列。可以理解为：已知观测变量数据集，推断隐变量数据集。 $$\begin{align}\widehat{\mathcal{D}}_T^z=\widehat{\bm{z}}=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^z}p\big(\mathcal{D}_T^{\cdot\mid\cdot}\big)=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^z}p\big(\mathcal{D}_T^z\mid \mathcal{D}_T^\bm{x}\big)=\mathop{\mathrm{argmax}}_{\bm{z}}p\big(\bm{z}\mid \bm{X}\big)\end{align}$$ 我们有条件数据集分布$$\begin{align}p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x} \big)&amp;=\frac{1}{Z\big(\bm{X}\big)}k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\\&amp;=\frac{1}{Z\big(\bm{X}\big)}\pi[v]\prod_{t=2}^TA_t[i,j]\rho_t[j]\end{align}$$ 令 $\displaystyle g\big(\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\big)=g\big(\bm{z},\bm{X}\big)=k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)=\pi[v]\prod_{t=2}^TA_t[i,j]\rho_t[j]$ 则有，我们去掉了一个常数 $1/Z\big(\bm{X}\big)$，以简化计算。$$\begin{align}\widehat{\mathcal{D}}_T^z&amp;=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^z}p\big(\mathcal{D}_T^z\mid \mathcal{D}_T^\bm{x}\big)\propto \mathop{\mathrm{argmax}}_{\mathcal{D}_T^z}g\big(\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\big)\end{align}$$ 也就是说我们选择最大化 $$\begin{align}\max_{\mathcal{D}_T^z}g\big(\mathcal{D}_T^z,\mathcal{D}_T^\bm{x}\big)&amp;=\max_{\mathcal{D}_T^z}\bigg[k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\bigg]\\&amp;=\max_{\mathcal{D}_T^z}\bigg[\pi[i_1]\prod_{t=2}^TA_t[i_{t-1},i_{t}]\rho_t[i_{t}]\bigg]\end{align}$$ 其中 $\displaystyle i_t$是隐变量 $\displaystyle z_t$在时刻 $\displaystyle t$的状态。请反复体会这翻来覆去的表达式变换。不同的形式在不同场景下有利于说明不同的问题。 4.2、前向计算我们可以拆分隐变量数据集 $\displaystyle \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup\{z_t\}$，这一拆分的关键自觉是时刻 $\displaystyle 1 \rightsquigarrow t$的最可能路径必须是由 $\displaystyle 1 \rightsquigarrow t-1$的最可能路径组成的。问题就变成 $$\begin{align}\max_{\mathcal{D}_t^z}g\big(\mathcal{D}_t^z,\mathcal{D}_t^\bm{x}\big)&amp;=\max_{z_t}\max_{\mathcal{D}_{t-1}^z}g\big(\mathcal{D}_{t-1}^z,z_t,\mathcal{D}_t^\bm{x}\big)\end{align}$$ 这种操作也称为 max-out. 已知条件数据集和模型参数，计算条件数据集分布概率的前向计算中我们使用了一种 sum-product的操作简化了问题，同样在维比特算法的前向计算中，我们也可以使用叫做 max-product的操作来简化问题。追寻这一思想，下面来具体分析一下。令时刻 $\displaystyle t$的状态为 $\displaystyle i_t$，进而定义路径 $\displaystyle \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup\{z_t=i_t\}$的最大权重： $$\begin{align}\delta_t[i_t]=\max_{\mathcal{D}_{t-1}^z}g\big(\mathcal{D}_{t-1}^z,z_t,\mathcal{D}_t^\bm{x}\big)\end{align}$$ 我们继续拆分数据集：令时刻 $\displaystyle t-1$的状态为 $\displaystyle i_{t-1}$，于是有 $$\begin{align}\delta_t[i_t]&amp;=\max_{\mathcal{D}_{t-1}^z}g\big(\mathcal{D}_{t-1}^z,z_t,\mathcal{D}_T^\bm{x}\big)\\&amp;=\max_{\mathcal{D}_{t-2}^z,z_{t-1}=i_{t-1}}\bigg[k\big(z_1,\bm{X}\big)\prod_{\tau=2}^{t-2}s\big(z_{\tau-1},z_\tau,\bm{X}\big)k\big(z_\tau,\bm{X}\big)s\big(z_{t-2},z_{t-1}=i_{t-1},\bm{X}\big)k\big(z_{t-1}=i_{t-1},\bm{X}\big)\\&amp;\quad s\big(z_{t-1}=i_{t-1},z_{t}=i_{t},\bm{X}\big)k\big(z_{t}=i_{t},\bm{X}\big)\bigg]\\&amp;=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\end{align}$$ 也就是说：时刻 $\displaystyle t$行至状态 $\displaystyle i_t$的最可能路径必须有是由时刻$\displaystyle t-1$行至其他状态 $\displaystyle i_{t-1}$的最可能路径中权重最大的那个。 $$\begin{align}\delta_t[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\end{align}$$ 写成矩阵形式固定 $\displaystyle i_t$，令$\displaystyle a[i_{t-1}]=A[i_{t-1},i_t]$则有 $$\begin{align}\delta_t[i_t],\widehat{i}_{t-1}[z_t=i_t]&amp;=\max[\bm{\delta}_{t-1}\odot\bm{a}_{t-1}]\rho_t[i_t]\\\bm{\delta}_t,\widehat{\bm{i}}_{t-1}&amp;=\big[\mathop{\mathrm{colmax}}[\bm{\delta}_{t-1}\odot\bm{A}]^\text{T}\odot\bm{\rho}_t\end{align}$$ 加上维度应该更容易理解 $$\begin{align}\bm{\delta}_t,\widehat{\bm{i}}_{t-1}&amp;=\big[\underbrace{\mathop{\mathop{\mathrm{colmax}}[\underbrace{\mathop{\bm{\delta}_{t-1}}}_{1\times C}\odot\underbrace{\mathop{\bm{A}}}_{C \times C}}}_{C\times 1}\big]^\text{T}\odot\underbrace{\mathop{\bm{\rho}_t}}_{1\times C}\end{align}$$ 定义初始状态 $ \delta_1[i_1]=k\big(z_1=i_1,\bm{X}\big)=\pi[i_1]$，同时我们注意到 $\displaystyle i_t$是可以任意的，递归计算我们得到一个网格图 trellis diagram。 $$\begin{align}\Delta=\{\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\}\end{align}$$ 4.3、后向回溯经过前向计算我们得到了最大的权重 $$\begin{align}\max_{\mathcal{D}_t^z}g\big(\mathcal{D}_t^z,\mathcal{D}_t^\bm{x}\big)=\max_{i_t}\delta_t[i_t]=\max_{i_t}\max_{i_{t-1}}\max_{\cdots}\max_{i_2}\max_{i_2}\max_{i_1}\pi[i_1]\prod_{t=2}^TA[i_{t-1},i_t]\rho_t[i_t]\end{align}$$ 求最佳隐数据集本质是网格图 trellis diagram $\displaystyle \Delta=\{\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\}$上搜索最短距离。为了解决这个问题，回顾动态规划思想：最优路径 $ \widehat{\mathcal{D}}_{1:T}^z$的一部分 $ \widehat{\mathcal{D}}_{t:T}^z$对于 $ t:T$的所有可能路径 $ \mathcal{D}_{t:T}^z$必然是最优。如果存在另外一条路径 $ \tilde{\mathcal{D}}_{t:T}^z$是最优的，那么会出现矛盾 $ \widehat{\mathcal{D}}_{1:t}^z\cup \tilde{\mathcal{D}}_{t:T}^z\neq \widehat{\mathcal{D}}_{1:T}^z $，所以 $ \widehat{\mathcal{D}}_{t:T}^z$ 必须是最优的。根据这一思想，我们定义回溯操作 traceback : $ \omega_t[\cdot]$，来从后向前还原最优状态序列。 $$\begin{align}\widehat{z}_{t-1}=\omega_t[z_t=i_t]=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$ 定义 $ T$时刻最优状态 $\displaystyle \widehat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]$。应用回溯操作，得到最优路径：$$\begin{align}\widehat{\mathcal{D}}_{T}^z=\{\widehat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]\}\cup\{\widehat{z}_{t-1}=\omega_t[\hat{z}_{t}]\}_{t=T}^1\end{align}$$ 为了解决数据下溢问题，我们可以取对数$$\begin{align}&amp;\ln\delta_{t}[i_t]=\max_{i_{t-1}}\big[\ln\rho_t[i_t]+\ln A[i_{t-1},i_t]+\ln\delta_{t-1}[i_{t-1}]\big]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\big[\ln\rho_t[i_t]+\ln A[i_{t-1},i_t]+\ln\delta_{t-1}[i_{t-1}]\big]\end{align}$$ 算法：维特比算法 1 $\displaystyle \bm{\delta}_1=\bm{\pi}$2 $\displaystyle \text{ for }\,t=2:T$ $\displaystyle\quad\begin{array}{|lc}\text{ for }\,i_{t}=1:C \\\quad\begin{array}{|lc}\displaystyle \text{Traceback_Data}=\big[\ln\delta_t[i_t],\omega_{t-1}[i_t]\big]=\big[\mathop{\mathrm{colmax}}[\ln\bm{\delta}_{t-1}+\ln\bm{A}\big]^\text{T}+\ln\bm{\rho}_t\end{array}\\\text{ end}\\\end{array}$3 end$\displaystyle [\ln\delta_T,\hat{z}_{T}]=\max_{i_T}\,\ln\bm{\delta}_T$4 $\displaystyle \text{ for }\,t=T:2$ $\displaystyle\quad\begin{array}{|lc}\hat{z}_{t-1}=\omega_{t}[\hat{z}_{t}]\end{array}$5 end6 $\displaystyle \hat{\bm{z}}$ 五、条件随机场的训练5.1、数据集的对数似然函数$$\begin{align}p\big(\bm{z}\mid\bm{X}\big)&amp;=p\big(\mathcal{D}^{\cdot|\cdot}_T\big)=p\big(\mathcal{D}_T^z\mid\mathcal{D}_T^\bm{x}\big)=\frac{1}{Z\big(\bm{X}\big)}\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)\prod_{t=1}^Tk\big(z_t,\bm{X}\big)\\&amp;=\frac{1}{Z\big(\bm{X}\big)}k\big(z_1,\bm{X}\big)\prod_{t=2}^Ts\big(z_{t-1},z_t,\bm{X}\big)k\big(z_t,\bm{X}\big)\\\end{align}$$ 定义整体数据集 $\displaystyle \mathcal{D}=\{\mathcal{[D}_T^z\mid \mathcal{D}_T^\bm{x}]_i\}_{i=1}^n$，含义是很清楚的：如果是句子的序列标注，就是 $\displaystyle n$带标注的句子。其中标注就是隐变量数据集，句子就是观测变量数据集。多个带标注的句子就是我们的 $\displaystyle \mathcal{D}$。 $$\begin{align}\ln p\big(\mathcal{D}\big)&amp;=\ln \prod_{i=1}^np\big([\mathcal{D}_T^z\mid \mathcal{D}_T^\bm{x}]_i\big)=\sum_{i=1}^n\ln p\big([\mathcal{D}_T^z\mid \mathcal{D}_T^\bm{x}]_i\big)\\&amp;=\sum_{i=1}^n\Bigg[\ln k\big(z_{1,i},\bm{X}_i\big)+\sum_{t=2}^T\bigg[\ln s\big(z_{t-1,i},z_{t,i},\bm{X}_i\big)+\ln k\big(z_{t,i},\bm{X}_i\big)\bigg]-\ln Z\big(\bm{X}_i\big)\Bigg]\\&amp;=\sum_{i=1}^n\ln k\big(z_{1,i},\bm{X}_i\big)+\sum_{i=1}^n\sum_{t=2}^T\bigg[\ln s\big(z_{t-1,i},z_{t,i},\bm{X}_i\big)+\ln k\big(z_{t,i},\bm{X}_i\big)\bigg]-\sum_{i=1}^n\ln Z\big(\bm{X}_i\big)\\&amp;=\sum_{i=1}^n\sum_{t=1}^T\ln k\big(z_{t,i},\bm{X}_i\big)+\sum_{i=1}^n\sum_{t=2}^T\ln s\big(z_{t-1,i},z_{t,i},\bm{X}_i\big)-\sum_{i=1}^n\ln Z\big(\bm{X}_i\big)\\&amp;=\sum_{i=1}^n\ln g\big(\bm{z}_{i},\bm{X}_{i}\big)-\sum_{i=1}^n\ln Z\big(\bm{X}_i\big)\\\end{align}$$ 我们必须清晰定义函数 $\displaystyle s,k$才能使得求导变得显然(注意转置 $\displaystyle \mathrm{T}$与下标 $\displaystyle T$)的区别，同时注意 $\displaystyle i$表示的是隐变量的状态。这样发射分数 $\displaystyle k(\cdot)$和转换分数函数 $\displaystyle s(\cdot)$就是类似于分类分布和矩阵分类分布的形势，如下： $$\begin{align}k\big(z_t=i,\bm{X}\big)&amp;=\pi_{tc}^{\mathbb{I}(z_t=i)}\\s\big(z_{t-1}=i,z_t=j,\bm{X}\big)&amp;=A[i,j]^{\mathbb{I}(z_{t-1}=i)\mathbb{I}(z_t=j)}\end{align}$$为了更加简洁，我们重新定义隐变量 $\displaystyle z_t\in \mathcal{Z}=\{1,\cdots,C\}$的表现形式。我们使用 $\displaystyle 0-1$编码对隐边重新定义： $\displaystyle \bm{z}_t\in \mathcal{Z}=\{0,1\}^C$$$\begin{align}\bm{z}_t=[ 0\cdots1\cdots0 ]^\text{T}\iff \bm{z}_t[i]=z_{ti}=1\iff z_t=i\end{align}$$同时我们定义 $\displaystyle \bm{Z}_{T\times C}=[\bm{z}_1,\cdots,\bm{z}_t,\cdots,\bm{z}_T]^T$、$\displaystyle [\bm{\pi}_t]_{C\times 1}=\big[\pi_{z_1=1},\cdots,\pi_{z_t=i},\cdots,\pi_{z_T=C}\big]^T$。还定义 $\displaystyle \bm{\Pi}_{C\times T}=[\bm{\pi}_1,\cdots,\bm{\pi}_T]$。需要注意的是我们对向量的定义默认是列向量，对数据集的矩阵定义已行就是一条数据、一列就是一个特征。相关参数的定义适应数据集矩阵定义。继续，这样有 $$\begin{align}k\big(z_t=i,\bm{X}\big)&amp;=\bm{I}^\mathrm{T}\big[\bm{z}_t\odot\bm{\pi}_t\big]\\s\big(z_{t-1}=i,z_t=j,\bm{X}\big)&amp;=\bm{I}^\mathrm{T}\big[\bm{z}_{t-1}\bm{z}_t\odot \bm{A}\big]\bm{I}\end{align}$$ 我们将注意力再次集中到似然函数：$$\begin{align}\ell\big(\mathcal{D}\big)&amp;=\ln p\big(\mathcal{D}\big)=\sum_{i=1}^n\ln g\big(\bm{Z}_{i},\bm{X}_{i}\big)-\sum_{i=1}^n\ln Z\big(\bm{X}_{i}\big)\\&amp;=\sum_{i=1}^n\sum_{t=1}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t,i}\odot\ln\bm{\pi}_t\big]+\sum_{i=1}^n\sum_{t=2}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t-1,i}\bm{z}_{t,i}^\mathrm{T}\odot\ln\bm{A}\big]\bm{I}-\sum_{i=1}^n\ln Z\big(\bm{X_i}\big)\\&amp;=\sum_{i=1}^n\bm{I}^\mathrm{T}\big[\bm{Z}^\mathrm{T}\odot\ln\bm{\Pi}\big]\bm{I}+\sum_{i=1}^n\sum_{t=2}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t-1,i}\bm{z}_{t,i}^\mathrm{T}\odot\ln\bm{A}\big]\bm{I}-\sum_{i=1}^n\ln Z\big(\bm{X_i}\big)\end{align}$$ 其中$\displaystyle \ln g\big(\bm{Z},\bm{X}\big)=\bm{I}^\mathrm{T}\big[\bm{Z}^\mathrm{T}\odot\ln\bm{\Pi}\big]\bm{I}+\sum_{t=2}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t-1}\bm{z}_{t}^\mathrm{T}\odot\ln\bm{A}\big]\bm{I}$$\displaystyle \ln Z\big(\bm{X}\big)=\ln\sum_{\bm{Z}}g(\bm{Z}, \bm{X})=\ln\sum_{\bm{Z}}\bigg[\bm{I}^\mathrm{T}\big[\bm{Z}^\mathrm{T}\odot\bm{\Pi}\big]\bm{I}+\sum_{t=2}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t-1}\bm{z}_{t}^\mathrm{T}\odot\bm{A}\big]\bm{I}\bigg]$ 5.2、对数似然函数求导于是对参数 $\displaystyle \bm{\theta}=\{\bm{\Pi},\bm{A}\}$求导有 $$\begin{align}\frac{\partial \ell}{\partial \bm{\varXi}}&amp;=\frac{\sum_{i}^n \bm{Z}_i^\mathrm{T}}{\bm{\Pi}}-\sum_{i=1}^n\frac{\sum_{\bm{Z}\in \mathcal{Z}}\bm{Z}_i^\mathrm{T}}{Z\big(\bm{X}_i\big)}=\frac{\sum_{i}^n \bm{Z}_i^\mathrm{T}}{\bm{\Pi}}-\frac{nC^{C-1}\bm{I}_{C\times T}}{\sum_{i=1}^nZ\big(\bm{X}_i\big)}=\bm{0}\\\frac{\partial \ell}{\partial \bm{A}}&amp;=\frac{\sum_{i}^n \bm{z}_{t-1,i}\bm{z}_{t,i}^\mathrm{T}}{\bm{A}}-\sum_{i=1}^n\frac{\sum_{\bm{Z}\in \mathcal{Z}}\bm{z}_{t-1,i}\bm{z}_{t,i}^\mathrm{T}}{Z\big(\bm{X}_i\big)}=\frac{\bm{S}}{\bm{A}}-\frac{nC^{C-1}\bm{I}_{C\times C}}{\sum_{i}^nZ\big(\bm{X}_i\big)}=\bm{0}\end{align}$$ 求解我们有：$$\begin{align}\bm{\Pi}&amp;=\frac{1}{nC^{C-1}}\sum_{i=1}^nZ\big(\bm{X}_i\big)\sum_{i}^n \bm{Z}_i^\mathrm{T}\\\bm{A}&amp;=\frac{1}{nC^{C-1}}\sum_{i=1}^nZ\big(\bm{X}_i\big)\bm{S}\end{align}$$ 我们来理解一下这个公式的含义，发射分数实际上就是整体数据集中序列状态（0-1编码向量）的加和然后取平均。转移分数也是整体数据序列转移状态(0-1编码矩阵)的加和然后取平均，最后它们都乘以了一个归一常数。到这里参数 $\displaystyle \bm{\pi}_t$似乎从天而降,我们来明细一下。令 $\displaystyle \dim[\bm{X}]=T\times F$。观测变量 $\displaystyle \bm{x}$有 $\displaystyle F$个特征维度。定义 $\displaystyle \bm{\pi}_t=\bm{W}^\mathrm{T}\bm{x}_t+\bm{\omega}$ 于是又有$$\begin{align}\bm{\Pi}_{C\times T}=\bm{W}_{C\times F}\bm{X}_{T\times F}^{\mathrm{T}}+\bm{\omega}_{C\times 1}\odot \bm{I}_{C\times T}=\bm{W}\bm{X}^{\mathrm{T}}+\bm{\varXi}\end{align}$$$$\begin{align}\ell\big(\mathcal{D}\big)&amp;=\ln p\big(\mathcal{D}\big)=\sum_{i=1}^n\ln g\big(\bm{Z}_{i},\bm{X}_{i}\big)-\sum_{i=1}^n\ln Z\big(\bm{X}_{i}\big)\\&amp;=\sum_{i=1}^n\bm{I}^\mathrm{T}\big[\bm{Z}^\mathrm{T}\odot\ln \big[\bm{W}\bm{X}^{\mathrm{T}}+\bm{\varXi}\big]\big]\bm{I}+\sum_{i=1}^n\sum_{t=2}^T\bm{I}^\mathrm{T}\big[\bm{z}_{t-1,i}\bm{z}_{t,i}^\mathrm{T}\odot\ln\bm{A}\big]\bm{I}-\sum_{i=1}^n\ln Z\big(\bm{X_i}\big)\end{align}$$表达式已经比较复杂，求解析解存在困难，这里我们一般用拟牛顿，或者其他各种优化方法求解。 六、总结1、我们使用条件随机场定义条件概率 $\displaystyle p\big(\bm{z}\mid \bm{X}\big)$。我们是基于整体特征定义了观测变量与隐变量的关系 $\displaystyle k\big(z_t,\bm{X}\big)$，而不像隐马尔可夫模型那样 $\displaystyle p\big(\bm{x}_t\mid z_t\big)$。这大大利于充分利用观测序列的信息。而不是局部信息。2、然后我们使用图模型的信念传播 Belife Propagation,先后使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。3、最后我们定义了特殊的发射分数函数，和转移分数函数，解析的求解了参数，获得了参数自觉的一些解释。4、人类可以通过一些精巧的设计，来获取超乎直观想象结果，上帝似乎很惊异。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/875b887a5e7e39e56eea929a72336d4e/如果您需要引用本文，请参考：引线小白. (Mar. 12, 2017). 《条件随机场模型》[Blog post]. Retrieved from https://www.limoncc.com/post/875b887a5e7e39e56eea929a72336d4e@online{limoncc-875b887a5e7e39e56eea929a72336d4e,title={条件随机场模型},author={引线小白},year={2017},month={Mar},date={12},url={\url{https://www.limoncc.com/post/875b887a5e7e39e56eea929a72336d4e}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率图</tag>
        <tag>条件随机场模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型]]></title>
    <url>%2Fpost%2F9b154bbdc2a51d2ea34ec070684b5132%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清隐马尔可夫模型的问题。若有错误，请大家指正。关键词: 隐马尔可夫模型,前向-后向算法,维特比算法,鲍姆-韦尔奇算法 一、马尔可夫模型1.1、基本概念1.1.1、符号我们开始讨论，离散时间离散状态，也就是离散随机序列。假定：离散时间： $\displaystyle \{1,\cdots,T\}$；离散状态：$\displaystyle x_t\in \{1,\cdots,c,\cdots,C\}$。我们称 $$\begin{align}p(\bm{x}_{1:T})=p(x_1)\prod_{t=2}^Tp\big(x_t\mid x_{t-1}\big)\end{align}$$为一阶马尔可夫模型。下面来初步认识一下这个模型。 1.1.2、转移概率转移概率: 从状态 $\displaystyle i$到状态 $\displaystyle j$的概率 $\displaystyle A_{ij}=p\big(x_t=j\mid x_{t-1}=i\big)$，于是有转移矩阵$\displaystyle \bm{A}=\big[A_{ij}\big]$，且有 $\displaystyle \sum_{j=1}^KA_{ij}=\bm{I}^\text{T}\bm{a}_i=1$ 也就说 $\displaystyle \bm{A}$的每一行相加等于1。这样的矩阵我们称之为随机矩阵 $\displaystyle \textit{(stochastic matrix)}$。注意这个时候我们定义的转移矩阵与离散的时间没有关系。 下面我们定义第 $\displaystyle n$步转移矩阵：$\displaystyle A_{ij}\big(n\big)=p\big(x_{t+n}=j\mid x_{t}=i\big)$，已知 $\displaystyle \bm{A}(1)=\bm{A}$，我们有查普曼-柯尔莫哥洛夫等式 $\displaystyle \textit{(Chapman-Kolmogorov equations)}$ $$\begin{align}A_{ij}(m+n)=\sum_{k=1}^KA_{ik}(m)A_{kj}(n)\end{align}$$亦有：$$\begin{align}\bm{A}(m+n)=\bm{A}(m)\bm{A}(n)\end{align}$$于是有 $\displaystyle \bm{A}(n)=\bm{A}\bm{A}(n-1)=\bm{A}\bm{A}\bm{A}(n-2)=\cdots=\bm{A}^n$ 1.2、应用马尔可夫模型可以用于多个领域。例如把词序列看成是概率分布，状态空间是所有词的集合。我们可以定义一元模型、二元模型、三元模型、$\displaystyle n$ 元模型。进而可用于这些领域：句子补全、数据压缩、文本分类、自动写作等 1.3、极大似然法与二元模型马尔可夫过程考虑单变量历史状态模型 $\displaystyle x_t\in\{1,\cdots,C\}$，同时定义一条链的数据集合 $\displaystyle x_{1:T}=\bm{x}$ 有： $$\begin{align}p\big(x_{1:T}\mid \bm{\pi},\bm{A}\big)&amp;=p\big(x_1\big)A\big(x_1,x_2\big)\cdots A\big(x_{T-1},x_{T}\big)\\p\big(\bm{x}\mid \bm{\pi},\bm{A}\big)&amp;=\mathrm{Cat}\big(x_1\mid\bm{\pi}\big)\cdots A\big(x_{T-1},x_{T}\big)\\&amp;=\prod_{c=1}^C\pi_c^{\mathbb{I}(x_1=c)}\cdot\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t=s,x_{t-1}=c\right)}\end{align}$$ 定义：$\displaystyle \bm{x}_i=x_{1:T}^i$，进而定义链的集合 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^N$，有该链集的对数似然函数：$$\begin{align} &amp;\ell\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big)=\ln p\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big) \\&amp;=\ln \prod_{i=1}^Np\big(\bm{x}_i\mid \bm{\pi},\bm{A}\big)=\sum_{i=1}^N\ln\left[\prod_{c=1}^K\pi_c^{\mathbb{I}(x_1^i=c)}\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)}\right]\\&amp;=\sum_{i=1}^N\ln \left[\prod_{c=1}^C\pi_c^{\mathbb{I}(x_1^i=c)}\right]+\sum_{i=1}^N\ln \left[\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)}\right]\\&amp;=\sum_{c=1}^C \left[\sum_{i=1}^N\mathbb{I}(x_1^i=c)\ln\pi_c\right]+\sum_{c=1}^C\sum_{s=1}^C \left[\sum_{i=1}^N\sum_{t=2}^T\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)\ln A_{cs}\right]\\&amp;=\sum_{c=1}^CN_{c}^{1}\ln\pi_c+\sum_{c=1}^C\sum_{s=1}^C N_{cs}\ln A_{cs}\\&amp;={\bm{N}_1}^\text{T}\ln \bm{\pi}+ \bm{I}^\text{T}\left[\bm{N}\odot\ln\bm{A}\right]\bm{I}\end{align}$$ 其中：$\displaystyle N_c^1=\sum_{i=1}^N\mathbb{I}(x_1^i=c)$，$\displaystyle N_{cs}=\sum_{i=1}^N\sum_{t=2}^T\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)$，且有$\displaystyle \bm{I}^\text{T}\bm{\pi}=1, \bm{A}\bm{I}=\bm{I}$。将约束代入，我们有：$$\begin{align}\ell\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big) ={\bm{N}_1}^\text{T}\ln \bm{\pi}+ \bm{I}^\text{T}\left[\bm{N}\odot\ln\bm{A}\right]\bm{I}+\lambda \left[\bm{I}^\text{T}\bm{\pi}-1\right]+ \left[\bm{A}\bm{I}-\bm{I}\right]^\text{T}\bm{\delta}\end{align}$$于是求先验概率参数有： $$\begin{align}\frac{\partial \ell}{\partial \bm{\pi}}&amp;=\frac{\bm{N}_1}{\bm{\pi}}+\lambda \bm{I}=0\\\frac{\partial \ell}{\partial \lambda}&amp;=\bm{I}^\text{T}\bm{\pi}-1=0\end{align}$$得 $$\begin{align}\bm{\pi}=\frac{\bm{N}_1}{\bm{I}^\text{T}\bm{N}_1}\end{align}$$ 又求转移概率有： $$\begin{align}\frac{\partial \ell}{\partial \bm{A}}=\frac{\bm{I}\bm{I}^\text{T}\odot\bm{N}}{\bm{A}}+ \bm{I}^\text{T}\bm{\delta}=\frac{\bm{N}}{\bm{A}}+\bm{I}^\text{T}\bm{\delta}=\bm{0}\\\frac{\partial \ell}{\partial \bm{\delta}}&amp;=\bm{A}\bm{I}-\bm{I}=\bm{0}\end{align}$$ 得$$\begin{align}\bm{A}=\frac{\bm{N}}{\bm{N}\bm{I}}=\frac{\bm{N}}{\bm{N}_c}\end{align}$$ 我们注意到：对于 $\displaystyle n$元模型 参数将会指数级增加 $\displaystyle O(C^n)$。对于5万个单词的语言模型，参数将到达惊人的25亿。显然我们是无法凑齐足够多数据，来使用极大似然法求参数。我们将面临严重的 零数问题。 1.4、平滑方法1.4.1、拉普拉斯加一平滑当然我们可以使用 拉普拉斯加一平滑。当这不是唯一平滑的方法。 1.4.2、删除插值$$\begin{align}A=(1-\lambda)\frac{N_{cs}}{N_c}+\lambda \frac{N_s}{N}\end{align}$$ 删除插值约等于一个贝叶斯模型，假设一个狄利克雷先验，易得之 1.4.3、古德-图灵估计与卡茨退避法我们有 古德-图灵估计 $\displaystyle \textit{(Good-Turing Estimate)}$ 二、隐马尔可夫模型2.1、基本定义1、隐变量的一阶马尔可夫假设 $\displaystyle p\big(z_t\mid z_{t-1},\cdots,z_1\big)=p\big(z_t\mid z_{t-1}\big)$2、观测变量的独立假设 $\displaystyle p\big(\bm{x}_t\mid \bm{x}_{t-1},\cdots,\bm{x}_1,z_t,\cdots,z_1\big)=p \big(\bm{x}_t\mid z_t\big)$ 隐变量数据集 $\displaystyle \mathcal{D}_T^z=\{z_t\}_{t=1}^T$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}=\{\bm{x}_t\}_{t=1}^T$,完全数据集 $\displaystyle \mathcal{D}^+_T=\{\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\}$，且有：$$\begin{align}p \big(\mathcal{D}_T^+\big)=p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=p\big(\mathcal{D}_T^\bm{x}\mid \mathcal{D}_T^z\big)p\big(\mathcal{D}^z\big)\end{align}$$ 隐马尔可夫模型 为了简洁记，我们来定义一些符号。考虑隐变量有 $\displaystyle K$个状态 $\displaystyle z_t\in\{1,\cdots,K\}$，观测变量有 $\displaystyle O$个状态 $\displaystyle \bm{x}_t\in\{1,\cdots,O\}$。注意这里形式化的使用 $ o$来表示向量 $ \bm{x}_t$的状态，也就是说我们并没有定义向量是什么，因为这样能节约符号，和叙述上不必要的麻烦。继续定义若干参数：$\displaystyle \pi[i]=p(z_1=i)$$\displaystyle A[i,j]=p(z_t=j\mid z_{t-1}=i)$$\displaystyle B[i,o]=p(\bm{x}_t=o\mid z_t=i)$令参数集合为：$$\begin{align}\bm{\theta}=\{\bm{\pi},\bm{A},\bm{B}\}\end{align}$$ 注意这里的 $ \bm{\theta}$应该做形式化的理解，它既不是矩阵，也不是向量，而是一个相当于集合符号。在机器学习领域 $ \bm{\theta}$经常被用来表示参数，不对其不加具体定义。而对于观测变量是连续的，我们定义条件概率分布 $\displaystyle \rho_t[i]=p\big(\bm{x}_t\mid z_t=i\big)$ ，注意如果 $\displaystyle \bm{x}_t$未知，则 $\displaystyle \rho_t[i]$是个概率分布。而如果我们有数据集 $\displaystyle \mathcal{D}_t^{\bm{x}}=\{\bm{x}_1,\cdots,\bm{x}_t\}$， 也就说 $\displaystyle \bm{x}_t$已知，那么 $\displaystyle \rho_t[i]$是个数。那么参数集合是： $$\begin{align}\bm{\theta}=\{\bm{\pi},\bm{A},\bm{\rho}_t\}\end{align}$$ 注意：关于符号 $\displaystyle p(x)$和 $\displaystyle p(x=a)$的解释，这时的符号 $\displaystyle p$有点点转换。由一个函数变成了一个数。 2.2、向前向后算法2.2.1、直接计算我们要解决如何高效求观测变量数据集联合分布的问题，下面我们要分析一下这个基本问题。考虑离散隐变量 $\displaystyle z_t$，离散观测变量 $\displaystyle x_t$，这时隐马尔可夫模型观测数据集 $\displaystyle \mathcal{D}_T^{\bm{x}}=\{\bm{x}_t=o(t)\}_{t=1}^T$：$$\begin{align}p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=p\big(\mathcal{D}_T^\bm{x}\mid \mathcal{D}_T^z\big)p\big(\mathcal{D}_T^z\big)=p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\Bigg[\prod_{t=2}^Tp\big(z_t\mid z_{t-1}\big)p\big(\bm{x}_t\mid z_t\big)\Bigg]\end{align}$$整理写成：$$\begin{align}&amp;p\big(\mathcal{D}_T^\bm{x}\mid \bm{\theta}\big)=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K p\big(\mathcal{D}_T^{\bm{x}},\mathcal{D}_T^z\mid \bm{\theta}\big)\\&amp;=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K \left[\Bigg[p(z_1)\prod_{t=2}^Tp\big(z_t\mid z_{t-1}\big)\Bigg]\Bigg[\prod_{t=1}^Tp\big(x_t\mid z_t\big)\Bigg]\right]\\&amp;=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K \left[\pi\big[i(1)\big]\cdot\prod_{t=2}^TA_t\big[i(t-1),i(t)\big]\cdot\prod_{t=1}^TB_t\big[i(t),o(t)\big]\right]\\&amp;=\sum_{i(1)=1}^K\cdots\sum_{i(T)=1}^K\Bigg[\pi\big[i(1)\big]B_1\big[i(1),o(1)\big]\cdot\prod_{t=2}^T \bigg[A_t\big[i(t-1),i(t)\big]\cdot B_t\big[i(t),o(t)\big]\bigg]\Bigg]\end{align}$$ 观察一下大中括号里面的式子 $$\begin{align}\pi\big[i(1)\big]B_1\big[i(1),o(1)\big]A_2\big[i(1),i(2)\big] B_2\big[i(2),o(2)\big]\cdots A_T\big[i(T-1),i(T)\big] B_T\big[i(T),o(T)\big]\end{align}$$ 若观测变量是连续的有：$$\begin{align}p\big(\mathcal{D}_T^\bm{x}\mid \bm{\theta}\big)=\sum_{i_1=1}^K\cdots\sum_{i_T=1}^K\Bigg[\pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]\Bigg]\end{align}$$这个方法的计算复杂度是 $\displaystyle O\big({2TK}^T\big)$, 例如 $\displaystyle K=5,T=10$，那么就要约计算 $\displaystyle 1.95\times10^8$已经非常夸张了。为了更快的计算观测变量的联合分布，我们需要充分利用隐马尔可夫的性质： 2.2.2、前向算法现在定义一个新的数据集，或者叫做在线数据集 $\displaystyle \mathcal{D}_t^{\bm{x}}=\{\bm{x}_i\}_{i=1}^t$我们定义 $\displaystyle \alpha_t[j]=p\big(\mathcal{D}_{t}^\bm{x}, z_{t}=j\big)$于是有：$$\begin{align}\require{cancel}&amp;\alpha_{t}[j]=p\big(\mathcal{D}_t^{\bm{x}},z_t=j)=p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)p \big(\mathcal{D}_{t-1}^{\bm{x}}, z_t=j\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_t=j,z_{t-1}=i\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_t=j\mid z_{t-1}=i\big) p \big(z_{t-1}=i\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}}\mid \cancel{z_t=j},z_{t-1}=i\big)p \big(z_t=j\mid z_{t-1}=i\big) p \big(z_{t-1}=i\big)\\&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_{t-1}=i\big)p \big(z_t=j \mid z_{t-1}=i\big)\\&amp;=\rho_t[j]\sum_{i=1}^K\bigg[ \alpha_{t-1}[i]A[i,j]\bigg]\\\end{align}$$写成矩阵形式有：$$\begin{align}\bm{\alpha}_{t}=\bm{\rho}_t\odot\left[\bm{A}^\text{T}\bm{\alpha}_{t-1}\right]\end{align}$$ 于是有观测变量数据集联合分布$$\begin{align} p\big(\mathcal{D}_T^{\bm{x}}\mid\bm{\theta}\big)=\sum_{j=1}^K \alpha_T[j]=\bm{I}^\text{T}\bm{\alpha}_{T}\end{align}$$ 计算复杂度变为了 $\displaystyle O({TK}^2)$。 2.2.3、前向后向算法。之前我们定义了 $\displaystyle \alpha_t[j]=p\big(\mathcal{D}_{t}, z_{t}=j\big)$，知道观测变量的条件概率：$\displaystyle \rho_t[j]=p\big(\bm{x}_t\mid z_t=j\big)$。我们再定义未来数据集的条件似然(有些书上也称后向概率)$\displaystyle \beta_t[i]=p\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=i\big)$，利用隐马尔可夫的条件假设，推导得到：$$\begin{align}\require{cancel}\beta_{t-1}[i]&amp;=p\big(\mathcal{D}_{t:T}^{\bm{x}}\mid z_{t-1}=i\big)\\&amp;=\sum_{j=1}^Kp\big(z_t=j,\bm{x}_t,\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_{t-1}=i\big)\\&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j,\cancel{z_{t-1}=i},\cancel{\bm{x}_t}\big)p\big(z_t=j,\bm{x}_t\mid z_{t-1}=i\big)\\&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p\big(\bm{x}_t\mid z_t=j,\cancel{z_{t-1}=i}\big)p\big(z_t=j\mid z_{t-1}=i\big)\\&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p\big(\bm{x}_t\mid z_t=j\big)p\big(z_t=j\mid z_{t-1}=i\big)\\&amp;=\sum_{j=1}^K\beta_{t}[j]\cdot\rho_t[j]\cdot A[i,j]\end{align}$$写成矩阵形式。$$\begin{align}\bm{\beta}_{t-1}=\bm{A} \left[\,\bm{\rho}_t\odot\bm{\beta}_t\right]\end{align}$$ 其中有 $\displaystyle \beta_{T}[i]=p\big(\mathcal{D}_{T+1:T}^{\bm{x}}\mid z_T=i\big)=p \big(\emptyset\mid z_T=i\big)=1$，于是有 $\displaystyle \bm{\beta}_T=\bm{I}$用这个似然和初始概率计算观测变量数据集联合分布有： $$\begin{align} p\big(\mathcal{D}_T^{\bm{x}}\mid\bm{\theta}\big) = \sum_{i=1}^K\alpha_1[i]\beta_1[i] =\sum_{i=1}^K\alpha_t[i]\beta_t[i]=\bm{I}^\text{T}\big[\bm{\alpha}_t\odot\bm{\beta}_{t}\big]=\bm{I}^\text{T}\bm{\alpha}_{T}\end{align}$$ 2.2.5、隐变量后验边际概率定义隐变量后验边际概率 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T\big)$，这个概率对我们颇为重要，下面使用向前向后算法计算它$$\begin{align} \eta_t[j] &amp;=p\big(z_t=j\mid \mathcal{D}_T^{\bm{x}}\big) =\frac{p \big(\mathcal{D}_T^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)}{p \big(\mathcal{D}_T\big)}\\ &amp;=\frac{p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)}{p \big(\mathcal{D}_T^{\bm{x}}\big)} =\frac{p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}, z_t=j\big)}{p \big(\mathcal{D}_T^{\bm{x}}\big)}\\ &amp;=\frac{\alpha_t[j]\cdot\beta_t[j]}{\displaystyle\sum_{i=1}^K \alpha_t[j]\cdot\beta_t[j]} \propto \alpha_t[j]\cdot\beta_t[j]\end{align}$$写成矩阵形式$$\begin{align} \bm{\eta}_t = \frac{\bm{\alpha}_t\odot \bm{\beta}_t}{\bm{I}^\text{T}\big[\bm{\alpha}_t\odot \bm{\beta}_t\big]} \propto \bm{\alpha}_t\odot \bm{\beta}_t\end{align}$$ 2.2.4、隐变量两点边际概率隐变量两点边际概率在模型的参数估计中有重要应用，使用向前向后算法可以计算它$$\begin{align}p \big(\bm{z}_{t-1}, \bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)=\frac{p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{z}_{t-1},\bm{z}_t\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}=\frac{p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\end{align}$$ 单独考虑$$\begin{align}\require{cancel}p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{x}_t,\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_{t-1},\bm{z}_t\big)\\&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}},\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{x}_t\mid\cancel{\bm{z}_{t-1}},\bm{z}_t\big)\\&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1},\cancel{\bm{z}_t}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\cancel{\bm{z}_{t-1}},\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)\\&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)\end{align}$$ 代入得到$$\begin{align}p \big(\bm{z}_{t-1}, \bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)&amp;=\frac{p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\&amp;=\frac{p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\&amp;=\frac{p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{z}_{t-1}\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\\end{align}$$ 我们令 $\displaystyle \xi_{t}[i,j]=p\big(z_{t-1}=i,z_t=j\mid \mathcal{D}_T^\bm{x}\big)$则有：$$\begin{align} \xi_{t}[i,j]\propto A[i,j]\cdot\alpha_{t-1}[i]\cdot\rho_{t}[j]\cdot\beta_t[j]\end{align}$$如果同时令 $\displaystyle \bm{Z}_t=\bm{z}_{t-1}\bm{z}_t^\text{T}$， $\displaystyle \bm{\varXi}_t=\big[\xi_{t}[i,j]\big]$。亦有 $\displaystyle \bm{Z}_t\sim \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)$，注意这是一个矩阵分类分布。 $$\begin{align}\mathrm{E}_q\big[\bm{Z}_t\big]=\mathrm{E}_q\big[\bm{z}_{t-1}\bm{z}_t^\text{T}\big]=\sum_{\bm{Z}\in\{0,1\}^{K\times K}} \bm{Z}_t \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)=\bm{\varXi}_t=\frac{\bm{A}\odot\bigg[\bm{\alpha}_{t-1}\big[\bm{\rho}_t\odot \bm{\beta}_t\big]^\text{T}\bigg]}{\bm{I}^\text{T}\big[\bm{\alpha}_t\odot\bm{\beta}_{t}\big]}\end{align}$$ 也就说 $\displaystyle \mathrm{E}_q\big[z_{t-1}=i,z_t=j\big]=\xi_{t}[i,j]$。 2.3、鲍姆-韦尔奇算法2.3.1、完全数据集对数似然函数期望对于隐变量我们有 $\displaystyle \mathcal{D}_T^z=\{z_t\}_{t=1}^T$， $\displaystyle z_t\in \{1,\cdots,K\}$。现在我们要改变隐变量符号的表示方法，即 $\displaystyle \mathcal{D}_T^\bm{z}=\{\bm{z}_t\}_{t=1}^T$, $\displaystyle \bm{z}_t\in \{0,1\}^K$。这个向量表示方法与之前的表示方法是等价的，务必熟悉这两个表示方法。$$\begin{align}\bm{z}_t=[ 0\cdots1\cdots0 ]^\text{T}\iff \bm{z}_t[k]=z_{tk}=1\iff z_t=k\end{align}$$ 我们有隐变量初始先验分布、观测变量条件分布、转移概率。$$\begin{align}\begin{cases}p \big(\bm{z}_t\mid \bm{\pi}\big)&amp;\displaystyle=\mathrm{Cat} \big(\bm{z}_t\mid \bm{\pi}\big)=\prod_{k=1}^K\pi_k^{z_t[k]}\\p \big(\bm{x}_t\mid \bm{z}_t,\bm{\varphi}\big)&amp;\displaystyle=\prod_{i=1}^Kp \big(\bm{x}_t\mid \varphi_i\big)^{z_t[i]}\\p \big(\bm{z}_t\mid \bm{z}_{t-1},\bm{A}\big)&amp;\displaystyle=\prod_{i=1}^K\prod_{j=1}^K A_{ij}^{z_t[i]z_{t-1}[j]}\end{cases}\end{align}$$ 代入有联合概率分布：$$\begin{align}p \big(\mathcal{D}_T^+\big)&amp;=p\big(\bm{z}_1\big)p\big(\bm{x}_1\mid z_1\big)\Bigg[\prod_{t=2}^Tp\big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{x}_t\mid \bm{z}_t\big)\Bigg]\\&amp;=\prod_{k=1}^K\pi_k^{z_1[k]}\prod_{k=1}^Kp \big(\bm{x}_1\mid\varphi_k\big)^{z_1[k]}\left[\prod_{t=2}^T\Bigg[\prod_{i=1}^K\prod_{j=1}^K A_{ij}^{z_{t-1}[i]z_{t}[j]}\prod_{j=1}^Kp \big(\bm{x}_t\mid\varphi_j\big)^{z_t[j]}\Bigg]\right]\end{align}$$同时我们有 $\displaystyle q=q \big(\mathcal{D}_T^\bm{z}\mid \mathcal{D}_T^\bm{x},\theta\big)$。 为简洁记，我们定义一个数 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T^\bm{x}\big)$，是隐变量后验边际概率。这样有 $\displaystyle p\big(\bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)=\mathrm{Cat} \big(\bm{z}_t\mid \bm{\eta}_t\big)=\prod_{j=1}^K\big(\eta_t[j]\big)^{z_t[j]}$。根据 $\displaystyle \textit{EM}$算法，我们来推导一下完全数据集对数似然函数期望具体表示形式。$$\begin{align}&amp;\ell \big(\mathcal{D}_T^\bm{x}\mid \theta\big)=\ln p \big(\mathcal{D}_T^\bm{x}\mid \theta\big)=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}_T^+\mid\theta\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\\&amp;\propto\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}_T^+\mid \theta\big)\big]=\mathrm{E}^\text{T}[\bm{z}_1]\ln\bm{\pi}+\sum_{t=1}^T\sum_{k=1}^K\mathrm{E}\big[z_t[k]\big]\ln p \big(\bm{x}_t\mid \varphi_k\big)+\sum_{t=2}^T\bm{I}^\text{T}\left[\mathrm{E}\big[\bm{Z}_t\big]\odot\ln\bm{A}\right]\bm{I}\\&amp;=\bm{\eta}_1^\text{T}\ln\bm{\pi}+\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}\end{align}$$ 其中：1、 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T^\bm{x}\big)$，也就说有 $\displaystyle \mathrm{E}_q\big[z_t[k]\big]=\eta_t[k]$，这根据分类分布不难理解。写成向量形式 $$\begin{align}\mathrm{E}_q\big[\bm{z}_t\big]=\bm{\eta}_t\end{align}$$ 2、 $\displaystyle \xi_{t}[i,j]=p\big(z_{t-1}=i,z_t=j\mid \mathcal{D}_T^\bm{x}\big)$是隐变量两点边际概率。我们可以定义 $\displaystyle \bm{Z}_t=\bm{z}_{t-1}\bm{z}_t^\text{T}$，那么根据我们上面的推导有 $$\begin{align}\mathrm{E}_q\big[\bm{Z}_t\big]=\sum_{\bm{Z}\in\{0,1\}^{K\times K}} \bm{Z}_t \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)=\bm{\varXi}_t\end{align}$$也就说 $\displaystyle \mathrm{E}_q\big[z_{t-1}=i,z_t=j\big]=\xi_{t}[i,j]$。 2.3.2、求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$显然若要求解参数 $\displaystyle \bm{\theta}=\{\bm{\pi},\bm{A},\bm{\rho}\}$，就必须知道 $\displaystyle \bm{\eta}_t，\bm{\varXi}_t$。这在之后的前向先后算法中我们已经求出，也就是 E 步。现在推导 $\displaystyle \textit{EM}$算法的 M 步。有：$$\begin{align}\exists \,\theta_{end}\to\max\ell \big(\mathcal{D}^\bm{x}\mid \theta\big)\iff\max \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \theta\big)\big]\end{align}$$记$$\begin{align}\mathrm{E}_{\mathcal{D}_T^+}=\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}_T^+\mid \theta\big)\big]=\bm{\eta}_1^\text{T}\ln\bm{\pi}+\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}\end{align}$$忽略 $\displaystyle p \big(\bm{x}_t\mid \varphi_k\big)$的严格定义 。先求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$。我们有：$$\begin{align}\mathrm{E}_{\mathcal{D}_T^+}\propto \bm{\eta}_1^\text{T}\ln\bm{\pi}+\lambda\big[\bm{I}^\text{T}\bm{\pi}-1\big]\end{align}$$得：$$\begin{align}\begin{cases}\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \bm{\pi}}&amp;\displaystyle=\frac{\bm{\eta}_1}{\bm{\pi}}+\lambda \bm{I}=\bm{0}\\\\\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \lambda}&amp;=\bm{I}^\text{T}\bm{\pi}-1=0\end{cases}\end{align}$$易得：$$\begin{align}\hat{\bm{\pi}}=\frac{\bm{\eta}_1}{\bm{I}^\text{T}\bm{\eta}_1}\end{align}$$考虑 $ \bm{A}$有：$$\begin{align}\mathrm{E}_{\mathcal{D}_T^+}\propto \sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}+\big[\bm{A}\bm{I}-\bm{I}\big]^\text{T}\bm{\delta}\end{align}$$有：$$\begin{align}\begin{cases}\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \bm{A}}&amp;\displaystyle= \frac{\sum_{t=2}^T{\bm{I}\bm{I}}^\text{T}\odot \bm{\varXi}_t}{\displaystyle\bm{A}}+\bm{I}^\text{T}\bm{\delta}=\bm{0}\\\\\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \bm{\delta}}&amp;=\bm{A}\bm{I}-\bm{I}=\bm{0}\end{cases}\end{align}$$得：$$\begin{align}\hat{\bm{A}}=\frac{\sum_{t=2}^T\bm{\varXi}_t}{\sum_{t=2}^T\bm{\varXi}_t \bm{I}}=\sum_{t=2}^\text{T} \frac{\bm{\varXi}_t}{\bm{\varXi}_t \bm{I}}\end{align}$$ 2.3.3、求解观测变量条件概率参数考虑 $\displaystyle \bm{x}_t$服从分类分布，有条件密度：$$\begin{align}\rho_t[k]=p \big(\bm{x}_t\mid \varphi_k\big)=\mathrm{Cat}\big(\bm{x}_t\mid \bm{\mu}_k\big)=\prod_{c=1}^C\mu_{kc}^{x_t[c]}\end{align}$$同时我们定义 $\displaystyle \bm{U}=[\mu_{ck}]_{K\times C}$，有：$$\begin{align}\mathrm{E}_{\mathcal{D}_T^+}&amp;\propto\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}=\sum_{t=1}^T\sum_{k=1}^K\eta_t[k]\ln\bigg[\prod_{c=1}^C\mu_{kc}^{x_t[c]}\bigg]+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}\\&amp;=\sum_{t=1}^T\sum_{k=1}^K\eta_t[k] \cdot\bm{x}_t ^\text{T}\ln \bm{\mu}_k+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}\\&amp;=\sum_{t=1}^T \bm{\eta}_t^\text{T}\cdot\ln [\bm{U}]\cdot\bm{x}_t+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}\end{align}$$ 求极值有：$$\begin{align}\begin{cases}\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \bm{U}}&amp;\displaystyle= \frac{\sum_{t=1}^T\bm{\eta}_t \bm{x}_t ^\text{T}}{\bm{U}}+\bm{I}^\text{T}\bm{\kappa}=\bm{0}\\\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial \bm{\kappa}}&amp;=\bm{U}\bm{I}-\bm{I}=\bm{0}\end{cases}\end{align}$$可得$$\begin{align}\hat{\bm{U}}=\sum_{t=1}^T \frac{\bm{\eta}_t \bm{x}_t ^\text{T}}{\bm{\eta}_t \bm{x}_t ^\text{T} \bm{I}}\end{align}$$ 2.4、维特比算法2.4.1、问题描述定义隐变量数据集 $\displaystyle \mathcal{D}_T^z$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}$,完全数据集 $\displaystyle \mathcal{D}^+_T$。维特比算法要解决的是求隐变量最可能状态序列。可以理解为：已知观测变量数据集，推断隐变量数据集： $$\begin{align}\hat{\mathcal{D}}_T^\bm{z}=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^\bm{z}}\,p \big(\mathcal{D}_T^\bm{z}\mid\mathcal{D}_T^\bm{x}\big)\end{align}$$注意到：$$\begin{align}p \big(\mathcal{D}_{t}^z\mid \mathcal{D}_t^\bm{x}\big)=p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\big/p \big(\mathcal{D}_t^\bm{x}\big)\propto p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\end{align}$$相差一个常数 $ p \big(\mathcal{D}_t^\bm{x}\big)$，通过直接优化完全数据集会简化问题。我们选择最大化下式：$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{\mathcal{D}_t^z}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\\end{align}$$其中完全数据集的概率 $\displaystyle \pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]$，写成对数形式有：$$\begin{align}\ln\pi[i_1]+\ln\rho_1[i_1]+\sum_{t=2}^T \bigg[\ln A_t[i_{t-1},i_{t}]+\ln \rho_t[i_t]\bigg]\end{align}$$这个形式有利于我们着手分析。 2.4.2、前向计算我们可以拆分隐变量数据集 $ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t\}$，拆分的关键直觉是时刻 $ t$的最可能路径必须有是由时刻 $ t-1$的最可能路径组成。问题变为$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{z_t}\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$追寻这一关键思想，下面来具体化：我们假设 $ t$时刻的状态为 $ i_t$，进而定义路径$ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t=i_t\}$的最大概率(权重)：$$\begin{align}\delta_t[i_t]=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$为了充分利用隐马尔可夫模型的条件独立性质和动态规划思想，假设 $ t-1$的状态为 $ i_{t-1}$。继续拆分数据集于是有：$$\begin{align}\delta_{t}[i_t]&amp;=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\\&amp;=\max_{\mathcal{D}_{t-2}^z,z_{t-1}=i_{t-1}}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\&amp;=\max_{i_{t-1}}\bigg[p\big(z_t=i_t\mid z_{t-1}=i_{t-1}\big)p\big(\bm{x}_t\mid z_t=i_t\big)\max_{\mathcal{D}_{t-2}^z} p \big(\mathcal{D}_{t-2}^\bm{x},\mathcal{D}_{t-1}^z,z_{t-1}=i_{t-1}\big)\bigg]\\&amp;=\max_{i_{t-1}} \delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\\\end{align}$$也就是说：时刻 $ t$行至状态 $ i_t$的最可能路径必须有是由时刻 $ t-1$ 行至其他状态 $ i_{t-1}$的最可能路径组成。$$\begin{align}\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$ 写成矩阵形式固定 $\displaystyle i_t$，令$\displaystyle a[i_{t-1}]=A[i_{t-1},i_t]$则有 $$\begin{align}\delta_t[i_t],\widehat{i}_{t-1}[z_t=i_t]&amp;=\max[\bm{\delta}_{t-1}\odot\bm{a}_{t-1}]\rho_t[i_t]\\\bm{\delta}_t,\widehat{\bm{i}}_{t-1}&amp;=\big[\mathop{\mathrm{colmax}}[\bm{\delta}_{t-1}\odot\bm{A}]^\text{T}\odot\bm{\rho}_t\end{align}$$ 加上维度应该更容易理解 $$\begin{align}\bm{\delta}_t,\widehat{\bm{i}}_{t-1}&amp;=\big[\underbrace{\mathop{\mathop{\mathrm{colmax}}[\underbrace{\mathop{\bm{\delta}_{t-1}}}_{1\times C}\odot\underbrace{\mathop{\bm{A}}}_{C \times C}}}_{C\times 1}\big]^\text{T}\odot\underbrace{\mathop{\bm{\rho}_t}}_{1\times C}\end{align}$$ 定义初始状态 $ \delta_1[i_1]=\pi[i_1]\rho_1[i_1]$，同时我们注意到 $\displaystyle i_t$是可以任意的，递归计算我们得到一个网格图 trellis diagram。 $$\begin{align}\Delta=\{\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\}\end{align}$$ 2.4.3、后向回溯回到我们的问题：已知观测变量数据集，推断隐变量数据集。最大化联合概率问题变为$$\begin{align}\max_{\mathcal{D}_T^z} p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=\max_{i_T}\max_{i_{T-1}}A[i_{T-1},i_t] \rho_T[i_T]\cdots\max_{i_1}\delta_1[i_1]A[i_{1},i_2] \rho_2[i_2]\end{align}$$ 为了解决这个问题，回顾动态规划思想：最优路径 $ \hat{\mathcal{D}}_{1:T}^z$的一部分 $ \hat{\mathcal{D}}_{t:T}^z$对于 $ t:T$的所有可能路径 $ \mathcal{D}_{t:T}^z$必然是最优。如果存在另外一条路径 $ \tilde{\mathcal{D}}_{t:T}^z$是最优的，那么会出现矛盾 $ \hat{\mathcal{D}}_{1:t}^z\cup \tilde{\mathcal{D}}_{t:T}^z\neq \hat{\mathcal{D}}_{1:T}^z $，所以 $ \hat{\mathcal{D}}_{t:T}^z$ 必须是最优的。根据这一思想，我们定义回溯操作 traceback : $ \omega_t[\cdot]$，来从后向前还原最优状态序列。$$\begin{align}\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$定义 $ T$时刻最优状态 $\displaystyle \hat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]$。应用回溯操作，得到最优路径：$$\begin{align}\hat{\mathcal{D}}_{T}^z=\{\hat{z}_{t-1}=\omega_t[\hat{z}_{t}]\}_{t=T}^1\end{align}$$ 为了解决数据下溢问题，我们可以取对数$$\begin{align}&amp;\ln\delta_{t}[i_t]=\max_{i_{t-1}}\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\end{align}$$ 算法：维特比算法 1 $\displaystyle \bm{\delta}_1=\bm{\pi}$2 $\displaystyle \text{ for }\,t=2:T$ $\displaystyle\quad\begin{array}{|lc}\text{ for }\,i_{t}=1:C \\\quad\begin{array}{|lc}\displaystyle \text{Traceback_Data}=\big[\ln\delta_t[i_t],\omega_{t-1}[i_t]\big]=\big[\mathop{\mathrm{colmax}}[\ln\bm{\delta}_{t-1}+\ln\bm{A}\big]^\text{T}+\ln\bm{\rho}_t\end{array}\\\text{ end}\\\end{array}$3 end$\displaystyle [\ln\delta_T,\hat{z}_{T}]=\max_{i_T}\,\ln\bm{\delta}_T$4 $\displaystyle \text{ for }\,t=T:2$ $\displaystyle\quad\begin{array}{|lc}\hat{z}_{t-1}=\omega_{t}[\hat{z}_{t}]\end{array}$5 end6 $\displaystyle \hat{\bm{z}}$ 三、总结1、隐马尔可夫是一个古老的模型，开始我们回顾了一下它的基本问题2、然后我们使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。3、中间我们使用 $\displaystyle EM$算法，解决了参数求解的问题。4、人类可以通过一些精巧的设计，来获取超乎直观想象结果，当人类的思想开始集成，开始向深处，广处延伸时，上帝似乎很惊异。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/如果您需要引用本文，请参考：引线小白. (Mar. 11, 2017). 《隐马尔可夫模型》[Blog post]. Retrieved from https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132@online{limoncc-9b154bbdc2a51d2ea34ec070684b5132,title={隐马尔可夫模型},author={引线小白},year={2017},month={Mar},date={11},url={\url{https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率图</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图基础]]></title>
    <url>%2Fpost%2Fb9646fbf6c8d542fdb68ac01ad3c1c5a%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/b9646fbf6c8d542fdb68ac01ad3c1c5a/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理解与分析图模型。若有错误，请大家指正。关键词: 图模型,图论,有向图,无向图 一、问题来源有向图模型($\displaystyle \textit{Directed Graphical Models}$)又称贝叶斯网络($\displaystyle \textit{Bayes Nets}$)。 对于用简单方法来训练复杂系统，我基本知道两个原则：模块化和抽象化。在机器学习领域，我是计算概率的辩护者。因为我相信概率论用它深入和迷人的方式执行了两个原则——因子分解与平均。在我看来，充分利用这两种机制是机器学习的前进方向。—— $\displaystyle \textit{Michael Jordan 1997}$ $\displaystyle \textit{(quted in(Frey 1998)}$ 假设我们观察多个相关变量，例如文档中的单词、图像中的像素或微阵列中的基因。 1、我们如何能简洁地表示联合分布 $\displaystyle p\big(\bm{x}\mid \bm{\theta}\big)$？2、在合理的计算时间内，给定其他，我们如何利用这个分布来推断一组变量?3、我们如何通过合理的数据量来学习这个分布的参数? 这些问题是概率建模、推断和学习的核心，也是概率图的主题。我们参照 $\displaystyle \textit{Matlab}$的向量形式定义如下符号：$\displaystyle \bm{x}_{1:t-1}=x_t,\cdots,x_{t-1}$。定义如下术语：条件概率表格 CPTs $\displaystyle \textit{(conditional probability tables)}$、条件概率密度 CPD$\displaystyle \textit{(conditional probability distribution)}$、条件独立 CI$\displaystyle \textit{(Conditional independence)}$ 我们来考察一下马尔可夫模型：给定现在，未来独立于过去。 这叫做一阶马尔可夫假设(二元模型假设)$$\begin{align}p(\bm{x})=p(x_1)\prod_{t=2}^Tp\big(x_t\mid x_{t-1}\big)\end{align}$$尽管一阶马尔可夫假设对于定义一维序列的分布很有用，但是如何定义二维图像的分布，或者三维视频，或者更一般的：任意变量集合(例如属于某些生物通路的基因)？这正是我们引入图模型的切入点。 一个图模型是应用条件独立假设来表达联合概率分布的方法。是一套能简洁紧凑地表达变量关系的工具。下面我们将不厌其烦的对图论术语下定义(这些术语都是顾名思义的)和显然的。 二、基本概念2.1、图模型的基本符号图 $graph$： $\displaystyle \mathcal{G}=\{V,E\}$节点集 $nodes$： $\displaystyle V(\mathcal{G})$边集 $edges$： $\displaystyle E(\mathcal{G})=\{st\mid s,t\in V\}$阶 $order$： $\displaystyle n=n(\mathcal{G})=\mid\mathcal{G}\mid$边数 $edges numbers$： $\displaystyle e=e(\mathcal{G})=\parallel\mathcal{G}\parallel$ 2.2、节点与边的符号【节点】：引入一个符号 $\displaystyle v_s$，表示第 $\displaystyle s$个节点，这意味着我们也可以用 $\displaystyle s$表示第 $\displaystyle s$个节点。我们将会根据上下文，灵活使用这两种表示符号。 【边】：引入另外一个符号来表示边 有向边：$\displaystyle s\to t=v_s\to v_t\Rightarrow\mathcal{G}(s,t)=1\Rightarrow &lt;st>\in E(\mathcal{G})$无向边：$\displaystyle s-t=v_s-v_t\Rightarrow \mathcal{G}(s,t)=1\land\mathcal{G}(s,t)=1\Rightarrow(st)\in E(\mathcal{G})$某种边：$\displaystyle s\rightleftarrows t=v_s\rightleftarrows v_t\Rightarrow \mathcal{G}(s,t)=1\lor\mathcal{G}(s,t)=1\Rightarrow st\in E(\mathcal{G})$没有边：$\displaystyle \require{cancel} s\cancel{\rightleftarrows}t\Rightarrow\mathcal{G}(s,t)=0\Rightarrow st \notin E(\mathcal{G})$ 【分类】如果一个图的所有边都是有向边，则称其为有向图 $\displaystyle \textit{(Directed Graph)}$；如果都是无向边，则称其为无向图 $\displaystyle \textit{(Undirected Graph)}$；如果皆有之，则称其为混合图$\displaystyle \textit{(Mixed Graph)}$ 2.3、关于节点认识2.3.1、有向图下的节点基本概念一个节点的 $Parent$父节点集: $\displaystyle pa(s)=\{t\mid\mathcal{G}(t,s)=1,t\in V(\mathcal{G})\}$一个节点的 $Child$子节点集: $\displaystyle ch(s)=\{t\mid\mathcal{G}(s,t)=1,t\in V(\mathcal{G}) \}$一个节点的 $Family$族节点集: $\displaystyle fam(s)=\{s\}\cup Pa(s)$ 一个节点是 $root$根: $\displaystyle \exists pa(s)=\emptyset\to \textit{s is a root}$一个节点是 $leaf$叶: $\displaystyle \exists ch(s)=\emptyset\to \textit{s is a leaf}$ 一个节点的 $Ancestors$祖节点集： $\displaystyle an(s)=\{t\mid t\leadsto s\}$一个节点的 $Descendants$孙节点集： $\displaystyle de(s)=\{t\mid s\leadsto t\}$ 拓扑序 $\displaystyle \textit{(Topological Ordering)}$一个节点下标序列满足 $\displaystyle x_s\to x_t\Rightarrow s&lt;t$，也就是说父节点下标比子节点的小。这种下标排序成为图 $\displaystyle \mathcal{G}$的一个拓扑序。 2.3.2、任意图下的节点基本概念邻接点 $Neighbour$: $\displaystyle nb(s)=\{t\mid\mathcal{G}(s,t)=1\lor \mathcal{G}(t,s)=1,t\in V(\mathcal{G}) \}$ 边界点 $Boundary$: $\displaystyle bd(s)= pa(s)\cap nb(s)$ 节点度 $degree$： $\displaystyle d(v)=\mid E(v)\mid$ 含义是关联到节点 $\displaystyle v$的边的条数。特别的，对于有向图。有入度in-degree（父节点数量），出度out-degree（子节点数量）。 2.4、关于边的认识路 $\displaystyle \textit{(Path)}$有节点集合 $\displaystyle P=\{v_1,\cdots,v_k\},k\geqslant 3$， 若有 $\displaystyle \forall v_i\in P\subseteq V(\mathcal{G})\Rightarrow v_i\to v_{i+1}\lor v_i-v_{i+1}$存在，那么就说该节点集合 $\displaystyle P$在图 $\displaystyle \mathcal{G}$中形成了一条路径，记为 $\displaystyle \mathcal{G}_P$。 迹 $\displaystyle \textit{(Trail)}$有节点集合 $\displaystyle T=\{v_1,\cdots,v_k\},k\geqslant 3$， 若有 $\displaystyle \forall v_i\in T\subseteq V(\mathcal{G})\Rightarrow v_i\rightleftarrows v_{i+1}$存在，那么就说该节点集合 $\displaystyle T$在图 $\displaystyle \mathcal{G}$中形成了一条迹，记为 $\displaystyle \mathcal{G}_T$。 环 $\displaystyle \textit{(Cycle)}$节点集合 $\displaystyle C=\{v_1,\cdots,v_k\}$是图 $\displaystyle \mathcal{G}$的一条路，若还有 $\displaystyle v_k\to v_1\lor v_k-v_{1}$存在，这称节集合 $\displaystyle C$在图 $\displaystyle \mathcal{G}$中的形成了一个环，记为 $\displaystyle \mathcal{G}_C$。如果一个图不包含环，称该图为无环图 $\displaystyle \textit{(Acyclic Graph)}$。 圈 $\displaystyle \textit{(Loop)}$节点集合 $\displaystyle L=\{v_1,\cdots,v_k\}$是图 $\displaystyle \mathcal{G}$的一条迹，若有 $\displaystyle v_k\rightleftarrows v_1$存在，这称节点集合 $\displaystyle L$在图 $\displaystyle \mathcal{G}$中形成了一个圈，记为 $\displaystyle \mathcal{G}_L$。 连通非空图 $\displaystyle \mathcal{G}$的任意两个节点都有一条路存在，就成 $\displaystyle \mathcal{G}$是连通的。 无圈图是森林，特别的有向无圈图是多重树，如果每个节点至多只有一个父节点，则该有向图为森林，连通的森林是树。 弦图 $\displaystyle \mathcal{G}$中的一个圈 $\displaystyle \mathcal{G}_L$，在 $\displaystyle L$中不连贯的节点的边成为弦。 2.5、导出子图与团导出子图 $\displaystyle \textit{(Induced Subgraph)}$在概率图中，我们关心的是导出子图：若 $\displaystyle A\subseteq V(\mathcal{G})$, 由 $\displaystyle A$生产一个图 $\displaystyle \mathcal{G}_A$，且 $\displaystyle E(\mathcal{G}_A)=\{st\mid \forall st\in E(\mathcal{G})\land s,t\in A\} $，我们称 $\displaystyle \mathcal{G}_A$是 $\displaystyle A$在图 $\displaystyle \mathcal{G}$中的导出子图。也就是说 $A= V(\mathcal{G}_{A})\subseteq V(\mathcal{G})$， $E(\mathcal{G}_A)\subseteq E(\mathcal{G})$ 团 $\displaystyle \textit{(clique)}$若导出子图 $\displaystyle \mathcal{G}_A$中，这个命题成立：$\displaystyle \forall s,t \in A\to \mathcal{G}_A(s,t)=1$；也就是说 $\displaystyle A$中的任意节点都有一条边，就称 $\displaystyle A$为团。特别的 $\displaystyle \forall B\supset A,B\subseteq V(\mathcal{G})$， $\displaystyle B$不是团，我们就称 $\displaystyle A$为极大团 $\displaystyle \textit{(Maximal Clique)}$。 2.6、有向图与联合分布有向图中，联合概率分布根据链式法则可以表示为一系列条件概率的乘积。我们将条件概率分布的条件的随机变量，作为边的起点。这样有: 有向图与联合分布 三、有向无环图模型$\displaystyle \textit{ (DGM or DAG) }$任意图模型的核心是一组条件独立假设。我们发现有不同方法来描述有向无环图模型 $\displaystyle \textit{(Directed Aclyclic Graphy)}$的这些条件独立假设，最后我们可以证明它们说的是一个意思。 3.1、有向马尔可夫性质【有序马尔可夫性质 $\displaystyle O$】: 给定一个有向无环图的拓扑序 $\displaystyle \{1,\cdots,k,\cdots,K\}$，则有 $\displaystyle p(\bm{x})=\prod_{k=1}^Kp\big(x_k\mid Pa(x_k)\big)$。也就是说：给定父节点，当前节点与前置节点独立$$\begin{align}O=\left\{S\mid S:\forall x_k\in V(\mathcal{G})\to x_k\perp_\mathcal{G} \bm{x}_{pred(k)-pa(k)}\mid \bm{x}_{pa(k)}\right\}\end{align}$$ 【有向局部马尔可夫性质 $\displaystyle L$】: 给定父节点，当前节点与非后代节点独立$$\begin{align}L=\{S\mid S:\forall x_t\in V(\mathcal{G})\to x_t\perp_\mathcal{G} \bm{x}_{nd(t)-pa(t)}\mid \bm{x}_{pa(t)}\}\end{align}$$其中 $\displaystyle nd(t)=V(\mathcal{G})-\{t\}\cup de(t)$，且有$\displaystyle pred(t)\subseteq nd(t)$。 【全局马尔可夫性质 $\displaystyle G$】$$\begin{align}G=\{S \mid S:\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C\land A,B,C\subseteq V(\mathcal{G})\}\end{align}$$这些性质统称有向马尔可夫性质，它们之间是等价的。没错是等价的！！！ 3.2、有向马尔可夫性质定理$$\begin{align}O\Longleftrightarrow L \Longleftrightarrow G\end{align}$$ 3.3、有向分离与贝叶斯球算法3.3.1、迹的有向分离 如何找到全局马尔可夫性质，下面我们引入有向分离 $\displaystyle \textit{(D-Separation)}$的概念,首先，我们说一条迹 $\displaystyle T$被节点集 $\displaystyle E\textit{ (containing the evidence)}$有向分离，那么该迹至少满足下面一条：1、 $\displaystyle T$ 含有链迹 $\displaystyle x\to y\to z$ 或者 $\displaystyle x\leftarrow y\leftarrow z$，且 $\displaystyle y\in E$ 2、 $\displaystyle T$ 含有叉迹 $\displaystyle x \swarrow ^y \searrow z$，且 $\displaystyle y\in E$ 3、$\displaystyle T$ 含有撞迹 $\displaystyle x \searrow _y \swarrow z$,且 $\displaystyle y,\notin E \land de(y)\cap E=\emptyset$ 3.3.2、节点集的有向分离接下来，我们说给定已观测节点集 $\displaystyle E$，一节点集合 $\displaystyle A$与另一节点集 $\displaystyle B$有向分离，意思是说，对于每一条 $\displaystyle a\in A$到 $\displaystyle b\in B$的迹被节点集 $\displaystyle E\textit{ (containing the evidence)}$有向分离。 3.3.3、有向无环图的条件独立性质最后，我们就能把图的分离性定义与分布的条件独立性质结合起来：$$\begin{align}\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_E \iff \textit{ A is d-separation from B given E}\end{align}$$ 我们来详细考察一下：【链迹】考察联合分布 $\displaystyle p(x,y,z)=p(x)p(y\mid x)p(z\mid y)$，然后我们考察基于 $\displaystyle y$的条件分布：$$\begin{align}p(x,z\mid y)=\frac{p(x)p(y\mid x)p(z\mid y)}{p(y)}=\frac{p(x,y)p(z\mid y)}{p(y)}=p(x\mid y)p(z\mid y)\iff x \perp z\mid y\end{align}$$ 【叉迹】考察联合分布 $\displaystyle p(x,y,z)=p(y)p(x\mid y)p(z\mid y)$，同样我们考察 $\displaystyle y$的分布：$$\begin{align}p(x,z\mid y)=\frac{p(y)p(x\mid y)p(z\mid y)}{p(y)}=p(x\mid y)p(z\mid y)\iff x \perp z\mid y\end{align}$$ 【撞迹】考察联合分布 $\displaystyle p(x,y,z)=p(x)p(z)p(y\mid x,z)$，同样我们考察 $\displaystyle y$的分布：$$\begin{align}p(x,z\mid y)=\frac{p(x)p(z)p(y\mid x,z)}{p(y)}\iff x \require{cancel} \cancel{\perp} z\mid y\end{align}$$但是，我们发现$$\begin{align}p(x,z)=\int p(x)p(z)p(y\mid x,z) dy=p(x)p(z)\iff x\perp z\end{align}$$我们把撞迹的这种现象叫解释消除 $\displaystyle \textit{(explaining away)}$ 3.3.4、贝叶斯球算法贝叶斯球算法 $\displaystyle \textit{(shachter 1998)}$是一种判断节点集是否有向分离的简单方法： 贝叶斯球算法 3.4、马尔可夫毯与全条件下面我们将考察单个节点与其他节点的独立性质。我们定义图中所有与 $\displaystyle t$节点独立的其他节点集称为 $ t$的马尔可夫毯 Markov Blanket ，记为 $\displaystyle mb(t)$。 特别的在有向无环图中一个节点的马尔可夫毯是父节点、子节点、同父节点 (co-parents) (或者说它子节点的父节点)的集合。即：$$\begin{align}mb(t)=ch(t)\cup pa(t) \cup copa(t)\end{align}$$ 为何同父节点在马尔科夫毯中？注意到： $$\begin{align}p\big(x_t\mid \bm{x}_{-t}\big)= \frac{ p\big(x_t ,\bm{x}_{-t}\big)}{ p\big( \bm{x}_{-t}\big)}\end{align}$$ 其中所有不包含 $\displaystyle x_t$的项将在分子和分母之间被消掉，所以在这个视角下只剩下含有 $\displaystyle x_t$的概率密度的乘积。这个一点使用贝叶斯球算法的链迹、叉迹、撞迹、阻塞、反弹的概念判定独立不难理解。因此有： $$\begin{align} p\big(x_t\mid \bm{x}_{-t}\big) \propto p \big(x_t\mid \bm{x}_{pa(t)}\big)\prod_{s\in ch(t)}p \big(x_s\mid \bm{x}_{pa(s)}\big) \end{align}$$ 得到的表达式被称为 $\displaystyle t$节点的全条件 (Full Conditional) ，当我们学习吉布斯抽样时这一结论是非常重要的。 四、图模型与联合概率分布图模型是一种语言，联合概率分布也是一种语言。它们可以表达相同的事物。正如几何与代数的关系。下面我们将引入一系列定义，以方便寻找它们的联系。 4.1、图模型的条件独立性质与 $\displaystyle \text{I-Map}$我们即将引入 $\displaystyle \text{I-Map}$的概念与定义，以方便简洁表达。这种手法是发展数学的动力之一。我们知道：任意图模型 $\displaystyle \mathcal{G}$的核心是一组条件独立假设集合，可以记为 $$\begin{align}\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C\end{align}$$ 我们根据这一叙述可以定义一个“陈述” $\displaystyle \textit{(Senantics)}$。 $\displaystyle S:\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C$, $\displaystyle A,B,C\subseteq V(\mathcal{G})$。然后我们把“所有条件独立陈述的集合”定义为：$\displaystyle \mathcal{I}(\mathcal{G})=\{S\}$。也就是说：$\displaystyle\mathcal{I}(\mathcal{G})$是图模型 $\displaystyle \mathcal{G}$的所有条件独立陈述的集合。 同样样给出一个联合概率分布 $\displaystyle p$的所有条件独立陈述的集合，记为： $\displaystyle \mathcal{I}(p)$。 当然这些陈述是可以判断真假的，是真命题。至于为什么没有直接使用“命题的集合”这个的术语，考虑到我们是在给现实世界建模，而模型是否为真，还不一定，使用“陈述”这一术语是恰当的。 特别的：“一个联合分布 $\displaystyle p$满足一个图模型 $\displaystyle \mathcal{G}$的所有条件独立假设”。这句话可以写为：$$\begin{align}\mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)\end{align}$$这种情况下，我们就说“图 $\displaystyle \mathcal{G}$是分布 $\displaystyle p$的一个 $\displaystyle\text{I-Map}$”。很快我们就会发现关于 $\displaystyle\mathcal{I}(\mathcal{G})$定义中的“所有”并不是必须的。也就说一个 $\displaystyle p$的 $\displaystyle\text{I-Map}$可以有很多个，例如完全图是任意分布的 $\displaystyle p$的 $\displaystyle\text{I-Map}$。 4.2、因子分解同样，我们引入因子分解 $\displaystyle \textit{(factorization)}$的概念与定义。注意因子分解应该理解为一个动词。 若有向无环图模型 $\displaystyle \mathcal{G} $，有随机变量集(节点集) $\displaystyle \mathcal{X}=V(\mathcal{G})=\{x_i\}_{i=1}^n$，如果联合分布 $\displaystyle p$能表示为如下乘积形式：$$\begin{align}\displaystyle p(\bm{x})=\prod_{i=1}^np\big(x_i\mid \bm{x}_{pa(i)}\big)\end{align}$$那么我们称联合分布 $\displaystyle p$基于图 $\displaystyle \mathcal{G}$在同一空间 $\displaystyle \mathcal{X}$ 上因子分解 ($\displaystyle \textit{ p over the same space } $$\displaystyle \mathcal{X}$ $\displaystyle \textit{ factorizes according to }\mathcal{G}$)，记为 $\displaystyle F$。 4.3、条件独立因子分解定理 $$\begin{align}\mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)\Longleftrightarrow p(\bm{x})=\prod_{i=1}^np\big(x_i\mid \bm{x}_{pa(i)}\big)\end{align}$$ 也就是说一个联合分布 $\displaystyle p$满足一个图模型 $\displaystyle \mathcal{G}$的所有条件独立假设，与联合分布 $\displaystyle p$基于图 $\displaystyle \mathcal{G}$在同一空间 $\displaystyle \mathcal{X}$ 上因子分解等价。 于是我们有：$$\begin{align}O\Longleftrightarrow L \Longleftrightarrow G \Longleftrightarrow F\end{align}$$ 五、无向图模型(Undirected Graphical Model(UGM))5.1、UGMs的特点无向图模型 Undirected Graphical Model(UGM) ，也称为马尔可夫随机场 Markov random field(MRF) 或马尔可夫网络。它不要求我们指定边方向，而且对于一些问题，如图像分析和空间统计，更自然。 UGMs 相对 DGMs 的主要优势是:(1)它们是对称的，因此对于某些领域(如空间或关系数据)更“自然”;(2)定义 $\displaystyle p \big(\bm{y}\mid \bm{x}\big)$条件密度的判别 UGMs (即条件随机场 Conditional random ﬁelds, or CRFs )，比判别 DGMs 更有效。 与 DGMs 相比， UGMs 的主要缺点是:(1)参数的可解释性更小，模块化更少。(2)参数估计在计算成本上更加昂贵。 5.2、UGMs的条件独立性质UGMs 通过简单的图分离来定义 CI 关系。对于节点 $ A$、 $ B$和 $ C$的集合，如果在图 $ \mathcal{G}$中 $ C$把 $ A$从 $ B$分离，我们就说 $\displaystyle \bm{x}_A \perp_{\mathcal{G}} \bm{x}_b\mid \bm{x}_c$。这意味着，当我们删除 $ C$中所有节点，如果没有任何路(Path)连接 $ A$中任何节点到 $ B$中的任何节点，就称 CI 性质。这被称为 UGMs 的全局马尔可夫性质 (global Markov property) 。 在一个图中，使与节点 $ t$有条件独立性质最小节点集称为 $ t$的马尔可夫毯 Markov blanket ；我们将用 $ mb(t)$表示。在形式上，马尔可夫毯满足以下性质: $$\begin{align}t\perp V(\mathcal{G})-cl(t)\mid mb(t)\end{align}$$ 其中 $\displaystyle cl(t)=mb(t)\cup \{t\}$是节点 $ t$的闭包 (closure) 。易知，在一个 UGM 中，节点的马尔可夫毯是它紧邻的集合。这叫做无向局部马尔可夫性质。 从局部马尔可夫性质，我们可以很容易看出如果两个节点之间没有直接连接的边，给定其余节点，它们是条件独立的。这叫做成对马尔可夫性质(Pairwise Markov Property) 记为 $ P$ 。 $$\begin{align}s\perp t \mid V(\mathcal{G})-\{s,t\}\iff \mathcal{G}(s,t)=0\end{align}$$ 很明显，全局马尔可夫性质包含了局部马尔可夫性质，它意味着成对马尔可夫性质。虽然不那么明显，但仍然是正确的(假设对所有 $ x$的 $ p(x)&gt;0$，即 $ p$是一个正密度)，这时成对马尔可夫性蕴含全局马尔可夫性质，因此所有这些马尔可夫性质都是等价的，如图19.3所示(参见Koller和Friedman 2009,p119)。这一结果的重要性在于，它通常更容易根据经验来评估成对条件独立性；可以使用这种成对条件独立性语句构造一个图形，而从这个图中提取的全局独立性语句也成立。 $$\begin{align} G\Longrightarrow L \Longrightarrow P \mathop{===\Longrightarrow}^{\forall x\to p(x)&gt;0} G\end{align}$$ 或者说$$\begin{align}\forall x\to p(x)&gt;0 \Rightarrow G \iff L \iff P\end{align}$$ 5.3、Hammersley-Clifford定理由于没有与无向图相关的拓扑排序，我们不能使用链式法则来表示 $ p \big(\bm{x}\big)$。因此，我们将概率密度与每个节点关联起来替换方法是，将势函数或因子与图中每个极大团联系起来。我们将团 $ c$势函数记为 $ \psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)$。势函数可以是其参数的任何非负函数。然后定义联合分布与团势函数乘积成比例。令人惊讶的是，可以通过这样的方式来表示任何可以由无向图模型表达条件独立性质的正分布。我们更正式地陈述这个结果如下 【 Hammersley-Clifford 定理】一个正分布 $ p \big(\bm{x}\big)&gt;0$ 能满足一个无向图模型 $ \mathcal{G}$的条件独立性质，当且仅当 $ p$能够被每个极大团因子的乘积表示。 $$\begin{align}\forall x\to p(x)&gt;0\Rightarrow \mathcal{I}(p)=\mathcal{I}(\mathcal{G})\iff p \big(\bm{x}\mid \bm{\theta}\big)=\frac{1}{Z(\bm{\theta})}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 其中 $ \mathcal{C}$是图 $ \mathcal{G}$ 所有极大团的集合。 $ Z(\bm{\theta})$是配分函数：$$\begin{align}Z(\bm{\theta})=\sum_{\bm{x}}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 请注意，配分函数确保了整体分布总和为 $ 1$。证明从未发表过，但可以在例如(Koller and Friedman 2009)中找到。 六、比较有向图模型和无向图模型哪一个模型有更强的“表达能力”，是有向图模型还是无向图模型？为了形式化这个问题，回顾一下：如果 $ \mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)$，则 $ \mathcal{G}$是一个分布 $ p$的 $\text{I-Map}$。如果 $ \mathcal{I}(\mathcal{G})= \mathcal{I}(p)$，那么就称 $ \mathcal{G}$为 $ p$的完美映射 (Perfect Map) $ \text{P-Map}$，换句话说，图可以表示分布的所有(且唯一)条件独立性质。问题就变为：对于不同分布集合，是有向图模型还是无向图模型是完美映射。从这个意义上说，两者都不是比另一个更强大的表示语言。 一些条件独立关系的例子可以完全由有向图模型建模，而无向图模型不能。考虑一个 $ v$结构$ A \searrow _C \swarrow B$，这表示 $ A\perp B$ 且 $\require{cancel} A\cancel{\perp} B\mid C$。如果我们去掉箭头，得到 $ A-C-B$，这表示 $\require{cancel} A\cancel{\perp} B$ 且 $ A\perp B\mid C$。这种去掉箭头的操作显然是不对的。事实上，没有无向图模型能精准完全唯一地表达由 $ v$结构编码的两个条件独立性质。一般的，无向图模型的条件独立性质是单调的，这里的单调有如下含义：如果 $ A \perp B \mid C$，则 $ A\perp B \mid \big(C\cup D\big)$。但在有向图模型中，条件独立性质是非单调的，因为存在附加变量的条件作用可以消除由于解释消除 $\displaystyle \textit{(explaining away)} $生产的条件独立性质。 图模型比较 一些条件独立关系的例子可以完全由无向图模型建模的，而有向图模型不能。可以考虑如图(a)所示的 $ 4$环。图(b)显示了用有向图模型建模的尝试，它能正确表达 $ A\perp C \mid B,D$，但表达的 $ B\perp D\mid A$是错误的。图(c)是另一个不正确的有向图模型，它正确地编码了 $ A\perp C \mid B,D$，但是编码的 $ B\perp D$是错误的。事实上，没有有向图模型能够精准完全唯一地表达由这个由无向图模型编码的条件独立陈述集合。 有些分布可以完全由有向图模或无向图模型建模；得到的图称为可分解 (decomposable) 或弦 (chordal) 。粗略地说，这意味着：如果我们把每个最大团的所有变量一起坍缩生成“大变量” (“mega-variables”,) ，结果图将是一棵树。当然，如果图已经是树了(包括链作为特殊情况)，它将是弦。有关详细信息，我们以后详细介绍。 七、总结1、概率是一个好工具，这个工具的核心在于概率的条件独立性质。或者说高级一点条件期望。没有条件期望，概率论不过是测度论的一个表现形式。正是我们定义了条件概率，概率论才枝深叶茂起来。但是表达条件概率不仅仅用数学公式，还可以用图。于是一门新的学科诞生了：概率图。2、无论是无向图、有向图，其核心都是一组条件概率假设(或者条件概率陈述)。而在非常宽泛条件下，一组条件概率假设和一个图表意是一致。这就奠定了概率图应用的基础。3、有了图模型这一强有力的可视化建模工具，我们足矣为各种复杂场景建模，星辰大海就在前方，我们将继续探索。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/b9646fbf6c8d542fdb68ac01ad3c1c5a/如果您需要引用本文，请参考：引线小白. (Mar. 10, 2017). 《概率图基础》[Blog post]. Retrieved from https://www.limoncc.com/post/b9646fbf6c8d542fdb68ac01ad3c1c5a@online{limoncc-b9646fbf6c8d542fdb68ac01ad3c1c5a,title={概率图基础},author={引线小白},year={2017},month={Mar},date={10},url={\url{https://www.limoncc.com/post/b9646fbf6c8d542fdb68ac01ad3c1c5a}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广义线性模型]]></title>
    <url>%2Fpost%2Fc7d551950374c4114400ca0e635169ba%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/c7d551950374c4114400ca0e635169ba/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清广义线性模型的问题。若有错误，请大家指正。关键词: 广义线性模型,广义线性混合模型,指数族,probit 回归 一、 简述我们现在遇到了各种各样的概率分布：高斯分布、伯努利分布、学生 $\displaystyle \text{t}$分布、均匀分布、伽玛分布等等，这些都是广义分布的广义函数，称为指数族。在本章中，我们将讨论这个家族的各种性质。这使我们能够得到具有非常广泛适用性的定理和算法。 我们将会看到，如何方便地将指数家族中的任何成员作为一个类条件密度，以便做一个生成分类器。此外，我们还将讨论如何建立判别模型，其中的响应变量有指数族分布，其均值是输入的线性函数；这被称为广义线性模型，并将 $\displaystyle \textit{logistic } $回归中的概念推广到其他类型的响应变量中。 二、 指数族在定义指数族之前，我们先提几个重要原因: •可以证明，在一定的规律性条件下，指数族是唯一具有有限大小统计量的分布族，这意味着我们可以将数据压缩成一个固定大小的摘要，而不会丢失信息。这对于在线学习特别有用，我们稍后会看到。 •指数族是有共轭先验分布存在的唯一分布族，这简化了后验计算。 •在满足一些约束的假设下，指数族是熵最大的一个分布家族。 •指数族是广义线性模型的核心。 •指数族是变分推断的核心。 2.1、定义参数为 $\displaystyle \bm{w} $，变量为 $\displaystyle \bm{x} $ 的指数分布族定义为下面形式的概率分布集合：$$\begin{align}p(\bm{x}\mid \bm{w})&amp;=\frac{1}{Z(\bm{w})}h(\bm{x})\exp\left[\bm{w}^\text{T}\bm{T}(\bm{x})\right]=h(\bm{x})\exp\big[\bm{w}^\text{T}\bm{T}(\bm{x})-A(\bm{w})\big]\\end{align}$$特别的为了适合上述形式，参数通常是需要变形 $\displaystyle \bm{\eta}=\bm{\eta}(\bm{w})$，于是有：$$\begin{align}p(\bm{x}\mid \bm{w})&amp;=\frac{1}{Z(\bm{w})}h(\bm{x})\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})\right]\\\\&amp;=h(\bm{x})\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})-A(\bm{\eta}(\bm{w}))\right]\\\\&amp;=\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})-A\left(\bm{\eta}(\bm{w})\right)+B(\bm{x})\right]\\\\&amp;=h(\bm{x})g(\bm{\eta})\exp\left[\bm{\eta}(\bm{w})\cdot\bm{T}(\bm{x})\right]\end{align}$$其中：$\displaystyle Z(\bm{w})=\int_{\mathcal{X}^n}h(\bm{x})\exp\left[\bm{\eta^\text{T}(\bm{w})}\bm{T}(\bm{x})\right]\mathrm{d}\bm{x}$ $\displaystyle A(\bm{w})=\ln Z(\bm{w}) $1、$\displaystyle \bm{w} $叫做自然参数 $\displaystyle \textit{(natural parameters)} $、$\displaystyle \bm{\eta(\bm{w})} $叫做规范参数 $\displaystyle \textit{(canonical parameters)} $ 。如果 $\displaystyle \dim(\bm{w})&lt;\dim(\bm{\eta}) $，称分布为曲线指数族 $\displaystyle \textit{(curved exponential family)} $。这意味比参数更多的充分统计量。 2、 $\displaystyle \bm{T}(\bm{x})\in\mathbb{R}^k $叫做充分统计向量 $\displaystyle \textit{(vector of sufficient statistics)} $。 3、$\displaystyle Z(\bm{w}) $叫做配分函数 $\displaystyle \textit{(partition function)} $ 4、$\displaystyle A(\bm{w}) $叫做对数配分函数 $\displaystyle \textit{(log partition function)} $或者累积量函数 $\displaystyle \textit{(cumulant function)} $ 5、$\displaystyle h(\bm{x}) $叫做放缩常量 $\displaystyle \textit{(scaling constant)} $，通常是1。 6、特别的 $\displaystyle \bm{T}(\bm{x})=\bm{x} $ 称为自然指数族 $\displaystyle \textit{(natural exponential family)} $或者规范形 $\displaystyle \textit{(canonical form)} $ 2.2、对数配分函数指数族的一个重要性质是对数配分函数的导数可以用来生成充分统计量的累积量。所以 $\displaystyle A \big(\bm{\eta}\big)$有时被称为累积量函数。$$\begin{align}A(\bm{\eta})=\ln \bigg[\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}\bigg]\end{align}$$于是有$$\begin{align}\dot{A}(\bm{\eta})=\frac{\partial A}{\partial \bm{\eta}}&amp;=\frac{\displaystyle\frac{\partial }{\partial \bm{\eta}}\bigg[\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}\bigg]}{\displaystyle\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}}=\frac{\displaystyle \int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]\bm{T}d \bm{x}}{\exp \big[A(\bm{\eta})\big]}\\\\&amp;=\int \bm{T} \cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})-A(\bm{\eta})\big]d \bm{x}\\\\&amp;=\int \bm{T} \cdot p \big(\bm{x}\big)d \bm{x}=\mathrm{E}\big[\bm{T}(\bm{x})\big]\end{align}$$还有：$$\begin{align}\ddot{A}(\bm{\eta})=\frac{\partial^2 A}{\partial \bm{\eta}\partial\bm{\eta}^\text{T}}&amp;=\frac{\partial }{\partial \bm{\eta}^\text{T}}\bigg[\int \bm{T} \cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\cdot\bm{T}-A(\bm{\eta})\big]d \bm{x}\bigg]\\\\&amp;=\int \bm{T}^\text{T}\cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\cdot\bm{T}-A(\bm{\eta})\big]\big[\bm{T}-\dot{A}(\bm{\eta})\big]d \bm{x}\\\\&amp;=\int \bm{T}^\text{T}\big[\bm{T}-\dot{A}(\bm{\eta})\big]p (\bm{x})d \bm{x}\\\\&amp;=\mathrm{E}\big[\bm{T}^\text{T}\bm{T}\big]- \mathrm{E}\big[\bm{T}^\text{T}\big]\mathrm{E}\big[\mathrm{E}\big[\bm{T}\big]\big]=\mathrm{E}\big[\bm{T}^\text{T}\bm{T}\big]-\mathrm{E}^\text{T}\big[\bm{T}\big]\mathrm{E}\big[\bm{T}\big]\\\\&amp;=\mathrm{cov}\big[\bm{T}(\bm{x})\big]\end{align}$$也就是说$$\begin{align}\nabla^2A \big(\bm{\eta}\big)=\mathrm{cov}\big[\bm{T}(\bm{x})\big]\end{align}$$ 由于协方差是正定的，我们看到一个 $\displaystyle A \big(\bm{\eta}\big)$是一个凸函数(参见7.3.3)。注意上面的推导交换了积分和求导顺序，这需要满足一致收敛条件。这显然是满足的。 2.3、例子让我们考虑一些例子来让事情更清楚些。 2.3.1、 $\displaystyle \textit{0-1}$分布有 $\displaystyle x\in\{0,1\}$的 $\displaystyle \textit{0-1}$分布写成指数族形式：$$\begin{align}\mathrm{Ber}\big(x\mid \mu\big)=\mu^x(1-\mu)^{1-x}=\exp \big[x\ln \mu+(1-x)\ln (1-\mu)\big]=\exp \big[\bm{\eta}^\text{T}\bm{T}(x)\big]\end{align}$$ 其中：$\displaystyle \bm{\eta}=\big[\ln \mu,\ln (1-\mu)\big]^\text{T}$$\displaystyle \bm{T}(x) =\big[\mathbb{I}\big(x=0\big),\mathbb{I}\big(x=1\big)\big]^\text{T}$然后这个表示是过于完备以至于很啰嗦，因为特征之间有线性关系，这很不简洁。$$\begin{align}\bm{I}^\text{T}\bm{T}(x)=\mathbb{I}\big(x=0\big)+\mathbb{I}\big(x=1\big)=1\end{align}$$ 因为 $\displaystyle \bm{\eta}$不是唯一的，这要求我们 $\displaystyle \bm{\eta}$最好是最简洁的。也就是说分布均值和唯一的 $\displaystyle \bm{\eta}$联系。这样我们可以定义：$$\begin{align}\mathrm{Ber}\big(x\mid \mu\big)=(1-\mu)\exp \bigg[x\ln \bigg(\frac{\mu}{1-\mu}\bigg)\bigg]\end{align}$$现在 $\displaystyle \eta=\ln \bigg(\frac{\mu}{1-\mu}\bigg)$这是 $\displaystyle \textit{log-odds ratio} $，$\displaystyle T(x)=x$， $\displaystyle Z=1/(1-\mu)$。于是有：$$\begin{align}A(\eta)=\ln Z=\ln \bigg(\frac{1}{1-\mu}\bigg)=\ln \big[1+\mathrm{e}^\eta\big]\end{align}$$来自规范参数的均值 $\displaystyle \mu$$$\begin{align}\mu=\dot{A}(\eta)=\mathrm{sigm}\big(\eta\big)=\frac{1}{1+\mathrm{e}^{-\eta}}=\eta^{-1}(\mu)\end{align}$$ 2.3.2 、分类分布令 $\displaystyle x_i=\mathbb{I}(x=i)$，分类分布指数族形式是：$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\prod_{i=1}^c\mu_i^{x_i}=\exp \Bigg[\sum_{i=1}^cx_i\ln \mu_i\Bigg]\\\\&amp;=\exp \Bigg[\sum_{i=1}^{c-1}x_i\ln \mu_i+\bigg(1-\sum_{j=1}^{c-1}x_j\bigg)\ln \bigg(1-\sum_{j=1}^{c-1}\mu_j\bigg)\Bigg]\\\\&amp;=\exp \Bigg[\sum_{i=1}^{c-1}x_i\ln \bigg(\frac{\mu_i}{1-\sum_{j=1}^{c-1}\mu_j}\bigg)+\ln\bigg(1-\sum_{j=1}^{c-1}\mu_j\bigg)\Bigg]\\\\&amp;=\exp\Bigg[\sum_{i=1}^{c-1}x_i\ln \bigg(\frac{\mu_i}{\mu_c}\bigg)+\ln \mu_c\Bigg]\end{align}$$ 其中 $\displaystyle \mu_c=1-\sum_{j=1}^{c-1}\mu_j$，有：$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\exp \big[\bm{\eta}^\text{T}\bm{T}\big(\bm{x}\big)-A \big(\bm{w}\big)\big]\end{align}$$ 其中$\displaystyle \bm{\eta}=\big[\ln \frac{\mu_1}{\mu_c},\cdots,\ln \frac{\mu_{c-1}}{\mu_c}\big]^\text{T}$$\displaystyle \bm{T}\big(\bm{x}\big)=\bm{x}=\big[\mathbb{I}(x=1),\cdots,\mathbb{I}(x=c-1)\big]^\text{T}$同样，我们可以通过使用的规范参数来恢复均值参数$$\begin{align}\mu_i=\frac{\mathrm{e}^{\eta_i}}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}\end{align}$$易知：$$\begin{align}\mu_c=1-\frac{\sum_{i=1}^{c-1}\mathrm{e}^{\eta_i}}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}=\frac{1}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}\end{align}$$亦有$$\begin{align}A \big(\bm{\eta}\big)=\ln \bigg(1+\sum_{i=1}^{c-1}\mathrm{e}^{\eta_i}\bigg)\end{align}$$如果我们定义 $\displaystyle \eta_c=0\to \mu_{i=c}=0$，那么有 $\displaystyle \bm{\eta}:=\big[\bm{\eta}^\text{T},0\big]^\text{T}$， $\displaystyle \bm{T}:=\big[\bm{T}^\text{T},\mathbb{I}\big(x=c\big)\big]^\text{T}$$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\exp \big[\bm{\bm{\eta}^\text{T}T}\big(\bm{x}\big)-A \big(\bm{\eta}\big)\big]\end{align}$$其中 $\displaystyle A \big(\bm{\eta}\big)=\ln \bigg(\sum_{i=1}^{c}\mathrm{e}^{\eta_i}\bigg)\\\\$，特别的有：$$\begin{align}\bm{\mu}&amp;=\mathcal{S}\big(\bm{\eta}\big)=\bm{\eta}^{-1}\big(\bm{\mu}\big)\end{align}$$ 这样我们就通过指数族建立了 $\displaystyle \textit{sigm}$ 函数与 $\displaystyle \textit{0-1} $分布、 $\displaystyle \textit{softmax} $函数与分类分布的联系。换句话说 $\displaystyle \textit{sigm} $ 函数、 $\displaystyle \textit{softmax} $ 函数是连接函数的逆函数。这也是sigm和sofmax函数在机器学习，人工智能广泛应用的原因，因为只要分类问题，就必然涉及。 2.3.3、单变量高斯单变量高斯可以写成指数族形式，如下: $$\begin{align}\mathcal{N}\big(x\mid \mu,\sigma^2\big)&amp;=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{1}{2\sigma^2}\big(x-\mu\big)^2\big]\\\\&amp;=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{\mu^2}{2\sigma^2}\big]\exp \big[-\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x\big]\\\\&amp;=\frac{1}{Z \big(\bm{\eta}\big)}\exp \big[\bm{\eta}^\text{T}\bm{T}(x)\big]\end{align}$$ 其中:$\displaystyle \bm{\eta}=\big[\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\big]^\text{T}$$\displaystyle \bm{T}(x)=\big[x,x^2\big]^\text{T}$$\displaystyle Z \big(\bm{\eta}\big)=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{\mu^2}{2\sigma^2}\big] $$\displaystyle A \big(\bm{\eta}\big)=\ln Z \big(\bm{\eta}\big)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\ln \big(-2\eta_2\big)-\frac{1}{2}\ln 2\pi$ 2.2.4、非例子并非所有的分布都属于指数族。例如均匀分布 $\displaystyle x\sim \mathrm{U}[a,b]$ 就不是，因为分布支撑集 $\displaystyle \mathcal{X}$取决于参数。此外学生 $\displaystyle T$分布也不属于，因为它没有必要的形式。$\displaystyle \textit{Pitman - Koopman - Darmois} $定理指出，在一定规范条件下，指数家族是唯一具有有限统计量的分布族。(在这里，大小有限与数据集的大小无关。)这个定理要求的条件之一是，分布支撑集 $\displaystyle \mathcal{X}$不依赖于参数。对于这样分布的简单示例：均匀分布$$\begin{align}p(x\mid w)=\mathrm{U}(x\mid w)=\frac{\mathbb{I}\big(0\leqslant x\leqslant w\big)}{w}\end{align}$$似然函数是：$$\begin{align}p \big(\mathcal{D}\big)=w^{-n}\mathbb{I}\big(0\leqslant\max\{x_i\}\leqslant w\big)\end{align}$$ 因此充分统计量是 $\displaystyle n$和 $\displaystyle T=\max_{i}x_i$。它是有限的，但均匀分布并不在指数家族中。因为它的支撑集 $\displaystyle \mathcal{X}=[0,w]$ 依赖于参数 $\displaystyle w$。 2.4、指数族的 $\displaystyle \textit{MLE}$指数家族模型似然函数：$$\begin{align}p \big(\mathcal{D}\mid \bm{w}\big)&amp;=\frac{1}{Z^n(\bm{w})}\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\sum_{i=1}^n \bm{T}(\bm{x}_i)\bigg]\\\\&amp;=\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 易见似然函数也是指数族，它的充分统计量是 $\displaystyle n$和$$\begin{align}\bm{T}\big(\mathcal{D}\big)=\bigg[\sum_{i=1}^n T_1(\bm{x}_i),\cdots,\sum_{i=1}^nT_k(\bm{x}_i)\bigg]^\text{T}\end{align}$$ 例如伯努利分布我们有 $\displaystyle \bm{T}=\bigg[\sum_{i=1}^n \mathbb{I}(\bm{x}_i)\bigg]$和单变量高斯分布 $\displaystyle \bm{T}=\big[\sum_{i=1}^nx_i,\sum_{i=1}^nx_i^2\big]$(我们也需要知道样本大小 $\displaystyle n$) 现在我们给定 $\displaystyle \mathrm{idd}$数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$，用标准指数家族模型来计算 $\displaystyle \textit{MLE} $。对数似然函数：$$\begin{align}\ell(\bm{\eta})=\ln p \big(\mathcal{D}\mid \bm{\eta}\big)=\bigg[\sum_{i=1}^n\ln h(\bm{x}_i)\bigg]+\bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 因为 $\displaystyle -A \big(\bm{\eta}\big)$关于 $\displaystyle \bm{\eta}$是凹的， $\displaystyle \bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})$关于 $\displaystyle \bm{\eta}$是线性的。故对数似然是凹的，因此有唯一的全局最大值。为了得到这个最大值，利用关于对数分配函数导数是充分统计量期望的结论有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\bm{T}(\mathcal{D})-n \mathrm{E}\big[\bm{T}(\bm{x})\big]=\bm{0}\end{align}$$有：$$\begin{align}\mathrm{E}\big[\bm{T}(\bm{x})\big]=\frac{1}{n}\bm{T}\big(\mathcal{D}\big)\end{align}$$ 充分统计量的经验平均值必须等于模型充分统计量的理论期望，即 $\displaystyle \hat{\bm{\eta}}$必须满足上式。这叫做矩匹配 $\displaystyle \textit{(moment matching)} $。例如在伯努利分布中，我们有 $\displaystyle T(\mathcal{D})=\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)$有：$$\begin{align}\mathrm{E}\big[\bm{T}(\bm{x})\big]=\hat{\mu}=\frac{1}{n}\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)\end{align}$$ 2.5、指数族的贝叶斯我们已经看到，如果先验概率是共轭的，那么精确贝叶斯分析是相当简单的。非正式这意味着先验 $\displaystyle p \big(\bm{\eta}\mid \bm{\tau}\big)$与似然函数 $\displaystyle p \big(\mathcal{D}\mid \bm{\eta}\big)$有相同形式。为了这个有意义，我们要求似然函数的充分统计量是有限，所以我们可以写 $\displaystyle p \big(\mathcal{D}\mid \bm{\eta}\big)=p \big(\bm{T}(\mathcal{D})\mid \bm{\eta}\big)$。这表明，唯一有共轭先验的分布族是指数族。稍后我们将推导先验和后验。 2.5.1、似然函数我们多角度地写出指数族似然函数：$$\begin{align}p \big(\mathcal{D}\mid \bm{w}\big)&amp;=\frac{1}{Z^n(\bm{w})}\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})\bigg]\\\\&amp;\propto\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\\\\&amp;\propto\exp \bigg[n\bm{\eta}^\text{T}\bar{\bm{T}}-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 其中 $\displaystyle \bar{\bm{T}}=\frac{1}{n}\bm{T}(\mathcal{D})$ 2.5.2、先验共轭先验有如下形式：$$\begin{align}p \big(\bm{\eta}\mid \kappa_0,\bm{\tau}_0\big)\propto g^{\kappa_0}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{\tau}_0\big]\end{align}$$ 令 $\displaystyle \bm{\tau}=\kappa_0\bar{\bm{\tau}}_0$，这样我们分离出先验的虚数据集大小 $\displaystyle \kappa_0$，和虚数据集充分统计量均值 $\displaystyle \bar{\bm{\tau}}_0$。在规范形式中，先验成为：$$\begin{align}p \big(\bm{\eta}\mid \kappa_0,\bar{\bm{\tau}}_0\big)\propto \exp \big[\kappa_0\bm{\eta}^\text{T}\bar{\bm{\tau}}_0-\kappa_0A(\bm{\eta})\big]\end{align}$$ 2.5.3、后验有后验$$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}\big)= p \big(\bm{\eta}\mid \kappa_n,\bm{\tau}_n\big)=p \big(\bm{\eta}\mid \kappa_0+n,\bm{\tau}_0+\bm{T}\big)\end{align}$$ 上式更新了超参。在规范形式下变成了$$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}\big)= p \big(\bm{\eta}\mid \kappa_n,\bar{\bm{\tau}}_n\big)=p \big(\bm{\eta}\mid \kappa_0+n,\frac{ \kappa_0\bar{\bm{\tau}}_0+n\bar{\bm{T}}}{\kappa_0+n}\big)\end{align}$$ 因此我们看到后验超参是先验超参均值和充分统计量平均值一个凸组合。 2.5.4、后验预测密度下面我们鉴于过去的数据集 $\displaystyle \mathcal{D}_t$，推导未来观测 $\displaystyle \mathcal{D}_{t+1}$的一个通用的预测密度的表达式。为简洁记，我们将把充分统计量与数据集大小写在一起： $\displaystyle \tilde{\bm{\tau}}_0=[\kappa_0;\bm{\tau}_0]$、 $\displaystyle \tilde{\bm{T}}_t=\big[n;\bm{T}\big(\mathcal{D}_t\big)\big]$、 $\displaystyle \tilde{\bm{T}}_{t+1}=\big[n;\bm{T}\big(\mathcal{D}_{t+1}\big)\big]$。先验、后验有：$$\begin{align}p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0\big)=\frac{1}{Z( \tilde{\bm{\tau}}_0)}g^{\kappa_0}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{\tau}_0\big]\end{align}$$ $$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}_t\big)&amp;= \frac{p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0\big)p \big(\mathcal{D}_t\mid \bm{\eta}\big)}{p(\mathcal{D}_t)}=p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t\big)\\\\&amp;=\frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}g^{\kappa_0+n_t}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t\big]\big]\end{align}$$ $\displaystyle p \big(\mathcal{D}_{t+1}\mid \bm{\eta}\big)=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg]g^{n_{t+1}}\big(\bm{\eta}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{T}_{t+1}\big]$那么后验预测为：$$\begin{align}p \big(\mathcal{D}_{t+1}\mid \mathcal{D}_t\big)&amp;=\int p \big(\mathcal{D}_{t+1}\mid \bm{\eta}\big)p \big(\bm{\eta}\mid \mathcal{D}_t\big)d \bm{\eta}\\\\&amp;=\int\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg]g^{n_{t+1}}\big(\bm{\eta}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{T}_{t+1}\big]\frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}g^{\kappa_0+n_t}(\bm{w})\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t\big]\big]d \bm{\eta}\\\\&amp;=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg] \frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}\int g^{\kappa_0+n_t+n_{t+1}}\big(\bm{w}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t+\bm{T}_{t+1}\big]\big]d \bm{\eta}\\\\&amp;=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg] \frac{Z\big( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t+\tilde{\bm{T}}_{t+1}\big)}{Z\big( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t\big)}\end{align}$$的可能性和后部有一个类似的形式。因此如果 $\displaystyle n_t = 0$，这就变成了 $\displaystyle \mathcal{D}_{t+1}$的边际分布，这是我们熟悉后验的归一乘以一个常数。 2.5.5、伯努利分布举例作为一个简单的例子，我们用新表示法来重新讨论一下：贝塔-伯努利模型。似然函数是$$\begin{align}p \big(\mathcal{D}\mid \mu\big)=(1-\mu)^n\exp \bigg[\ln \bigg(\frac{\mu}{1-\mu}\bigg)\sum_{i=1}^n x_i\bigg]\end{align}$$ 共轭先验是：$$\begin{align}p \big(\mu\mid \kappa_0,\tau_0\big)&amp;\propto (1-\mu)^{\kappa_0}\exp \bigg[\ln \bigg(\frac{\mu}{1-\mu}\bigg)\tau_0\bigg]=\mu^{\tau_0}\big(1-\mu\big)^{\kappa_0-\tau_0}\end{align}$$ 如果我们定义 $\displaystyle a_0=\tau_0+1$和 $\displaystyle b_0=\kappa_0-\tau_0+1$，我们可以看到这是一个贝塔分布。 我们可以推导出后验，其中 $\displaystyle T=\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)$是充分统计量:$$\begin{align}p \big(\mu\mid \mathcal{D}\big)&amp;\propto \mu^{\tau_0+T}\big(1-\mu\big)^{\kappa_0-\tau_0+n-T}\\\\&amp;=\mu^{\tau_n}\big(1-\mu\big)^{\kappa_n-\tau_n}\\\\&amp;=\mu^{a_n-1}\big(1-\mu\big)^{b_n-1}\end{align}$$其中 $\displaystyle a_n=\tau_n+1$、 $\displaystyle b_n=\kappa_n-\tau_n+1$ 我们可以推导出后验的预测分布。假设 $\displaystyle p (\mu)=\mathrm{Beta}\big(\mu\mid a_0,b_0\big)$，并让 $\displaystyle T_t=T(\mathcal{D}_t)$是硬币正面在过去的数量。我们可以预测给定一个未来序列 $\displaystyle \mathcal{D}_{t+1}$的出现正面的概率。令这个序列的充分统计量 $\displaystyle T_{t+1}=\sum_{i=1}^m \mathbb{I}\big(x_i^{t+1}=1\big)$$$\begin{align}p \big(\mathcal{D}_{t+1}\mid \mathcal{D}_t\big)&amp;=\int_0^1 p \big(\mathcal{D}_{t+1}\mid \mu\big)p \big(\mu\mid \mathcal{D}_t\big)d\mu\\\\&amp;=\int_0^1 p \big(\mathcal{D}_{t+1}\mid \mu\big)\mathrm{Beta} \big(\mu\mid a_{t},b_{t}\big)d\mu\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma{a_t}\Gamma(b_t)}\int_0^1 \mu^{a_t+T_{t+1}-1}\big(1-\mu\big)^{b_t+m-T_{t+1}-1}\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma{a_t}\Gamma(b_t)}\int_0^1 \mu^{a_{t+1}-1}\big(1-\mu\big)^{b_{t+1}-1}\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma(a_t)\Gamma(b_t)}\frac{\Gamma(a_{t+1})\Gamma(b_{t+1})}{\Gamma(a_{t+1}+b_{t+1})}\end{align}$$其中$\displaystyle a_{t+1}=a_t+T_{t+1}=\tau_0+T_t+1+T_{t+1}$$\displaystyle b_{t+1}=\kappa_n-\tau_n+1=\kappa_0+n-(\tau_0+T_t)+1+m-T_{t+1}$ 2.6、指数族与最大熵原理 $\displaystyle \textit{(Maximum Entropy)} $虽然指数家族很方便，但对它的使用有更深层次的理由吗？事实存在这样的情况：如果用最少约束来假设数据，特别是假设某些特征或函数的期望$$\begin{align}\int \bm{\phi}(\bm{x})p(\bm{x})d \bm{x}=\mathrm{E}\big[\bm{\phi}\big]=\bm{\zeta}\end{align}$$ $\displaystyle \bm{\zeta}\in \mathbb{R}^k$是已知常数向量， $\displaystyle \bm{\phi}(\bm{x})$是一个任意向量函数，即要求满足于分布矩与指定函数经验矩相匹配的约束条件，那么最大熵原理 $\displaystyle \textit{maxent} $告诉我们应该选择最大熵分布(最接近于均匀分布的那个)。有约束条件的熵最大化$$\begin{align}J(p)=&amp; \mathrm{H}[p]=-\int p (\bm{x})\ln p(\bm{x}) d \bm{x}\\\\&amp;\mathrm{s.t.}\begin{cases}\displaystyle p(\bm{x})\geqslant 0\\\\\displaystyle \int p(\bm{x})d \bm{x}= 1\\\\\displaystyle \int \bm{\phi}(\bm{x})p(\bm{x})d \bm{x}=\bm{\zeta}\end{cases}\end{align}$$我们需要使用拉格朗日乘数法，那么拉格朗日算符更新为：$$\begin{align}F(p)=-p (\bm{x})\ln p(\bm{x})+\lambda p(\bm{x})+\bm{\eta}^\text{T}\big[\bm{\phi}(\bm{x})p(\bm{x})\big]\\\\\end{align}$$ 我们可以用变分法计算函数 $\displaystyle p$，欧拉-拉格朗日方程是：$$\begin{align}-1-\ln p(\bm{x})+\lambda+\bm{\eta}^\text{T}\bm{\phi}(\bm{x})=0\iff p(\bm{x})=\exp[\lambda-1]\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]\end{align}$$亦有 $\displaystyle p(\bm{x})\propto\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]$、给出了归一化常数 $\displaystyle Z=\int \exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]d \bm{x}$ 有：$$\begin{align}p(\bm{x})=\frac{1}{Z}\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]\end{align}$$因此，最大熵分布 $\displaystyle p(x)$具有指数族的形式，也称为吉布斯分布 $\displaystyle \textit{(Gibbs Distribution)} $。当然我们也可以写成离散形式，这是很容易的，略。 三、广义线性模型$\displaystyle \textit{(GLMs)} $3.1、概要我们熟悉了一下指数族，建立广义模型的初衷是要解决，经典线性模型$$\begin{align}y=\bm{w}^\text{T}\bm{x}+e\sim\mathcal{N}(\mu,\sigma^2)\end{align}$$的缺点： 1、因变量 $\displaystyle y$是连续的且服从高斯分布。 2、方差是固定的。 广义线性模型 于是内尔得和韦德伯恩（Nelder &amp; Wedderburn，1972）提出了广义线性模型。我们对指数族的形式稍加修改。$$\begin{align}p\left(y\mid\eta,\delta\right)=\exp\left[\frac{y \cdot\eta-A(\eta)}{\delta}+c\left(y,\delta\right)\right]\end{align}$$其中 $\displaystyle \delta$是散度参数，通常是1、 $\displaystyle c\left(y,\delta\right)$是归一化参数、 $\displaystyle A(\eta) $ 是分配函数、 $\displaystyle \eta$是连接函数，所谓规范连接函数可以查阅维基百科。同时我们令连接函数：$\displaystyle g(\mu)=\eta=\bm{w}^\text{T}\bm{x}$ $\displaystyle \mu=\mathrm{E}\left[y\mid \bm{x}\right]=\dot{A}\left(\eta\right)=g^{-1}(\eta)$ $\displaystyle \mathrm{Var}\left[y\right]=\ddot{A}\left(\eta\right)\cdot\delta$于是有$$\begin{align}\left\{\begin{matrix}g(\mu)=\eta=\bm{w}^\text{T}\bm{x}\\\\e=y-\mu\\\\e\sim f \end{matrix}\right.\end{align}$$ 再次写出模型：$$\begin{align}p \left(y\mid \bm{x} ,\bm{w} ,\delta\right)=\exp\left[\frac{ y\cdot\bm{w}^\text{T}\bm{x}-A(\bm{w}^\text{T}\bm{x})}{\delta}+c\left(y,\delta\right)\right]\end{align}$$ 经过以上分析,可以知道广义线性模型的两个重要不同：1、连接函数：是因变量 $\displaystyle y$的期望的一个转换，此转换的变量 $\displaystyle g(\mu)$是回归参数 $\displaystyle \bm{w}$的一个线性函数 $\displaystyle \bm{x}^\text{T}\bm{w}$。 2、方差是因变量 $\displaystyle y$期望的函数： $\displaystyle\mathrm{Var}\left[y\right]=\ddot{A}\left(g(\mu)\right)\cdot\delta$ 3.2、对数似然函数广义线性模型有一个很吸引人的特性，即它可与逻辑斯蒂回归使用的方法相同。对于数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i,y_i\}_{i=1}^{n}$我们有对数似然函数： $$\begin{align}\ell(\bm{w})&amp;=\ln p \left(\mathcal{D}\mid \bm{w}\right)=\frac{1}{\delta}\left[\bm{y}^\text{T}\bm{X}\bm{w}-\sum_{i-1}^{n}A(\bm{w}^\text{T}\bm{x}_i)\right]+\sum_{i=1}^{n}c\left(y_i,\delta\right)\\\\&amp;=\frac{1}{\delta}\left[\bm{y}^\text{T}\bm{\eta}-\bm{I}^\text{T}A(\bm{\eta})\right]+\bm{I}^\text{T}c\left(\bm{y},\delta\right)\end{align}$$ 有梯度：$$\begin{align}&amp;\nabla\ell=\frac{\partial{\ell}}{\partial{\bm{w}}}=\frac{1}{\delta}\left[\bm{X}^\text{T}\bm{y}-\sum_{i=1}^{n}A’\cdot \bm{x}_i\right]=\frac{1}{\delta}\left[\bm{X}^\text{T}\bm{y}-\bm{X}^\text{T}\bm{\mu} \right]=\frac{1}{\delta}\bm{X}^\text{T}\left[\bm{y}-\bm{\mu} \right]\end{align}$$ 有海赛矩阵：$$\begin{align}&amp;\bm{H}\left(\ell\right)=\frac{\partial^2{\ell}}{\partial{\bm{w}\partial\bm{w}^\text{T}}}=-\frac{1}{\delta}\bm{X}^\text{T}\frac{\partial{\bm{\mu}}}{\partial{\bm{\eta}^\text{T}}}\frac{\partial{\bm{\eta}}}{\partial{\bm{w}^\text{T}}}=-\frac{1}{\delta}\bm{X}^\text{T}\bm{S}\bm{X}\\\\\end{align}$$ 其中：$\displaystyle \bm{S}=\frac{\partial{\bm{\mu}}}{\partial{\bm{\eta}^\text{T}}}=\mathrm{diag}\left[\frac{\partial \mu_i}{\partial\eta_i}\right]=\mathrm{diag}\left[\frac{\partial g^{-1}(\eta_i)}{\partial\eta_i}\right]$ $\displaystyle \frac{\partial{\bm{\eta}}}{\partial{\bm{w}^\text{T}}}=\frac{\partial \bm{X}\bm{w}}{\partial\bm{w}^\text{T}}=\bm{X} $ 3.3、牛顿-拉弗迭代法。为了求解模型，现在我们开考虑一下求极值问题,我们要求这样一个数量函数的极值：$$\begin{align}\max_{\bm{x}} f\left(\bm{x}\right)\end{align}$$ 我们有梯度： $\displaystyle \nabla f=\frac{\partial f}{\partial\bm{x}}=0$ 同时有海赛矩阵 $\displaystyle \bm{H}\left(f\right)=\frac{\partial f}{\partial\bm{x}\bm{x}^\text{T}}$。 于是我们有梯度的泰勒展开：$$\begin{align}\nabla\left(\bm{x}_t\right)=\nabla\left(\bm{x}_t\right)+\bm{H}\left(\bm{x}_t\right)\left[\bm{x}_{t+1}-\bm{x}_t\right]+\bm{r}\left(\bm{x}_t\right)=0\end{align}$$ 忽略余项，可以求得： $\displaystyle \bm{x}_{t+1}=\bm{x}_{t}-\bm{H}^{-1}\left(\bm{x}_t\right)\nabla \left(\bm{x}_t\right)$简写为：$$\begin{align}\bm{x}:=\bm{x}-\bm{H}^{-1}\nabla\end{align}$$ $\displaystyle \\\\$ 3.4、极大似然分析$$\begin{align}\bm{w}_{t+1}&amp;=\bm{w}_t -\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\left[\bm{y}-\bm{\mu}_t\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\bm{w}_t-\bm{X}^\text{T}\left[\bm{y}-\bm{\mu}_t\right]\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\left[\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\bm{\zeta}_t\end{align}$$其中：$\displaystyle \bm{\eta}_t=\bm{X}\bm{w}_t$$\displaystyle \bm{\mu}_t=g^{-1}\left(\bm{\eta}_t\right)$$\displaystyle \bm{S}_t=\frac{\partial\bm{\mu}_t}{\partial\bm{\eta}_t^\text{T}}=\mathrm{diag}\left[\frac{\partial g^{-1}(\eta_i^t)}{\partial\eta_i^t}\right]$$\displaystyle \bm{\zeta}_t=\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]$ 现在我们加以总结，历史上我们称这种算法为：迭代加权最小二乘法（IRLS） 算法：迭代加权最小二乘法（Iteratively reweighted least squares） 1 $\displaystyle \bm{w}_0=g(\bar{y})$2 $\displaystyle \text{while }\bm{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\bm{\eta}_t=\bm{X}\bm{w}_t\\\\\displaystyle\bm{\mu}_t=g^{-1}\left(\bm{\eta}_t\right)\\\\\displaystyle\bm{S}_t=\frac{\partial\bm{\mu}_t}{\partial\bm{\eta}_t^\text{T}}\\\\\displaystyle\bm{\zeta}_t=\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]\\\\\displaystyle\bm{w}_{t+1}=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\bm{\zeta}_t\end{array}\\\\$3 #end while 如果我们扩展这个推导来处理非规范连接函数，我们就会发现海赛矩阵有另一个术语。然而结果表明海赛矩阵期望与公式相同；使用海赛矩阵期望(称为 $\displaystyle \textit{Fisher} $信息矩阵)来替代实际的海赛矩阵，称为 $\displaystyle \textit{Fisher}$评分方法。 在此基础上，我们可以简单地将上述过程修改为高斯先验的 $\displaystyle \textit{MAP} $估计，即我们只需修改目标、梯度和海赛矩阵，就像我们对逻辑斯蒂回归增加 $\displaystyle \ell_2$正则一样。 4、评述使用广义线性模型我们解决了指数族分布的通用贝叶斯模型。这里我们跳过了多元高斯分布的模型。这涉及到高维，和多元统计的wishart分布。需要更为高级的工具： 1、格拉斯曼代数2、微分形式3、外微分 稍后，我们将一一提及它们。继续我们的星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/c7d551950374c4114400ca0e635169ba/如果您需要引用本文，请参考：引线小白. (Mar. 9, 2017). 《广义线性模型》[Blog post]. Retrieved from https://www.limoncc.com/post/c7d551950374c4114400ca0e635169ba@online{limoncc-c7d551950374c4114400ca0e635169ba,title={广义线性模型},author={引线小白},year={2017},month={Mar},date={9},url={\url{https://www.limoncc.com/post/c7d551950374c4114400ca0e635169ba}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>广义线性模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[狄利克雷-多项式模型(Dirichlet-Multionmial Model)]]></title>
    <url>%2Fpost%2F5afe722cdba3caacf42894f5bfd847d4%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/5afe722cdba3caacf42894f5bfd847d4/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 简介: 本文总结了狄利克雷-多项式模型的部分和我自己的一些体会，我们将再一次熟悉贝叶斯方法的基本概念、流程、特点。把我们思维进化到更高的维度。摘要：本文意在狄利克雷-多项式模型的问题。若有错误，请大家指正。关键词: 狄利克雷-多项式模型,分类分布,狄利克雷分布 一、狄利克雷-多项式模型(dirichlet-multionmial model)分类分布(Categorical distribution) 或者又叫1-C分布(multinoulli distribution) $$\begin{align}\bm{x}\sim \mathrm{Cat}(\bm{x}\mid\bm{\mu})\end{align}$$其中 $\displaystyle \bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\\\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1$ 在这里有必要解释一下符号问题。首先分类分布是对0-1分布的推广，这句话可能不好理解。我们可以这样思考，0-1分布是抛硬币。而分类分布类似于掷骰子。为了让符号统一我们使用小写 $\displaystyle c$表示有C个分类，例如 $\displaystyle c=6$可以类比于掷骰子。这样很多问题就好理解了。变量 $\displaystyle \bm{x}=\begin{bmatrix} x_1\\\ \vdots \\x_j\\\vdots\\x_c \end{bmatrix}$ 实例或者一个观测 $\displaystyle \bm{x}_i=\begin{bmatrix} x_{i1}\\\ \vdots \\x_{ij}\\\vdots\\x_{ic} \end{bmatrix}$ 例子： $\displaystyle \bm{x}_s=\begin{bmatrix} 0\ \\\vdots \\1\\\vdots\\0 \end{bmatrix} $ 1、分类分布(multinoulli distribution)概率质量函数：$$\begin{align}\mathrm{PMF：} \mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{i=1}^{c}\mu_i^{x_i}\end{align}$$ 其中： $\displaystyle \bm{x}=[x_1,x_2,…,x_c]^\text{T}\,,\bm{\mu}=[\mu_1,\mu_2,…,\mu_c]^\text{T}\,,\bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\,,\sum_{i=1}^{c}\mu_i=\bm{I}^\text{T}\bm{\mu}=1$这个表示方法也称为 $\displaystyle 1\,of\,c$编码方法。这个方法方便计算。 其他表示方法：$\displaystyle \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)},x\in\{1,…,c\}$,这个示性函数表示方法，有重要应用,降低表示维度，节约了有限的数学符号。 2、均值与方差我们知道向量函数微分结果： $\displaystyle \bm{f}(\bm{x})=\mathrm{e}^{\bm{A}\bm{x}} $ 有 $\displaystyle \frac{\partial\bm{f}}{\partial \bm{x}^\text{T}}=\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\bm{A}$。且有： $\displaystyle f(\bm{x})=\bm{\mu}^\text{T}\mathrm{e}^{\bm{A}\bm{x}}$，易得：$$\begin{align} &amp;\frac{\partial{f}}{\partial{\bm{x}}} =\mathrm{diag}\left[\mathrm{e}^{\bm{A}\bm{x}}\right]\bm{A}\bm{\mu}\\ &amp;\frac{\partial^2{f}}{\partial{\bm{x}}^2} =\bm{A}^\text{T}\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\mathrm{diag}[\bm{\mu}]\bm{A}\end{align}$$ 我们又知道特征函数： $\displaystyle \varphi_\bm{x}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{x}}]=\sum_{\bm{I}^\text{T}\bm{x}=1}p_j\mathrm{e}^{\mathrm{i}t x_j}=\sum_{j=1}^{c}\mu_i\mathrm{e}^{\mathrm{i}t x_j}=\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}$。于是可分析得到分类分布 $\displaystyle \bm{x}\sim \mathrm{Cat}(\bm{x}\mid \bm{\mu})$的期望与方差(协方差矩阵)： $\displaystyle \mathrm{E}[\bm{x}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}\right]\bm{X}\bm{\mu}=\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{x}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{x}]\mathrm{E}^\text{T}[\bm{x}]$$\displaystyle =\left.\bm{X}^\text{T}\mathrm{diag}[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}]\mathrm{diag}[\bm{\mu}]\bm{X}\right| _{\bm{t}=\bm{0}}-\bm{\mu}\bm{\mu}^\text{T}\\=\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}$ 其中： $\displaystyle \bm{X}=\bm{I}_{c\times c}$是 $\displaystyle c\times c$维的单位矩阵。也就说遍历了 $\displaystyle \bm{x}$所有可能取值组成的矩阵。为了表示简洁，我们在其中选用了$\displaystyle c\times c$维的单位矩阵。 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^{n}$于是有似然函数$$\begin{align}\mathrm{L}(\bm{\mu})=p(\mathcal{D}\mid\bm{\mu})&amp;=\prod_{i=1}^{n}\prod_{j=1}^{c}\mu_j^{x_j}=\prod_{j=1}^{c}\mu_j^{\sum_{i=1}^{n}x_{ij}}=\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中： $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$表示第 $\displaystyle j$个分类在 $\displaystyle n$次观测中发生了 $\displaystyle k_j$次。同时我们知道 $\displaystyle \sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1,\,\sum_{j=1}^{c}k_j=\bm{I}^\text{T}\bm{k}=n$ 4、对数似然函数$$\begin{align}\ell(\bm{\mu},\lambda)&amp;\propto\ln\prod_{j=1}^{c}\mu_j^{k_j}+\lambda(\sum_{j=1}^{c}\mu_j-1)\\&amp;=\bm{k}^\text{T}\ln\bm{\mu}+\lambda[\bm{I}^\text{T}\bm{\mu}-1]\end{align}$$ 5、求极大似然估计$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}&amp;=\frac{\bm{k}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}&amp;=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$分析可得 $\displaystyle \bm{\mu}_{MLE}=-\frac{\bm{k}}{\lambda} $ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}-1=0$得： $\displaystyle \frac{\bm{I}^\text{T}\bm{k}}{\lambda}=-1$由于 $\displaystyle \bm{I}^\text{T}\bm{k}=n$可得：$$\begin{align}\lambda=-n\end{align}$$ $$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 其中 $\displaystyle \mu_j^{MLE}=\frac{k_j}{n},\,k_j=\sum_{i=1}^{n}x_{ij}$。 6、多项式分布我们定义 $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$是分类 $\displaystyle j$在 $\displaystyle n$次观测中发生的次数。同时令 $\displaystyle \bm{k}=[k_1,…,k_j,…,k_c]^\text{T}$ 则有：$$\begin{align}\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)=\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中 $\displaystyle \sum_{j=1}^{c}k_j=n\,,\sum_{j=1}^{c}\mu_j=1$ 我们有特征函数：$\displaystyle \varphi_\bm{k}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{k}}]=\sum_{\bm{I}^\text{T}\bm{k}=n}p_i\mathrm{e}^{\mathrm{i}t k_j}=\sum_{\bm{I}^\text{T}\bm{k}=n}\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\left(\mu_j\mathrm{e}^{\mathrm{i}t}\right)^{k_j}=\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^n$。于是可得多项式分布的均值与协方差矩阵 $\displaystyle \mathrm{E}[\bm{k}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=-(\mathrm{i})^2n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}=n\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{k}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{k}]\mathrm{E}^\text{T}[\bm{k}]$$=\left.n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\mathrm{diag}[\bm{\mu}]\right| _{\bm{t}=\bm{0}}+\left.n(n-1)\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-2}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\left[\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\right]^\text{T}\right| _{\bm{t}=\bm{0}}-n^2\bm{\mu}\bm{\mu}^\text{T}$$=n\mathrm{diag}[\bm{\mu}]+n(n-1)\bm{\mu}\bm{\mu}^\text{T}-n^2\bm{\mu}\bm{\mu}^\text{T}\\=n\left[\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}\right]$ 7、共轭先验分布把 $\displaystyle \bm{\mu}$作为变量。观察似然函数，我们知道$$\begin{align}p(\bm{\mu})\propto p(\mathcal{D}\mid\bm{\mu})= \prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 我们知道狄利克雷分布 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\Gamma(a_0)}{\Gamma(a_1)\Gamma(a_2)…\Gamma(a_c)}\prod_{j=1}^{c}\mu_j^{a_j-1}$。而这正是我们需要的共轭先验，即后验与先验有相同的函数形式：$$\begin{align}\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\end{align}$$ 其中： $\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1\,,a_0=\bm{I}^\text{T}\bm{a}=a_1+…+a_c,\,a_j&gt;0$ 利用 $\displaystyle \Gamma,\,\mathrm{B}$函数性质容易知道：$\displaystyle \mathrm{E}(\bm{\mu})=\frac{\bm{a}}{\bm{I}^\text{T}\bm{a}}=\frac{\bm{a}}{a_0}$ $\displaystyle \mathrm{cov}(\bm{\mu})=\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\bm{a}-\bm{1}}{a_0-c}$ 为了方便使用，我们把它写成离散形式：$\displaystyle \mathrm{E}(\mu_j)=\frac{a_j}{a_0}$ $\displaystyle \mathrm{var}(\mu_j)=\frac{(a_0-a_j)a_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{cov}(\mu_i,\mu_j)=\frac{-a_ia_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\mu_j]=\mathop{\mathrm{argmax}}_{\mu_j}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{a_i-1}{a_0-c}$ 8、后验分布我们用似然函数乘以狄利克雷先验得到后验分布：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})\propto\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\end{align}$$ 归一化得：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})\end{align}$$ 1、在线学习我们发现狄利克雷分布也具有再生性质，和在线性学习性质，我们假设，陆续观测到两个的数据集 $\displaystyle \mathcal{D}_1\,\mathrm{D}_2$。1、当我们观察到 $\displaystyle \mathcal{D}_1$时，有$$ p(\bm{\mu}\mid\mathcal{D}_1)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_1+\bm{a}) $$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\bm{\mu}) p(\bm{\mu}\mid\mathcal{D}_1) $$易得： $$\begin{align}p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_2+\bm{k}_1+\bm{a})\end{align}$$ 2、后验分析最大后验估计$$\begin{align}\bm{\mu}_{MAP}=\mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}\end{align}$$ 极大似然估计：$$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 后验均值:$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}\end{align}$$ 后验均值是先验均值和最大似然估计的凸组合。知道 $\displaystyle \bm{\mu}=\frac{\bm{a}}{a_0}$$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}=\frac{a_0}{n+a_0}\bm{\mu}_0+\frac{n}{n+a_0}\bm{\mu}_{MLE}\end{align}$$我们发现 $\displaystyle a_0$可以理解为先验对于后验的等价样本大小。 $\displaystyle \frac{a_0}{n+a_0}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计，知道 $\displaystyle \mathrm{mode}[\bm{\mu}_0]=\frac{\bm{a}-\bm{1}}{a_0-c}$$$\begin{align}\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}=\frac{a_0-c}{n+a_0-c}\mathrm{mode}[\bm{\mu}_0]+\frac{n}{n+a_0-c}\bm{\mu}_{MLE}\end{align}$$ 3、 拉格朗日乘数法首先为了让符号有意义我们定义向量点除运算 $\displaystyle \frac{1}{\bm{a}}=1./\bm{a}=[1/a_1,…,1/a_n]^\text{T}$下面我们用拉格朗日乘数法在推理一遍，我们知道 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1 $，于是有：$$\begin{align}\ell(\bm{\mu},\lambda)=\ln \prod_{j}^{c}\mu_j^{k_j+a_j-1}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)=[\bm{k}+\bm{a}-\bm{1}]^\text{T}\ln\bm{\mu}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)\end{align}$$求解得：$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}=\frac{\bm{k}+\bm{a}-\bm{1}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$ 知道： $\displaystyle \bm{\mu}_{MAP}=-\frac{\bm{k}+\bm{a}-\bm{1}}{\lambda}$ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1$得： $\displaystyle \lambda=n+a_0-c$$$\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c} $$ 写成离散形式有$$\begin{align}\mu_j^{MAP}=\frac{k_j+a_j-1}{n+a_0-c}\end{align}$$ 4、后验协方差矩阵$$\begin{align}\mathrm{cov}(\bm{\mu})=\frac{(n+a_0)\mathrm{diag}[\bm{k}+\bm{a}]-[\bm{k}+\bm{a}][\bm{k}+\bm{a}]^\text{T}}{(n+a_0)^2(n+a_0+1)}\end{align}$$当 N足够大时： $$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降 5、后验预测分布开始分析之前，我们回忆一下B函数：$\displaystyle \mathrm{B}(\bm{a})=\int_{\bm{x}\in [0,1]^c}x_i^{a_i-1}\mathrm{d}\bm{x}\,,a_i&gt;0\,and\,\sum_{i-1}^{n}x_i=1$且有： $\displaystyle \mathrm{B}(\bm{a})=\frac{\prod_{i=1}^{n}\Gamma(a_1)\cdots \Gamma(a_n)}{\Gamma(a_0)}$。同时为了简化符号，我们令后验分布 $\displaystyle p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{a})$现在我们令下一次观测 $\displaystyle \bm{x}_{n+1}=\tilde{\bm{x}}$。我们现在想知道 $\displaystyle \tilde{\bm{x}}$各种情况下概率，以辅助决策。考虑到 $\displaystyle \tilde{\bm{x}}\in\{0,1\}^c \,,\bm{\mu}\in[0,1]^c=\mathcal{U}$。我们有：$$\begin{align}p(\tilde{\bm{x}}\mid \mathcal{D})&amp;=\int_{\mathcal{U}}p(\tilde{\bm{x}}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\mathrm{Cat}(\bm{x}\mid\bm{\mu})\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{\tilde{x}_j}\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{a_j+\tilde{x}_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma\left(\bm{I}^\text{T}[\bm{a}+\bm{x}]\right)}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma(a_0+1)}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}a_j^{\tilde{x}_j}\prod_{j=1}^{c}\Gamma(a_j)}{a_0^{\tilde{x}_j}\Gamma(a_0)}=\prod_{j=1}^{c}\frac{a_j}{a_0}^{\tilde{x}_j}=\prod_{j=1}^{c}\mathrm{E}^{\tilde{x}_j}(\bm{\mu}\mid\mathcal{D})\\&amp;=\mathrm{Cat}\left(\tilde{\bm{x}}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于 $\displaystyle plug-in\,\mathrm{E}[\mu\mid\mathcal{D}]$。我们也可以分析出类似贝塔-伯努利模型的结论。 6、多试验后验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么分类 $\displaystyle j$发生 $\displaystyle s_j$次的概率。写成向量形式 $\displaystyle \bm{s}$。于是有：$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\frac{m!}{s_1!…s_c!}\prod_{j=1}^{c}\mu_j^{s_j}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 我们称 $\displaystyle \mathrm{Dm}(\bm{s}\mid \mathcal{D},m)=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}$为狄利克雷-多项式分布(Dirichlet-multionmial distribution)。后验预测分布的均值与协方差矩阵问题 后验预测分布均值：$$\begin{align} \mathrm{E}[s_j\mid \mathcal{D},m] &amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\mathrm{E}(s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}=\int_{\mathcal{U}}m\mu_j\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=m\frac{\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+1)}=m\frac{a_j}{a_0}\end{align}$$ $\displaystyle \mathrm{E}[\bm{s}\mid \mathcal{D},m]=m\frac{\bm{a}}{a_0}$ 后验预测分布方差：$$\begin{align} \mathrm{var}[s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{E}(s_j^2)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}[m\mu_j+m(m-1)\mu_j^2]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{\Gamma(a_j+2)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{(a_j+1)a_j}{a_0(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=\frac{ma_j(a_0-a_j)}{a_0^2}\frac{a_0+m}{a_0+1}\end{align}$$ 后验预测分布协方差：$$\begin{align} \mathrm{cov}[s_i,s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{cov}(s_i,s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}[-m\mu_i\mu_j]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{\Gamma(a_i+1)\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_i)\Gamma(a_j)\Gamma(a_0+2)}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{a_ia_j}{(a_0+1)a_0}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=\frac{-ma_ia_j}{a_0^2}\frac{a_0+m}{a_0+1}\\\end{align}$$ 后验预测分布协方差矩阵：$\displaystyle \mathrm{cov}[\bm{s}\mid \mathcal{D},m]=m(m+a_0)\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ 对于 $plug−in\,\bm{\mu}_{MAP}$插值 $\displaystyle \mathrm{Mu}(\bm{s}\mid \bm{\mu}_{MAP},m)$，我们知道 $\displaystyle \bm{\mu}_{MAP}=\frac{\bm{a}-\bm{1}}{a_0-c}$其协方差矩阵为：$\displaystyle \mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]=m\left[\mathrm{diag}[\bm{\mu}_{MAP}]-\bm{\mu}_{MAP}\bm{\mu}_{MAP}^\text{T}\right]$ $\displaystyle \mathrm{var}[s_j\mid \bm{\mu}_{MAP},m]=\frac{m(a_j-1)(a_0-a_j+1-c)}{(a_0-c)^2}$ $\displaystyle \mathrm{cov}[s_i,s_j\mid \bm{\mu}_{MAP},m]=\frac{-m(a_i-1)(a_j-1)}{(a_0-1)^2}$比较大小容易证明：$$\begin{align}\mathrm{cov}[\bm{s}\mid \mathcal{D},m]\geqslant\mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]\end{align}$$所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论。 7、数据集后验预测与边缘似然函数有数据集 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，这有$$\begin{align} p(\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_t\mid\bm{\mu})p(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^t}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ $$\begin{align} p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid\mathcal{D}_t,\bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^{t+1}}\frac{1}{\mathrm{B}(\bm{k}_t+\bm{a})}\prod_{j=1}^{c}\mu_j^{k_j+a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{k}_t+\bm{a})}\end{align}$$ 所以有：$$\begin{align} p(\mathcal{D}_{t+1}\mid\mathcal{D}_t,\bm{a})=\frac{p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid\bm{a})}{ p(\mathcal{D}_t\mid\bm{a})}=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 9、评述通过狄利克雷-多项式模型，我们从离散二维拓展到了离散多维。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)}, x\in\{1,..,c\},\bm{\mu}\in[0,1]^c $ 这个分布由 $\displaystyle \bm{\mu}$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \bm{\mu}$。于是人类搞了个假设空间 $\displaystyle \mathcal{H}=\{\bm{\mu}_i\}$，为了找到上帝的那个 $\displaystyle \hat{\bm{\mu}}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。并且动用了自己的经验和感觉，人类假设 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}$。另外我们还可以结合，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}…\mathcal{D}_{\infty}$，来给假设空间的每个 $\displaystyle \bm{\mu}$更新概率。于是这个表达式横空出世 $\displaystyle p(\bm{\mu}\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid \mathcal{D}_{t})$。于是我们得到了： $$p(\bm{\mu}\mid\mathcal{D}_t)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_t+\bm{a})$$这样我们就把假设空间的 $\displaystyle \bm{\mu}$都给了个概率。这样我们就有关于 $\displaystyle \bm{\mu}$决策的信息。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$p(\tilde{x}\mid \mathcal{D})=\int_{\mathcal{U}}p(\tilde{x}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}=\mathrm{Cat}\left(\tilde{x}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)$$于是基于对假设空间再次赋概，我们对上帝有了新的认识 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{\bm{x}}$的次数是$\bm{s}$次的概率$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\frac{n!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$这样我们对 $\displaystyle \bm{s}$的可能值也赋概了。这个式子称之为狄利克雷-多项式分布。我们对上帝又有了新的认识 6、至此，由于 $\displaystyle p(\bm{\mu}\mid\mathcal{D}_t,\mathcal{D}_{t+1})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_{t+1}+\bm{k}_t+\bm{a})$。当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。因为$$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降，我们发现人类的认识 $\displaystyle \hat{\bm{\mu}}$会越来越逼近上帝的那个 $\displaystyle \bm{\mu}$概率。也就是说$$\begin{align}p(\hat{\bm{\mu}}\to \bm{\mu})\to1\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/5afe722cdba3caacf42894f5bfd847d4/如果您需要引用本文，请参考：引线小白. (Mar. 8, 2017). 《狄利克雷-多项式模型(Dirichlet-Multionmial Model)》[Blog post]. Retrieved from https://www.limoncc.com/post/5afe722cdba3caacf42894f5bfd847d4@online{limoncc-5afe722cdba3caacf42894f5bfd847d4,title={狄利克雷-多项式模型(Dirichlet-Multionmial Model)},author={引线小白},year={2017},month={Mar},date={8},url={\url{https://www.limoncc.com/post/5afe722cdba3caacf42894f5bfd847d4}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类分布</tag>
        <tag>人工智能</tag>
        <tag>狄利克雷-多项式模型</tag>
        <tag>狄利克雷分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝塔-伯努利模型(Beta-Binomial Model)]]></title>
    <url>%2Fpost%2F013798c6e4da1d7b0e91e91dd5294d91%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/013798c6e4da1d7b0e91e91dd5294d91/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝塔-伯努利模型(beta-binomial model)伯努利分布： $$\begin{align}x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1]\end{align}$$ 1、0-1分布(bernoulli distribution)概率质量函数可以表示为：$$\begin{align}\mathrm{PMF}：\mathrm{Ber}(x\mid \mu)=\mu^x(1-\mu)^{1-x}\end{align}$$其他表示方法：$\displaystyle \mathrm{Ber}(x\mid \mu)=\mu^{\mathbb{I}(x=1)}(1-\mu)^{\mathbb{I}(x=0)} $ $\displaystyle \mathrm{Ber}(x\mid \mu)=\begin{cases}\mu&amp;\text{if }x=1\\\nu=1-\mu&amp;\text{if }x=0\end{cases} $ 2、均值与方差：知道 $\displaystyle \varphi_x(t)=\mu\mathrm{e}^{\mathrm{i}t}+\nu$ $\displaystyle \mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\mu$ $\displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\mu(1-\mu)=\mu\nu$ 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$于是有似然函数 $$\begin{align}\mathrm{L}(\mu)=p(\mathcal{D}\mid\mu)=p(\boldsymbol{X}\mid\mu)=\prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 其中 $\displaystyle N_1=\sum_{i=1}^{n}\mathbb{I}(x_i=1)$， $\displaystyle N_0=\sum_{i=1}^{n}\mathbb{I}(x_i=0)$ 4、对数似然函数$$\begin{align}\mathcal{L}(\mu)&amp;=\ln p(\mathcal{D}\mid\mu)=\ln \prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\sum_{i=1}^{n}\left(x_i\ln\mu+(1-x_i)\ln(1-\mu)\right)\\&amp;=\boldsymbol{I}^\text{T}\boldsymbol{x}\ln\mu+\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})\ln(1-\mu)\end{align}$$ 5、求极大似然估计：$$ \frac{\partial{\mathcal{L}}}{\partial{\mu}}=\frac{1}{\mu}\boldsymbol{I}^\text{T}\boldsymbol{x}-\frac{1}{1-\mu}\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})=0\\\mu\boldsymbol{I}^\text{T}\boldsymbol{I}=\boldsymbol{I}^\text{T}\boldsymbol{x}$$$\begin{align}\mu_{MLE}=\frac{\boldsymbol{I}^\text{T}\boldsymbol{x}}{\boldsymbol{I}^\text{T}\boldsymbol{I}}=\frac{1}{n}\sum_{i=1}^{n}x_i=\bar{x}\end{align}$极大似然估计分析：我们知道 $\displaystyle x\in\{0,1\}$，在0-1分布的n次试验中, $\displaystyle x=1$的次数为 $\displaystyle k$。于是$$\begin{align}\mu_{MLE}=\frac{1}{n}\sum_{i=1}^{n}=\frac{k}{n}\end{align}$$现在我们假设抛硬币3次，3次都是正面朝上，那么 $\displaystyle n=k=3$，那么 $\displaystyle \mu_{MLE}=1$。这种情况下，极大似然的结果预测所有未来的观察值都是正面向上。常识告诉我们这是不合理的。事实上，这就是极大似然中过拟合现象的一个极端例子。下面我们引入 $\displaystyle \mu$的先验分布，我们会得到一个更合理的结论。 6、二项式分布(binomial distribution)我们现在把 $\displaystyle k$作为随机变量 $\displaystyle k=x_1+x_2+…+x_n$，于是我们有 $\displaystyle k\in\{N_1\}$，易知二项式分布和0-1分布的似然函数有相同的形式，是正比关系。最大化它们的 $\displaystyle \mu$都是 $\displaystyle \frac{k}{n}$。 $$\begin{align}\mathrm{L}(\mu)\propto\mathrm{Bin}(k\mid \mu,n)=\mathrm{C}_{n}^k\,\mu^k(1-\mu)^{n-k}\end{align}$$ 为了与后面符号衔接：我们令 $\displaystyle k=N_1,n=N,N_0=n-k$于是又有：$$\begin{align}p(\mathcal{D}\mid\mu)=\mathrm{L}(\mu)\propto\mathrm{Bin}(N_1\mid \mu,N_1+N_0)=\mathrm{C}_{N}^{N_1}\,\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 我们知道 $\displaystyle \mathrm{C}_{n}^{k}=\frac{n!}{(n-k)!k!}$。同时我们有离散随机变量特征函数$\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x_i}]=\sum_{i=1}^{\infty}p_i\mathrm{e}^{\mathrm{i}t x_i}$，注意虚数 $\displaystyle \mathrm{i}$和变量$\displaystyle i$的区别。我们令 $\displaystyle \nu=1-\mu$于是有：$$\begin{align}\varphi_k(t)=(\mu\mathrm{e}^{\mathrm{i}t}+\nu)^n\end{align} $$$\displaystyle \mathrm{E}[k]=\sum_{k=0}^{n}k\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=n\mu$$\displaystyle \mathrm{var}[k]=\sum_{k=0}^{n}(k-\mathrm{E}[k])^2\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[k]=n\mu(1-\mu)=n\mu\nu$ 7、共轭先验分布如果先验和似然函数有相同形式，那就非常方便：这样后验也有相同形式。这个时候我们称之为共轭先验(conjugate prior)。现在，我们把 $\displaystyle \mu$做为一个变量观察似然函数得到$$\begin{align}p(\mu)\propto\mu^{\rho_1}(1-\mu)^{\rho_2}\end{align}$$这样计算后验就很容易了： $\displaystyle p(\mu\mid\mathcal{D})\propto p(\mathcal{D}\mid \mu)p(\mu)=\mu^{N_1}(1-\mu)^{N_0}\mu^{\rho_1}(1-\mu)^{\rho_2}=\mu^{N_1+\rho_1}(1-\mu)^{N_0+\rho_2}$ 在给出先验分布之前，我们看看这个两个函数。之后我们会补充 $\displaystyle \Gamma(x),\mathrm{B}(a,b)$函数的性质的证明。$\displaystyle \Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u, x&gt;0$$\displaystyle \mathrm{B}(a,b)=\int_{0}^{1}x^{a-1}(1-x)^{b-1}\mathrm{d}x,a&gt;0,b&gt;0$$\displaystyle \Gamma(x+1)=x\Gamma(x)$$\displaystyle \Gamma(n+1)=n!$$\displaystyle \mathrm{B}(a,b)=\mathrm{B}(b,a)$$\displaystyle \mathrm{B}(a,b)=\frac{b-1}{a+b-1}\mathrm{B}(a,b-1),a&gt;0,b&gt;1$ $\displaystyle \mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ 然后我们知道贝塔分布： $\displaystyle \mathrm{Beta}(\mu\mid a,b)=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $。这正是我们要的共轭先验。 $$\begin{align}\mathrm{Beta}(\mu\mid a,b)=\frac{1}{\mathrm{B}(a,b)}\mu^{a-1}(1-\mu)^{b-1}\end{align}$$利用 $\displaystyle \Gamma$函数性质易知：$\displaystyle \mathrm{E}[\mu]=\frac{a}{a+b} $$\displaystyle \mathrm{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}$$\displaystyle \mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,\mathrm{Beta}(\mu\mid a,b)=\frac{a-1}{a+b-2},a&gt;0,b&gt;0$ 8、后验分布我们用似然函数乘以贝塔先验得到后验分布：$$\begin{align}p(\mu\mid\mathcal{D})\propto\mathrm{Bin}(N_1\mid\mu, N_1+N_0)\mathrm{Beta}(\mu \mid a,b)\end{align}$$ 归一化得：$$\begin{align}p(\mu\mid\mathcal{D})=\mathrm{Beta}(\mu\mid N_1+a,N_0+b)\end{align}$$ 1、在线学习容易证明 $\displaystyle \mathrm{Beta}$分布具有再生性质。于是我们可以假设我们有两个数据集 $\displaystyle \mathcal{D}_1,\mathcal{D}_2$。于是有：1、当我们观察到 $\displaystyle \mathrm{D}_1$时，有$$p(\mu\mid\mathcal{D}_1)=\mathrm{Beta}(\mu\mid N_1^1+a,N_0^1+b)$$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\mu) p(\mu\mid\mathcal{D}_1)$$易得：$$\begin{align}p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Beta}(\mu\mid N_1^1+N_1^2+a,N_0^1+N_0^2+b)\end{align}$$这表明 该贝叶斯推断具有在线学习的良好性质。 2、后验均值与众数最大后验估计：$$\begin{align}\mu_{MAP}=\mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,p(\mu\mid\mathcal{D})=\frac{N_1+a-1}{N+a+b-2}=\frac{k+a-1}{n+a+b-2}\end{align}$$极大似然估计： $$\begin{align} \mu_{MLE}=\frac{N_1}{N}=\frac{k}{n} \end{align}$$ 后验均值: $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{N_1+a}{N+a+b}=\frac{k+a}{n+a+b}\end{align}$$我们发现众数和均值不同。我们还发现 后验均值是先验均值和最大似然估计的凸组合。下面我们下证明这一点。 令先验均值$\displaystyle \frac{a}{a+b}=\mu_0,a+b=c$。 $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+a}{n+a+b}=\frac{c}{n+c}\mu_0+\frac{n}{n+c}\mu_{MLE}=\lambda\mu_0+(1-\lambda)\mu_{MLE}\end{align}$$ 我们发现 $\displaystyle c$可以理解为先验对于后验的等价样本大小。 $\displaystyle \lambda=\frac{c}{n+c}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计：$$\begin{align}\mu_{MAP}=\frac{c-2}{n+c-2}\mathrm{mode}[\mu_0]+\frac{n}{n+c-2}\mu_{MLE}=\eta\mathrm{mode}[\mu_0]+(1-\eta)\mu_{MLE}\end{align}$$ 3、后验方差$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]=\frac{(N_1+a)(N_0+b)}{(N_1+a+N_0+b)^2(N_1+a+N_0+b+1)}\end{align}$$当 $\displaystyle N$足够大时：$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]\approx\frac{N_1N_0}{NNN}=\frac{\mu_{MLE}(1-\mu_{MLE})}{n}\end{align}$$后验标准差 $$\begin{align}\sigma=\sqrt{\frac{\mu_{MLE}(1-\mu_{MLE})}{n}}\end{align}$$1、它随着我们数据的增加以 $\displaystyle \sqrt{\frac{1}{n}}$的速度下降。2、 $\displaystyle \mu=\mathop{\mathrm{argmax}}_{\mu}\sigma(\mu)=0.5$3、 $\displaystyle \mu=\mathop{\mathrm{argmin}}_{\mu}\sigma(\mu)=0\,or\,1$ 4、后验预测分布(posterior predictive distribution)上述，我们讨论了未知参数的推断，现在我们来讨论预测问题。在这之前：我们令后验分布 $\displaystyle p(\mu\mid\mathcal{D})=\mathrm{Beta}(a,b)$以简化符号 。考虑 $\displaystyle x_{n+1}$发生了，那么我们想预测 $\displaystyle x_{n+1}=0\,or\,1$的概率。为了简记，令 $\displaystyle \tilde{x}$为一随机变量，代表 $\displaystyle n$次观测后的值。同时考虑到 $\displaystyle \tilde{x}\in\{0,1\}$于是有：$$\begin{align}p(\tilde{x}\mid \mathcal{D})&amp;=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D})\mathrm{d}\mu\\&amp;=\int_0^1\mu^\tilde{x}(1-\mu)^{1-\tilde{x}}\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\int_0^1\mu^{a+\tilde{x}-1}(1-\mu)^{b+1-\tilde{x}-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\frac{\Gamma(a+\tilde{x})\Gamma(b+1-\tilde{x})}{\Gamma(a+b+1)}\\&amp;=\frac{a^{\tilde{x}}b^{1-\tilde{x}}}{(a+b)^{\tilde{x}}(a+b)^{1-\tilde{x}}}\\&amp;=\left(\frac{a}{a+b}\right)^{\tilde{x}}\left(\frac{b}{a+b}\right)^{1-\tilde{x}}\\&amp;=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于plug-in$\displaystyle \mathrm{E}[\mu\mid\mathcal{D}]$。这里的plug-in x是插值近似 plug-in approximation的意思。 5、过拟合与黑天鹅悖论如果我们使用 $\displaystyle plug-in\,\mu_{MLE}$：$$\begin{align}p(\tilde{x}\mid \mathcal{D})\approx\mathrm{Ber}\left(\tilde{x}\mid\mu_{MLE}\right)\end{align}$$1、当数据集较小，例如 $\displaystyle n=3,k=0,then,\mu_{MLE}=\frac{0}{3}=0$。这样我们预测 $\displaystyle \tilde{x}=0$的概率为 $\displaystyle 0$。这叫零数问题(zero count problem)，或者叫数据匮乏问题(sparse data problem)。这种问题在小样本情况，经常发生。当然有人就要说了，现在是大数据时代，干嘛关注这个问题？如果我们基于特定的目的，划分数据：例如特定的人做特别的事——个人购物推荐。这种情况下即使是在大数据时代，数据也不会很大。所以 $$即使在大数据当道的时代，贝叶斯方法依然是有用的！[Jordan2011]$$2、零数问题类似于黑天鹅事情。3、我们来用贝叶斯方法来解决这个问题： 使用均匀分布先验 $\displaystyle \mathrm{Beat}(1,1)=\mathrm{U}[0,1]$，plug-$\displaystyle in\,\mathrm{E}[\mu\mid\mathcal{D}]$给出了拉普拉斯继承规则(laplace’s rule of succession):$$\begin{align}p(\tilde{x}=1\mid\mathcal{D})=\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+1}{n+2}\end{align}$$可以理解为：在实践中，通常在数据集中加一应该是正确的做法。称之为加一平滑(add-one smoothing)4、注意到：plug-in$\displaystyle \mu_{MAP}$没有这个性质。 6、多试验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么发生 $\displaystyle \tilde{x}=1$的次数是 $\displaystyle s$次的概率。$$\begin{align}p(s\mid\mathcal{D},m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid a.b)\mathrm{d}\mu\\&amp;=\frac{\mathrm{C}_m^s}{\mathrm{B}(a.b)}\int_0^1\mu^{s+a-1}(1-\mu)^{m-s+b-1}\mathrm{d}\mu\\&amp;=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}\end{align}$$我们称 $\displaystyle \mathrm{Bb}(s\mid a,b,m)=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}$为贝塔-二项式分布(beta-binomial distribution)。易知(使用积分表达式,同时利用 $\displaystyle \mathrm{Bin}(s\mid \mu,m)$容易求得)：$\displaystyle \mathrm{E}[s\mid\mathcal{D},m]=m\frac{a}{a+b}$ $\displaystyle \mathrm{var}[s\mid\mathcal{D},m]=\frac{mab}{(a+b)^2}\frac{a+b+m}{a+b+1}$ $\displaystyle \mathrm{var}[s\mid\mu_{MAP}]=\frac{m(a-1)(b-1)}{(a+b-2)^2}$(插值求得的)于是有：$$\begin{align} \mathrm{var}[s\mid\mathcal{D},m]\geqslant\mathrm{var}[s\mid\mu_{MAP}]\end{align}$$至于证明，可以分析这个式子： $\displaystyle f(a,b)=ab \left( a+b-2 \right) ^{2} \left( a+b+m \right) - \left( a-1 \right) \left( b-1 \right) \left( a+b \right) ^{2} \left( a+b+1 \right) \geqslant 0,(a&gt;0,b&gt;0)$可以使用微积分一阶导数，二阶导数证明。所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论$$ 9、评述通过贝塔-伯努利模型，我们熟悉了贝叶斯方法的基本概念、流程、特点。一旦我们熟悉了伯努利、二项、贝塔、均匀、贝塔-伯努利分布后，很多关键的东西就可以用文字表述清楚了。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1] =\mu^x(1-\mu)^{1-x} $ 这个分布由 $\displaystyle \mu$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \mu$。于是人类搞了假设空间 $\displaystyle \mathcal{H}=\{\mu_i\}$，为了找到上帝的那个 $\displaystyle \hat{\mu}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。因为我们可以根据我们在某个时刻，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，来给假设空间的每个 $\displaystyle \mu$更新概率。于是这个表达式横空出世 $\displaystyle p(\mu\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\mu)p(\mu\mid \mathcal{D}_{t})$。于是我们得到了：$$p(\mu\mid\mathcal{D}_t)=\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b) $$这样我们就把假设空间的 $\displaystyle \mu$的都给了个概率。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$ p(\tilde{x}\mid \mathcal{D}_t)=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}_t]\right)$$于是基于对假设空间赋概，我们对未来的可能值也赋概。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{x}=1$的次数是$s$次的概率$$\begin{align}p(s\mid\mathcal{D}_t,m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu\\&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b)\mathrm{d}\mu\\&amp;=\mathrm{Bb}(s\mid N_1^t+a,N_0^t+b,m)\end{align}$$这样我们对 $\displaystyle s$的可能值也赋概了。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 6、至此，当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。 7、靠那个贝叶斯是谁，怎么总要出来嘟囔一下。决策论是啥？被冷落的贝叶斯微微一笑。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/013798c6e4da1d7b0e91e91dd5294d91/如果您需要引用本文，请参考：引线小白. (Mar. 7, 2017). 《贝塔-伯努利模型(Beta-Binomial Model)》[Blog post]. Retrieved from https://www.limoncc.com/post/013798c6e4da1d7b0e91e91dd5294d91@online{limoncc-013798c6e4da1d7b0e91e91dd5294d91,title={贝塔-伯努利模型(Beta-Binomial Model)},author={引线小白},year={2017},month={Mar},date={7},url={\url{https://www.limoncc.com/post/013798c6e4da1d7b0e91e91dd5294d91}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>贝塔-伯努利模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯统计学概论]]></title>
    <url>%2Fpost%2Fe9c2d3bda7c379c2b051cb34e04f2c3c%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/e9c2d3bda7c379c2b051cb34e04f2c3c/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝叶斯统计学框架经典统计学利用总计和样本信息来做统计分析，而贝叶斯统计学还加入了先验信息。下面我们用单参数一维随机变量加以说明： 1、记号以一维随机变量为例：频率学派中，依赖参数的概率密度(质量)函数表示为 $\displaystyle p_\beta(x) $或者 $\displaystyle p(x\,;\beta) $。表示在参数空间 $\displaystyle \mathcal{B}=\{\beta_i\} $中，不同 $\displaystyle \beta $对应不同密度概率(质量)函数。而在贝叶斯学派中，表示为 $\displaystyle p(x\mid \beta) $，代表了随机变量 $\displaystyle \beta $给定某个值时，总体 $x$的条件分布。而频率学派中不认为$\displaystyle \beta $是随机变量。它们认为上帝不玩骰子。 2、先验概率根据参数 $\displaystyle \beta $的先验信息确定先验分布$$\displaystyle p(\beta) $$ 3、样本的产生与似然函数贝叶斯观点认为：一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $产生要分两步：1、上帝从先验分布 $\displaystyle p(\beta) $中选了一个 $\displaystyle \beta_k $我们人类不知道，但是可以$设想$。2、从总体分布 $\displaystyle p(x\mid\beta_k) $产生一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $，这是具体的，我们人类能看到的。(按照频率学派观点，这里我们蕴含了 $\displaystyle x_i $是随机变量，且独立同分布的，贝叶斯认为这不需要，不过通常情况下，我们是使用IID，因为这样方便。)：$$\mathrm{L}(\beta_k)=p(\boldsymbol{x}\mid \beta_k)=\prod_{i=1}^{n}p(x_i\mid\beta_k) $$我们称 $\displaystyle \mathrm{L}(\beta_i) $为似然函数，它综合了总体信息和样本信息。 4、样本与参数的联合分布由于 $\displaystyle \beta_k $是上帝选的，我们人类$设想$的，它仍然是未知的，所以要把这个未知考虑进来，也是就 $\displaystyle \beta $的先验信息，对 $\displaystyle \beta $的一切可能加以考虑，而不仅仅是 $\displaystyle \beta_k $。这样我们人类就有了 $\displaystyle \boldsymbol{x} $和 $\displaystyle \beta $的联合分布：$$p(\boldsymbol{x},\beta)=p(\boldsymbol{x}\mid \beta)p(\beta) $$ 5、贝叶斯推断在没有样本信息时，人类只能根据先验分布 $\displaystyle p(\beta) $对 $\beta$做出推断。现在我们人类有了 $\displaystyle p(\boldsymbol{x},\beta) $，这样我们就可以做出新的推断了。1、先分解： $\displaystyle p(\boldsymbol{x},\beta)=p(\beta\mid\boldsymbol{x})p(\boldsymbol{x}) $。2、其中 $\displaystyle p(\boldsymbol{x})=\int_{\mathcal{B}}p(\boldsymbol{x},\beta)\mathrm{d}\beta=\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta$ ，它与$\beta$无关，或者说 $\displaystyle p(\boldsymbol{x}) $不含 $\displaystyle \beta $的任何信息。因此能用来推断的仅仅是条件分布：$$ p(\beta\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta)p(\beta)}{\displaystyle\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta} $$这就是贝叶斯公式的概率密度函数形式。 $\displaystyle p(\beta\mid\boldsymbol{x}) $史称后验分布，它集中了总体、样本、先验的一切信息，又排除了一切与 $\displaystyle \beta $无关的信息之后得到的结果。所以基于后验分布 $\displaystyle p(\beta\mid\boldsymbol{x}) $对 $\displaystyle \beta $进行统计推断是更为有效，也是最合理的。考虑离散情形：$$p(\beta_k\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta_k)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta_k)p(\beta_k)}{\displaystyle \sum_{\beta\in \mathcal{B}}p(\boldsymbol{x}\mid \beta)p(\beta)}$$ 自然语言表述的贝叶斯定理：$$\text{posterior}\propto\text{likeihood}\times\text{prior} $$或者说：$$p(\beta\mid\boldsymbol{x})\propto p(\boldsymbol{x}\mid \beta)p(\beta) $$ 6、贝叶斯统计分析的关键问题：1、确定先验分布： $\displaystyle p(\beta) $2、求联合分布： $\displaystyle p(\boldsymbol{x},\beta) $3、求后验分布： $\displaystyle p(\beta\mid \boldsymbol{x})$ 二、一个精彩的入门例子下面我们来通过一个入门的例子说明，贝叶斯定理是如何工作的。 [例子1.0] 为了提高相亲的成功率，小美考虑打扮一下自己，于是决定买一件羊绒大衣。预计要花费2000块。但是对相亲效果的影响，闺蜜们有2种意见： $\displaystyle \beta_1 $：相亲成功率提高到90%$\displaystyle \beta_2 $：相亲成功率提高到70% 小美当然希望 $\displaystyle \beta_1 $发生，有一个喜欢自己的男朋友，这笔花费还是值得的。根据一个好朋友的情况，先验概率：小美认为 $\displaystyle \beta_1 $的可信度只有40%， $\displaystyle \beta_2 $的可信度是60%。即： $$p(\beta_1)=0.4,\quad p(\beta_2)=0.6 $$小美不想花冤枉钱，于是她做了一个测试：把自己看中大衣，ps一下照片，给5个男性朋友看，结果：$$A: 5个男性朋友都认为小美更漂亮了 $$小美对测试很满意，于是她改变了看法，由二项分布知：$$ p(A\mid \beta_1)=0.9^5=0.590,\quad p(A\mid\beta_2)=0.7^5=0.168 $$由全概率公式 $\displaystyle p(A)=p(A\mid \beta_1)p(\beta_1)+p(A\mid \beta_2)p(\beta_2)=0.337 $。于是有后验概率 $$p(\beta_1\mid A)=\frac{p(A\mid \beta_1)p(\beta_1)}{p(A)}=0.7,\quad p(\beta_2)=\frac{p(A\mid \beta_2)p(\beta_2)}{p(A)}=0.3 $$这个概率综合了小美主观和实验的结果获得，要比小美之前认识的更有吸引力，更贴近实际。经过测试后，小美对买大衣有了兴趣，但是毕竟2000块还是很多的，于是小美再ps了一张图片，给她的男性朋友，结果如下：$$B: 10个男性朋友中，有9个都认为小美更漂亮了 $$$$ p(B\mid \beta_1)=C_{10}^{9}0.9^90.1=0.387,\quad p(B\mid\beta_2)=C_{10}^{9}0.7^90.3=0.121 $$由全概率公式 $\displaystyle p(B)=p(B\mid \beta_1)p(\beta_1)+p(B\mid \beta_2)p(\beta_2)=0.307 $。于是小美再次更新了自己的看法$$p(\beta_1\mid B)=\frac{p(B\mid \beta_1)p(\beta_1)}{p(B)}=0.883,\quad p(\beta_2)=\frac{p(B\mid \beta_2)p(\beta_2)}{p(B)}=0.117 $$小美经过两次测试，$\displaystyle \beta_1(相亲成功率提高到90\%) $的概率上升到了0.883，可以下决心买了。 三、共轭先验分布在叙述前，我们声明一下符号： $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X})=\frac{h(\boldsymbol{X},\boldsymbol{\beta})}{m(\boldsymbol{X})}=\frac{p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{\displaystyle\int_{\mathcal{B}}p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})\mathrm{d}\boldsymbol{\beta}} $ 1、共轭族定义设 $\displaystyle \boldsymbol{\beta} $是总体分布 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})$的参数向量， $\displaystyle \mathcal{F},\mathcal{P} $表示函数族。如果对任意的 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})\in\mathcal{F} $，存在先验分布函数$\displaystyle \pi(\boldsymbol{\beta})\in\mathcal{P} $，且 $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X}) \in \mathcal{P}$。就是说 $\displaystyle \mathcal{P} $是 $\displaystyle \mathcal{F} $的共轭族、称 $\displaystyle \pi(\boldsymbol{\beta}) $是共轭先验分布。 2、一维随机变量共轭先验分布例子1、方差已知下，一维高斯分布均值的先验分布是高斯分布。为了理解，我们先举一个简单的例子： $\displaystyle x\mid \mu\sim\mathcal{N}(x\mid \mu,\sigma) $，设 $\displaystyle \sigma $已知。有一组样本观测值 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $或者说有数据集 $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$。 我们现在开始分析：1、样本似然函数 $\displaystyle p(\mathcal{D}\mid\mu)=p(\boldsymbol{x}\mid\mu)=\prod_{i=1}^{n}p(x_i\mid\mu)=(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})\right]$2、取 $\displaystyle \mu\sim\mathcal{N}(\mu\mid \bar{\mu},\delta) $为先验分布，其中 $\displaystyle \bar{\mu},\delta $是已知的。接下来我们将看到它是共轭的。3、于是有联合分布：$\displaystyle p(\mathcal{D},\mu)=p(\mathcal{D}\mid \mu)p(\mu)=\frac{1}{(2\pi)^{\frac{n+1}{2}}\sigma^{-\frac{n}{2}}\delta^{-1}}\exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] $4、应用自然语言的贝叶斯定理，我们有后验分布：$$\begin{align}p(\mu\mid\mathcal{D})=\propto&amp; p(\boldsymbol{x}\mid \mu)p(\mu)\propto \exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] \\\propto&amp;\exp\left[-\frac{1}{2}\left((\frac{n}{\sigma^2}+\frac{1}{\delta^2})\mu^2-2(\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2})\mu+\frac{\boldsymbol{x}^\text{T}\boldsymbol{x}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2}\right)\right]\\\propto &amp;\exp\left[-\frac{1}{2}(A\mu^2-2B\mu+C)\right]\\\propto &amp;\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right]\end{align}$$ 其中 $\displaystyle A=\frac{n}{\sigma^2}+\frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2},B=\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2} =\frac{\bar{x}}{\bar{\sigma}^2}+\frac{\bar{\mu}}{\delta^2}$ 在这里我们一般把 $\displaystyle \exp\left[\boldsymbol{w}^\text{T}\boldsymbol{g}(\boldsymbol{x})\right] $称为正态分布的核。于是$$\displaystyle p(\mu\mid\mathcal{D})=p(\mu\mid\boldsymbol{x})=\frac{1}{(2\pi A^{-1})^{\frac{1}{2}}}\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right] $$也就是说后验分布是：$$\mu\mid\mathcal{D}\sim\mathcal{N}(\mu\mid \frac{B}{A},A^{-1}) $$ $\displaystyle \frac{B}{A}=\frac{\delta^2}{\bar{\sigma}^2+\delta^2}\bar{x}+\frac{\bar{\sigma}^2}{\bar{\sigma}^2+\delta^2}\bar{\mu}=\lambda\bar{x}+(1-\lambda)\bar{\mu} $$\displaystyle \frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2} $这就证明了：正态方差已知，它的均值的共轭先验分布是正态分布 3、若干技巧总结1、贝叶斯分析非常依赖于去求后验分布,如果按照定理，分母有一个积分，事实上它是一个数。于是我们经常应用$$\text{posterior}\propto\text{likelihood}\times\text{prior} $$这个式子分析，求得解后，在做归一化处理。就能得到posterior的表达式。2、在高斯分布下，我们经常需要配平方，以及观察随机变量的二次项(二次项的逆就是方差），一次项( 二次项的逆乘以一次项就是均值）。这是一个很重要的技巧。 四、充分统计量1、直观理解：就是不损失信息的统计量就是充分统计量。也就是说 $\displaystyle p_\beta(\boldsymbol{x}\mid T(\boldsymbol{x}))=p(\boldsymbol{x}\mid T(\boldsymbol{x})) $。 2、在这里我们只给出定理：设有样本 $\displaystyle \boldsymbol{x}=[x_1.x_2,…,x_n] $。样本密度 $\displaystyle p(x\mid \beta) $。有一个函数 $\displaystyle T: \boldsymbol{x}\mapsto \mathbb{R}$。 $\displaystyle t=T(\boldsymbol{x}) $它的密度为 $\displaystyle p(t\mid\beta) $。 $\displaystyle \mathcal{P}=\{\pi(\beta)\} $是$\beta$的某个先验分布族。如果对任意的 $\displaystyle \pi(\beta)\in\mathcal{P} $有 $$\displaystyle \pi(\beta\mid T(\boldsymbol{x}))=\pi(\beta\mid\boldsymbol{x}) $$这是 $\displaystyle T(\boldsymbol{x}) $是$\beta$的充分统计量的充要条件。 3、似然函数理解：$$\displaystyle \mathrm{L}(\beta)=p(\boldsymbol{x}\mid \beta)=h(\boldsymbol{x})g(T(\boldsymbol{x})\mid\beta)\propto g(T(\boldsymbol{x})\mid\beta) $$其中$h$与$\beta$无关，因此似然函数与$g(T(\boldsymbol{x})\mid\beta)$成比例，那么按照似然原理，有关$\beta$的推断可以有$T$给出。史称因子分解定理。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/e9c2d3bda7c379c2b051cb34e04f2c3c/如果您需要引用本文，请参考：引线小白. (Mar. 6, 2017). 《贝叶斯统计学概论》[Blog post]. Retrieved from https://www.limoncc.com/post/e9c2d3bda7c379c2b051cb34e04f2c3c@online{limoncc-e9c2d3bda7c379c2b051cb34e04f2c3c,title={贝叶斯统计学概论},author={引线小白},year={2017},month={Mar},date={6},url={\url{https://www.limoncc.com/post/e9c2d3bda7c379c2b051cb34e04f2c3c}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵高斯分布]]></title>
    <url>%2Fpost%2Fde5bb50dcf56fc4c002f955d5408509f%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/de5bb50dcf56fc4c002f955d5408509f/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Each day has enough trouble of its own.摘要：本文主要总结了矩阵高斯分布的若干基本问题，和我自己的一些体会。若有错误，请大家指正。关键词: 矩阵高斯分布,矩阵分布,统计学,概率论 一、标准矩阵高斯分布1、问题表述为了就研究数据集分布，我们将涉及：【矩阵分布问题】，当然矩阵分布是指的它所有元素的联合分布。 研究独立同分布的数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$的分布，我们将其写成数据矩阵： $\displaystyle \bm{X}=\big[\bm{x}_1,\bm{x}_2 \cdots \bm{x}_n\big]^\text{T}$。其中 $\displaystyle \bm{x}\in\mathbb{R}^k$且它的元素是相互独立的一元标准高斯分布： $\displaystyle x_i\sim\mathcal{N}(0,1)$。于是有： $$\begin{align}\mathrm{vec}\big(\bm{X}^\text{T}\big)\sim \mathcal{N}\big(\mathrm{vec}\big(\bm{0}_{n\times k}^\text{T}\big),\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 特别的我们用矩阵简洁的表示为： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$其中1、$\displaystyle \mathrm{vec}\big(\bm{X}_{n\times k}^\text{T}\big)=\big[\bm{x}_1^\text{T},\bm{x}_2^\text{T} \cdots \bm{x}_n^\text{T}\big]^\text{T}$，即 $\displaystyle \bm{X}$转置以后，按列拉成向量。 2、张量积 $\displaystyle \bm{A}\otimes\bm{B}=\big[a_{ij}\bm{B}\big]$。于是 $\displaystyle \bm{\varSigma}_{nk\times nk}=\bm{E}_n\otimes \bm{E}_k$ 3、 $\displaystyle \mathrm{tr}\big(\bm{A}^\text{T}\bm{B}\big)=\mathrm{vec}\big(\bm{A}\big)^\text{T}\mathrm{vec}\big(\bm{B}\big)$。于是 $\displaystyle \sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i=\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)=\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)$4、 $\displaystyle \mathrm{cov}\big[\mathrm{vec}\big(\bm{X}^\text{T}\big)\big]=\bm{E}_n\otimes \bm{E}_k=\bm{E}_{nk}$ 我们来简要说明一下：$$\begin{align}p\bigg(\mathrm{vec}\big(\bm{X}^\text{T}\big)\bigg)&amp;=p(\mathcal{D})=\prod_{i=1}^np(\bm{x}_i)=\prod_{i=1}^n\prod_{j=1}^kp(x_{ij})\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\bm{x}_1 ^\text{T}\bm{x}_1^\text{T}+\cdots+\bm{x}_n ^\text{T}\bm{x}_n^\text{T}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 到目前为止，遗留的问题是 $\displaystyle \bm{E}_n\otimes \bm{E}_k$这个参数做何理解。为何要写成克罗内克积的形式。 2、特征函数下面我们求上述矩阵分布的特征函数：我们定义：$\displaystyle \bm{T}=[\bm{t}_1,\bm{t}_2\cdots \bm{t}_n]^\text{T}$, 且知道 $\displaystyle \varphi_{\bm{x}_i}(\bm{t}_i)=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{E}_k\bm{t}_i\big]=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{t}_i\big]$。由独立随机变量联合分布特征函数等于这些随机变量的特征函数之积，知道$$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\varphi_{\mathrm{vec}\big(\bm{X}^\text{T}\big)}\big(\bm{T}\big)&amp;=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t})=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t}_i)\\&amp;=\exp\big[-\frac{1}{2}\big(\bm{t}_1 ^\text{T}\bm{t}_1+\bm{t}_2 ^\text{T}\bm{t}_2+\cdots+\bm{t}_n ^\text{T}\bm{t}_n\big)\big]\\&amp;=\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{T}\big)\big]\end{align}$$ 二、一般矩阵高斯分布1、分布形式现在我们开始考虑更一般的问题： $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}$，且 $\displaystyle \bm{W}=\bm{A}\bm{A}^\text{T}\,,\bm{V}=\bm{B}\bm{B}^\text{T}$，有： $$\begin{align}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\sim\mathcal{N}\big(\mathrm{vec}\big(\bm{M}^\text{T}\big),\bm{W}\otimes \bm{V}\big)\end{align}$$ $$\begin{align}\bm{Y}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\end{align}$$ 2、矩阵分布的特征函数下面，我们用特征函数来证明这一点：$$\begin{align}\varphi_{\bm{Y}}\big(\bm{T}\big)&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{vec}\big(\bm{T}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\big]\bigg]\\&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}\bm{Y}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{Y}\bm{T}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{Y}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{A}\bm{X}\bm{B}^\text{T}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{B}\bm{B}^\text{T}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big)\big]\\&amp;=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 也就是说矩阵分布： $\displaystyle \bm{X}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)$的特征函数是 $$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 3、一般矩阵高斯分布密度我们知道： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 由 $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}\to \bm{X}=\bm{A}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{B}^{-\text{T}}$、微分形式、变量代换定理有： $\displaystyle \frac{\partial \bm{X}}{\partial \bm{Y}}=\big|\,\bm{A}\,\big|^{-k}\big|\,\bm{B}^\text{T}\big|^{-n}=\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}$代入即可得到 $$\begin{align}p\big(\bm{Y}\big)=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\big)\right]\end{align}$$这样我们就得到了密度：$$\begin{align}\bm{X}&amp;\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\\&amp;=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\big)\right]\end{align}$$ 4、一般矩阵高斯分布性质1、$\displaystyle \bm{x}_{i,:}\sim \mathcal{N}(\bm{\mu}_i,w_{ii}\bm{V})$2、 $\displaystyle \bm{x}_{:,j}\sim\mathcal{N}(\bm{\mu}_j,v_{jj}\bm{W})$3、 $\displaystyle\mathrm{cov}[\bm{x}_{i,:},\bm{x}_{j,:}]=w_{ij}\bm{V}$4、 $\displaystyle\mathrm{cov}[\bm{x}_{:,i},\bm{x}_{:,j}]=v_{ij}\bm{W}$ 这个性质是显而易见的，然后如果你没发现“显然”，请仔细阅读上面的内容。要理解上述内容我们需要补充向量矩阵微分、微分形式、和变量代换定理。 三、评述充分熟悉矩阵微分、微分形式(外微分)、和变量代换定理是我们把握高维世界的基本工具。多加练习，容易掌握。矩阵微分大师：许宝騄。外微分大师：陈省生。可以读读他们的书。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/de5bb50dcf56fc4c002f955d5408509f/如果您需要引用本文，请参考：引线小白. (Jan. 11, 2017). 《矩阵高斯分布》[Blog post]. Retrieved from https://www.limoncc.com/post/de5bb50dcf56fc4c002f955d5408509f@online{limoncc-de5bb50dcf56fc4c002f955d5408509f,title={矩阵高斯分布},author={引线小白},year={2017},month={Jan},date={11},url={\url{https://www.limoncc.com/post/de5bb50dcf56fc4c002f955d5408509f}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>机器学习</tag>
        <tag>矩阵高斯分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布的熵]]></title>
    <url>%2Fpost%2F14200aef4e9c3ca0ec1cd56bf3b3a039%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/14200aef4e9c3ca0ec1cd56bf3b3a039/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、若干引理1、引理1.01、连续随机向量函数考虑一般情况，我们有随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$。现在有函数 $\displaystyle \bm{y}=\bm{g}(\bm{x}):\mathbb{R}^k\mapsto\mathbb{R}^d$。即有：$$\begin{align}\bm{y}=\bm{g}(\bm{x})\end{align}$$若上述方程有唯一解：$$\begin{align}\bm{x}=\bm{h}(\bm{y})\end{align}$$则称函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$是 $\displaystyle \bm{y}=\bm{g}(\bm{x})$的反函数。同时我们有雅可比行列式：$$\begin{align}\bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]\end{align}$$ 2、变量代换引理【定理1.0】对于连续随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$，函数 $\displaystyle \bm{y}=\bm{g}(\bm{x})$满足下列条件：1、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$有唯一反函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$2、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$和 $\displaystyle \bm{x}=\bm{h}(\bm{y})$连续3、 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]$存在而且连续那么$$\begin{align}f(\bm{y})=\left\{\begin{array}{l}f_\bm{x}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|&amp;\text{ 若 }\bm{y}\in G \\\ 0 &amp;\text{ 若 }\bm{y}\notin G \end{array}\right.\end{align}$$其中 $\displaystyle G=\{\bm{y}\mid \bm{y}=\bm{g(\bm{x})},\,\bm{x}\in \mathbb{R}^k\}$。 $\displaystyle \left|\,\bm{J}\right|$是雅可比行列式的绝对值，请勿与行列式符号混淆 证明： 为了区分随机变量与随机变量实例，我们定义 $\displaystyle \bm{\xi}$是随机向量，而 $\displaystyle \bm{x}$是随机向量$\displaystyle \bm{\xi}$的实例；$\displaystyle \bm{\eta}$是随机向量，而 $\displaystyle \bm{y}$是随机向量$\displaystyle \bm{\eta}$的实例当 $\displaystyle \bm{y}\notin G$时， 显然有 $\displaystyle f_\bm{\eta}(\bm{y})=0$。当 $\displaystyle \bm{y}\in G$时，有：$$\begin{align}F_ \bm{\eta}(\bm{y})=P(\bm{\eta}\leqslant \bm{y})=\int_A f_ \bm{\xi}(\bm{x})\mathrm{d}\bm{x}\end{align}$$其中： $\displaystyle A=\bigcap_{j=1}^{d}\{\bm{x}\mid g_j(\bm{x})\leqslant y_j\}$，在上式中换元： $\displaystyle \bm{x}=\bm{h}(\bm{y})$得：$$\begin{align}F_ \bm{\eta}(\bm{y})=\int_C\mathbb{I}_G(\bm{y})\times f_ \bm{\xi}\left[\bm{h}(\bm{y})\right]\cdot\left|\bm{J}\right|\mathrm{d}\bm{y}\end{align}$$其中 $\displaystyle C=\prod_{j=1}^{d}(-\infty,y_j]$， $\displaystyle \mathbb{I}_G(\bm{y})$是 $\displaystyle G$的示性函数。由此当 $\displaystyle \bm{y}\in G$时：$$\begin{align}f_ \bm{\eta}(\bm{y})=f_\bm{\xi}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|\end{align}$$证毕。其中证明中最关键的地方在于： $\displaystyle A\to C$的转变中，函数增减涉及积分方向的问题。这一问题的清晰说明较为繁琐，可以参考《数学分析原理》229页定理10.9以及微分形式的积分。 2、引理2.0定义方阵的幂(可以是分数) $\displaystyle \bm{A}^n=\bm{U}\bm{\Lambda}^n\bm{U}^\text{T}$。其中 $\displaystyle \bm{A}=\bm{U}\bm{\Lambda}\bm{U}^\text{T}$是约当分解或者叫谱分解，简单说就是对角化。 1、马哈拉诺比斯变换引理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{I}_k\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。当然我们也可以通过特征函数的方法对马哈拉诺比斯变换引理加以证明。 二、熵对于连续随机变量有： $\displaystyle \mathrm{H}[\bm{x}]=\mathrm{E}[\mathrm{I}(\bm{x})]=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}$下面我们推导多元高斯分布的熵：$$\begin{align}\mathrm{H}[\bm{x}]&amp;=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x})\ln \left[(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\right]\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x}) \left[\ln \left((2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\right)-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{x}) \left[(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{y})\times\bm{y}^\text{T}\bm{y}\mathrm{d}\bm{y}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\sum_{i=1}^k\mathrm{E}[y_i^2]\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{k}{2}\\&amp;=\ln \left[(2\pi\mathrm{e})^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right]\\&amp;=\frac{k}{2}\left(\ln2\pi+1\right)+\frac{1}{2}\ln\left|\bm{\varSigma}\right|\end{align}$$ 注意：推导中我们使用了马哈拉诺比斯变换引理。 三、评述1、在求解多元高斯分布的熵中，我们使用了变量代换，同时引用了马哈拉诺比斯变换引理。2、深层次的原理涉及到微分形式的积分。同时我们也可以浅层次的理解：使用特征函数导出马哈拉诺比斯变换引理3、好了我们不应止步，我们征途是星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/14200aef4e9c3ca0ec1cd56bf3b3a039/如果您需要引用本文，请参考：引线小白. (Jan. 10, 2017). 《多元高斯分布的熵》[Blog post]. Retrieved from https://www.limoncc.com/post/14200aef4e9c3ca0ec1cd56bf3b3a039@online{limoncc-14200aef4e9c3ca0ec1cd56bf3b3a039,title={多元高斯分布的熵},author={引线小白},year={2017},month={Jan},date={10},url={\url{https://www.limoncc.com/post/14200aef4e9c3ca0ec1cd56bf3b3a039}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布]]></title>
    <url>%2Fpost%2Ff42f3da0d7da5dc7a4ac62b0a0cdb640%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/f42f3da0d7da5dc7a4ac62b0a0cdb640/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 本文主要总结了多元高斯分布的若干基本问题，和我自己的一些体会。欢迎大家留言讨论，如有错误，请批评指正。 一、一元高斯分布$$\begin{align}x\sim\mathcal{N}(x\mid \mu,\sigma^2)=\left(2\pi\right)^{-1/2}(\sigma^2)^{-1/2}\exp\left[-\frac{1}{2}(x-\mu)^2\sigma^{-2}\right]\end{align}$$ 1、一元高斯分布特征函数我们有特征函数： $\displaystyle \varphi(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tx}p(x)\mathrm{d}x=\mathrm{e}^{i t\mu-\frac{1}{2}t^2\sigma^2}$。$$\begin{align}\varphi(t)=\exp \left[\mathrm{i} t\mu-\frac{1}{2}t^2\sigma^2\right]\end{align}$$下面我们来证明这一点： 令 $\displaystyle z=\frac {x-\mu}{\sigma}$,于是有 $\displaystyle z\sim\mathcal{N}\left(z\mid 0,1\right)=\left(2\pi\right)^{-1/2}\exp \left[-\frac{1}{2}z^2\right]$：$$\begin{align}\varphi_z(t)&amp;=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\end{align}$$ 1、我们知道虚数 $\displaystyle \mathbb{z}=\mathrm{e}^{\mathrm{i}\theta}=\cos(\theta)+\mathrm{i}\sin(\theta)$, 虚数的模 $\displaystyle \left| \mathbb{z}\right|=1$。令 $\displaystyle A(t)=\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}$于是有：$$\left|\frac{\partial{A}}{\partial{t}}\right|=\left|\mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right|=\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2} $$而且有：$$\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \left|\frac{\partial{A}}{\partial{t}}\right|\mathrm{d}z=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z&lt;\infty$$由于 $\displaystyle \int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z$收敛：故由含参反常积分一致收敛的可微性质知函数 $\displaystyle \left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \frac{\partial{A}}{\partial{t}}\mathrm{d}z$关于 $\displaystyle t\in (-\infty,+\infty)$上一致收敛。所以我们可以在 $\displaystyle \varphi_z(t)$ 的积分号下求导(交换积分与求导顺序) ：$$\varphi’_z(t)=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\mathrm{d}z $$2、现在对上式进行分布积分： $\displaystyle\begin{cases} u=\mathrm{e}^{\mathrm{i}tz}\\v=-\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $，同时 $\displaystyle \begin{cases} u’=\mathrm{i}t\mathrm{e}^{\mathrm{i}tz}\\v’=z\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $ 于是有：$$\begin{align}\varphi’_z(t)&amp;=i\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} u\mathrm{d}v\\&amp;= \left.-i\left(2\pi\right)^{-1/2}\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right| _{x=0}-t\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\\&amp;=-t\varphi_z(t)\end{align}$$ 得到微分方程：$$\begin{cases}\varphi’_z(t)+t\varphi_z(t)=0\\\varphi_z(0)=1\end{cases}$$解得：$$\begin{align}\varphi_z(t)=\mathrm{e}^{-\frac{1}{2}t^2}\end{align}$$又因为： $\displaystyle x=\mu+\sigma z$$$\begin{align}\varphi_x(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\mu+\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\varphi_z(\sigma t)=\mathrm{e}^{\mathrm{i}t\mu-\frac{1}{2}t^2\sigma^2}\end{align}$$ 二、多元高斯分布1、多元高斯分布我们知道一维的特征函数为：$$\begin{align}\varphi(t)=\exp \left[\mathrm{i}t\mu -\frac{1}{2}t^2\sigma^2\right]\end{align}$$ 多元情况： $$\begin{align}\bm{x}\sim\mathcal{N}(\bm{x}\mid\bm{\mu},\bm{\varSigma})=(2\pi)^{-k/2}\left|\bm{\varSigma}\right|^{-1/2}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$ 2、马哈拉诺比斯变换定理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{E}_p\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。 独立随机变量联合分布特征函数等于这些随机变量的特征函数之积。于是我们有$$\begin{align}\varphi_y(t)=\exp \left[-\frac{1}{2}t^2\right]\to\varphi_{\bm{y}}(\bm{t})=\prod_{i=1}^p\exp \left[-\frac{1}{2}t_i^2\right]=\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{t}\right]\end{align}$$ 3、多元高斯分布特征函数接着我们使用特征函数的线性变换性质有：$$\begin{align}\varphi_{\bm{x}}(\bm{t})&amp;=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{x}\right]\bigg]=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\left(\bm{\varSigma}^{1/2}\bm{y}+\bm{\mu}\right)\right]\bigg]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\varphi_{\bm{y}}\bigg(\big[\bm{\varSigma}^{1/2}\big]^\text{T}\bm{t}\bigg)=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}^{1/2}\bm{\varSigma}^{1/2}\bm{t}\right]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]\end{align}$$ 三、多元高斯分布的性质1、高斯随机向量的任意边缘依然是高斯分布有随机向量 $\displaystyle \bm{x}$是 $\displaystyle k$维的，且 $\displaystyle \bm{x}\sim \mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$ 现在我们从 $\displaystyle \{x_i\}_{i=1}^k$中任意选取 $\displaystyle p$个元素，令 $\displaystyle s:p\mapsto k$，则向量 $\displaystyle \tilde{\bm{x}}=[x_{s_1},x_{s_2}\cdots x_{s_p}]^\text{T}$仍然是高斯分布：$$\begin{align}\tilde{\bm{x}}\sim\mathcal{N}\big(\tilde{\bm{\mu}},\tilde{\bm{\varSigma}}\big)\end{align}$$其中： $\displaystyle \tilde{\bm{\mu}}=[\mu_{s_1},\mu_{s_2}\cdots \mu_{s_p}]^\text{T}$， $\displaystyle \tilde{\bm{\varSigma}}=[c_{ij}],i,j\in\{s\mid s{p}\}$即保留 $\displaystyle \bm{\varSigma}$的第 $\displaystyle s_1,s_2\cdots s_p$行和列的 $\displaystyle p$阶矩阵。 证明：我们有 $\displaystyle \bm{x}$的特征函数 $\displaystyle \varphi_{\bm{x}}(\bm{t})=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]$,我们令 $\displaystyle t_i=0,i\in \{s\mid k \lnot s(p)\}$有：$$\begin{align}\varphi_{\tilde{\bm{x}}}(\tilde{\bm{t}})=\exp \left[\mathrm{i}\tilde{\bm{t}}^\text{T}\tilde{\bm{\mu}}-\frac{1}{2}\tilde{\bm{t}}^\text{T}\tilde{\bm{\varSigma}}\tilde{\bm{t}}\right]\end{align}$$故而得证。 2、独立性与相关性等价有$\displaystyle \bm{x}_1\sim \mathcal{N}\big(\bm{\mu}_1,\bm{\varSigma}_1\big)$和 $\displaystyle \bm{x}_2\sim \mathcal{N}\big(\bm{\mu}_2,\bm{\varSigma}_2\big)$，我们有$\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0$，且有$\displaystyle \bm{x}=[\bm{x}_1,\bm{x}_2]^\text{T}$服从： $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu}, \bm{\varSigma}\big)$。其中： $\displaystyle \bm{\mu}=[\bm{\mu}_1,\bm{\mu}_2]^\text{T},\bm{\varSigma}=\begin{bmatrix} \bm{\varSigma}_1 &amp; \bm{0}\\\bm{0}&amp;\bm{\varSigma}_2 \end{bmatrix}$。 证明利用1、特征函数的唯一性定理2、独立随机变量联合分布的特征函数是它们特征函数之积。证明是显然的。$$\begin{align}\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0\end{align}$$ 3、仿射(线性)变换不变性有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$， 仿射变换 $\displaystyle \bm{y}=\bm{A}\bm{x}+\bm{b}$，则 $\displaystyle \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big) $ 证明：由特征函数的仿射变换性质有：$$\begin{align}\varphi_{\bm{y}}\big(\bm{t}\big)&amp;=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}\big]\varphi_{\bm{x}}\big(\bm{A}’\bm{t}\big)=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}+\mathrm{i}\big(\bm{A}^\text{T}\bm{t}\big)^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\big]\\&amp;=\exp\bigg[\mathrm{i}\bm{t}^\text{T}\big[\bm{A}\bm{\mu}+\bm{b}\big]–\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\bigg]\end{align}$$又由特征函数唯一性定理知： $$\begin{align} \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big)\end{align}$$【推论1】：分解：多元高斯分布随机向量都可以经过仿射变换为独立随机变量，且它们是标准高斯随机变量。【推论2】：降维：随机向量的线性组合是高斯分布 $\displaystyle \iff$随机向量服从高斯分布 推论2的性质颇为惊奇，我们来推导一下，以窥细节：证明：【充分性】若有一维高斯分布 ：$$\begin{align}y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)\end{align}$$知其特征函数为：$$\begin{align}\varphi_y(t)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$现在对其观察角度加以变换：令 $\displaystyle t=1$同时把 $\displaystyle \bm{a}$看成任意有：$$\begin{align}\varphi_{\bm{x}}\big(\bm{a}\big)=\mathrm{E}\big[\mathrm{i}\bm{a}^\text{T}\bm{x}\big]=\mathrm{E}\big[\mathrm{i}y\big]=\varphi_y\big(1\big)=\exp\big[\mathrm{i}\bm{a}^\text{T}\bm{x}-\frac{1}{2}\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$【必要性】若有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$，则 $\displaystyle y=\bm{a}^\text{T}\bm{x}$的特征函数为：$$\begin{align}\varphi_y(t)=\varphi_{\bm{x}}\big(t \bm{a}\big)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)$证毕 4、条件分布我们考虑如下分布 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_2\mid \bm{x}_1$依然是高斯分布： $$\begin{align} \bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 证明：1、为了利用独立性与相关性等价的结论，我们使用线性变换来构造两个独立的新随机变量： $$\begin{align}\bm{y}_1&amp;=\bm{x}_1\\\bm{y}_2&amp;=\bm{T}\bm{x}_1+\bm{x}_2\end{align}$$ 欲使 $\displaystyle \bm{y}_1\bot \bm{y}_2$，则：$$\begin{align}\mathrm{cov}\big[\bm{y}_1,\bm{y_2}\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_1- \mathrm{E}\big[\bm{y}_1\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\bm{x}_1+\bm{x}_2- \bm{T}\bm{\mu}_1- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)+\bm{x}_2- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_1- \bm{\mu}_1\big)^\text{T}\Big]\bm{T}^\text{T}+\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_2- \bm{\mu}_2\big)^\text{T}\Big]\\&amp;=\bm{\varSigma}_{11}\bm{T}^\text{T}+\bm{\varSigma}_{12}=\bm{0}\end{align}$$于是有：$$\begin{align}\bm{T}=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\end{align}$$也就是说：$$\begin{align}\bm{y}_2=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1+\bm{x}_2\end{align}$$这个线性变换是：$$\begin{align}\begin{bmatrix} \bm{y}_1 \\\ \bm{y}_2 \end{bmatrix}=\begin{bmatrix} \bm{I} &amp; \bm{0}\\ -\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}&amp;\bm{I}\end{bmatrix}\begin{bmatrix} \bm{x}_1 \\\\\bm{x}_2\end{bmatrix}\end{align}$$2、根据数字特征的性质，我们求出 $\displaystyle \bm{y}_1,\bm{y}_2$的期望和方差，由高斯分布的性质知道经过线性变换后的它们也是服从高斯分布的，从而我们可以确定其分布。容易知道：$\displaystyle \mathrm{E}\big[\bm{y}_1\big]=\bm{\mu}_2$，$\displaystyle \mathrm{cov}\big[\bm{y}_1\big]=\bm{\varSigma}_{11}$，$\displaystyle \mathrm{E}\big[\bm{y}_2\big]=\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1$。下面关键是求：$$\begin{align} \mathrm{cov}\big[\bm{y}_2\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)^\text{T}\Bigg]\\&amp;=\bm{\varSigma}_{22}+\bm{\varSigma}_{21}\bm{T}^\text{T}+\bm{T}\bm{\varSigma}_{12}+\bm{T}\bm{\varSigma}_{11}\bm{T}^\text{T}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{11}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$ 于是有：$$\begin{align}\bm{y}_2\sim\mathcal{N}\bigg(\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1,\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 3、并且我们有： $\displaystyle \bm{J}\big(\bm{y}\to \bm{x}\big)=\bigg|\frac{\partial \bm{y}}{\partial \bm{x}^\text{T}}\bigg|=1$，再有 $\displaystyle \bm{y}_1,\bm{y}_2$独立，于是：$$\begin{align}p\big(\bm{x}_1,\bm{x}_2\big)=p\big(\bm{y}_1,\bm{y}_2\big)\bm{J}\big(\bm{y}\to \bm{x}\big)=p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)\end{align}$$ 现在我们可以求得 $\displaystyle \bm{x}_2\mid \bm{x}_1$的分布：$$\begin{align}p\big(\bm{x}_2\mid \bm{x}_1\big)=\frac{p\big(\bm{x}_1,\bm{x}_2\big)}{p\big(\bm{x}_1\big)}=\frac{p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)}{p\big(\bm{y}_1\big)}=p\big(\bm{y}_2\big)=p_{\bm{y}_2}\big(\bm{x}_2- \bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1\big)\end{align}$$代入到密度函数，经过简单变换有：$$\begin{align}\bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$条件数学期望和协方差矩阵是：$$\begin{align}\bm{\mu}_{2\mid1}&amp;=\mathrm{E}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big]\\\bm{\varSigma}_{2\mid 1}&amp;=\mathrm{cov}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$证毕 当然我们也可以通过概率归一约束，和配平方加以推导。 5、高斯线性模型若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，则有：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}^{-1}=\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}$ $\displaystyle \bm{\mu}_{\bm{x}\mid \bm{y}}=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 证明：要想彻底说明这个问题，我们需要分块矩阵的若干引理：首先我们来证明一个引理 1、【引理1】单位矩阵引理：$$\begin{align} [\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$证明：$$\begin{align}&amp;[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E} \\&amp;\iff\bm{S}[\bm{E}-\bm{S}]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\big[\bm{S}^{-1}-\bm{S}\bm{S}^{-1}\big]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\bm{S}^{-1}[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}^{-1}\bm{S}\\&amp;\iff[\bm{S}^{-1}-\bm{E}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\\&amp;\iff\bm{E}=\bm{E}\end{align}$$证毕 2、【引理2】分块矩阵的逆证明：若分块矩阵的逆矩阵存在，：$$\begin{align}\bm{H}^{-1}=\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}\end{align}$$ 1、 【$\displaystyle \bm{A}$若可以逆】首先我们左乘一个矩阵，消除 $\displaystyle \bm{B} $ $$\begin{align} \left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{C} &amp; \bm{D}\end{array}\right]\end{align}$$ 第二步我们右乘一个矩阵，消除 $\displaystyle \bm{C} $ $$\begin{align}\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E}\end{bmatrix}= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{0} &amp; \bm{D}\end{array}\right]\end{align}$$ 简写$\displaystyle \bm{U}\bm{H}\bm{V}=\bm{W} $，于是 $\displaystyle\bm{H}^{-1}=\bm{V}\bm{W}^{-1}\bm{U} $（这是显然的） $$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}=\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E} \end{bmatrix}\left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} &amp; \bm{0} \\\bm{0} &amp; \bm{D}^{-1}\end{array}\right]\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\end{align}$$简化得：$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp;-[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}\bm{B}\bm{D}^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{M}&amp;- \bm{M} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}\bm{M}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}\bm{M}\bm{B}\bm{D}^{-1}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}$。 2、【同理若 $\displaystyle \bm{B}$可逆】$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1}&amp;-\bm{B}\bm{D}^{-1}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\\-[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}^{-1}\bm{C}&amp; [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{A}^{-1}+\bm{A}^{-1}\bm{B}\bm{M}\bm{C}\bm{A}^{-1}&amp;- \bm{B}\bm{D}^{-1}\bm{M}\\-\bm{M} \bm{D}^{-1}\bm{C}&amp;\bm{M}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}= [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}$ 3、【引理3】分块矩阵行列式：由引理2，消除 $\displaystyle \bm{B}$后的结论知道：$$\begin{align}\det \begin{bmatrix} \bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{bmatrix}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det \begin{bmatrix} \bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]\det[\bm{D}]\end{align}$$ 我们也可以这样（也就是说右乘矩阵消除 $\displaystyle \bm{B}$）： $$\begin{align}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}\det \begin{bmatrix} \bm{E} &amp; -\bm{A}^{-1}\bm{B} \\\bm{0} &amp; \bm{E}\end{bmatrix}=\det \begin{bmatrix} \bm{A} &amp; \bm{0}\\\bm{C}&amp;\bm{D}-\bm{C}\bm{A}^{-1}\bm{B} \end{bmatrix}=\det[\bm{A}]\det[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]\end{align}$$ 也就是说：$$\begin{align}\begin{vmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D}\end{vmatrix}=\big|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}\big|\big|\bm{D}\big|=\big|\bm{A}\big|\big|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\big|\end{align}$$ 我们也有$$\begin{align} \left|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} \right|=\left|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\right|\left|\bm{D}^{-1}\right|\big|\bm{A}\big|\end{align}$$ 4、【引理4】维度变换$\displaystyle [\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}$ $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} =\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1} $ $\displaystyle [\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}=\bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1} $证明：我们挑一个做说明,为了应用【引理1】我们右乘 $\displaystyle \bm{A}$于是有：$$\begin{align} &amp;[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\\ &amp;\iff[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}\bm{A}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\bm{A}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-\bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-[\bm{D}^{-1}\bm{C}^{-1}\bm{B}^{-1}\bm{A}+\bm{E}]^{-1}\\ &amp;\iff\big[\bm{E}-[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]\big]^{-1}-\big[[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}-\bm{E}\big]=\bm{E}\,,\bm{S}=-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}\\ &amp;\iff[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$当然我们也可以直接对比引理2的两个结论得出。证毕 5、【引理5】高斯线性回归模型的联合分布我们知道 $\displaystyle p(\bm{x},\bm{y})=p(\bm{x})p(\bm{y}\mid \bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，现在令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 我们取对数：$$\begin{align}&amp;\ln p(\bm{z})=\ln p(\bm{x})+\ln p(\bm{y}\mid \bm{x})\\&amp;=-\frac{1}{2}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)^\mathrm{T}\bm{\varSigma}_{\bm{x}}^{-1}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)-\frac{1}{2}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)^\mathrm{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)+\\&amp;\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1} \end{bmatrix}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}^{-1}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=\ln\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中我们使用了一些二次型的技巧：简要说明一下，以免显得突兀,其中 $\displaystyle \bm{S}$是对称矩阵,有二次型：$$\begin{align}Q&amp;=\frac{1}{2}(\bm{x}-\bm{\mu})^\text{T}\bm{S}(\bm{x}-\bm{\mu})=\frac{1}{2}\bm{x}^\text{T}\bm{S}\bm{x}-\bm{x}^\text{T}\bm{S}\bm{\mu}+\frac{1}{2}\bm{\mu}^\text{T}\bm{S}\bm{\mu}\\&amp;=\frac{1}{2}\bm{x}^\text{T}\bm{A}\bm{x}-\bm{x}^\text{T}\bm{B}+\frac{1}{2}\bm{\mu}^\text{T}\bm{A}\bm{\mu}\end{align}$$其中：$$\begin{align}\bm{S}&amp;=\bm{A}\\\bm{\mu}&amp;=\bm{A}^{-1}\bm{B}\end{align}$$也就是说我们可以通过观测一次项、二次项的系数来求得参数。 考察二次项：$$\begin{align}&amp;-\frac{1}{2}\bm{x}^\text{T}[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}]\bm{x}-\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}+\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}\bm{x}+\frac{1}{2}\bm{x}^\text{T}\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}\\&amp;=-\frac{1}{2}\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{cc}\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\end{array}\right]\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]\end{align}$$考虑一次项$$\begin{align}\bm{x}^\text{T}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}=\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{c}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}-\bm{A}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\\\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\end{array}\right]\end{align}$$这样再通过分块矩阵求逆，和二次型的特点可以求得：$$\begin{align}\bm{\mu}_{\bm{x}}&amp;=\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\\\bm{\varSigma}_{\bm{z}}&amp;=\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\end{align}$$同时我们通过分块矩阵行列式注意到：$$\begin{align}\big|\bm{\varSigma}_{\bm{z}}\big|=\big|\bm{\varSigma}_{\bm{x}}\big|\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|\end{align}$$以上就是联合分布是高斯分布的推导细节。 【高斯线性回归模型的联合分布】若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 则：$$\begin{align}\bm{z}\sim\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中：$\displaystyle \bm{\varLambda}_{\bm{z}}=\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1} \end{bmatrix}$###### 6、【引理6】精度矩阵与协方差矩阵再看出显然之前我们有必要叙述一精度矩阵的概念: 精度矩阵是协方差矩阵的逆$$\begin{align}\bm{\varLambda}=\bm{\varSigma}^{-1}=\begin{bmatrix} \bm{\varLambda}_{11} &amp; \bm{\varLambda}_{12} \\\bm{\varLambda}_{21}&amp;\bm{\varLambda}_{22}\end{bmatrix}\end{align}$$若 $\displaystyle \bm{\varSigma}_{11},\bm{\varSigma}_{22}$可以逆，根据上述引理我们容易知道：$\displaystyle \bm{\varLambda}_{11}=\big[\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\big]^{-1}$$\displaystyle \bm{\varLambda}_{22}=\big[\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\big]^{-1}$$\displaystyle \bm{\varLambda}_{12}=-\bm{\varLambda}_{11}\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}$$\displaystyle \bm{\varLambda}_{21}=-\bm{\varLambda}_{22}\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}$有了精度矩阵，我们可以重写条件分布定理： 若 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_1\mid \bm{x}_2$依然是高斯分布： $$\begin{align} \bm{x}_1\mid \bm{x}_2\sim\mathcal{N}\bigg(\bm{\mu}_1+\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big],\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\bigg)\end{align}$$且有$$\begin{align}\bm{\mu}_{1\mid2}&amp;=\bm{\mu}_1+\bm{\varSigma}_{11}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\mu}_1- \bm{\varLambda}_{11}^{-1}\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\varLambda}_{11}^{-1}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\\&amp;=\bm{\varSigma}_{1\mid 2}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\end{align}$$ $$\begin{align}\bm{\varSigma}_{1\mid 2}=\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}=\bm{\varLambda}_{11}^{-1}\end{align}$$ 7、最后的战斗有了联合分布我们就可以用应用条件分布定理了：结论是显然的：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}=\big[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}\big]^{-1}=\bm{\varLambda}_{xx}^{-1}$ $$\begin{align}\bm{\mu}_{\bm{x}\mid \bm{y}}&amp;=\bm{\varSigma}_{\bm{x}\mid\bm{y}}\bigg[\bm{\varLambda}_{\bm{x}\bm{x}}\bm{\mu}_{\bm{x}}-\bm{\varLambda}_{\bm{x}\bm{y}}\big[\bm{y}-\bm{A}\bm{\mu}_{x}-\bm{b}\big]\bigg]\\&amp;=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]\end{align}$$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 四、评述1、我们来总结一下：$$\begin{align}\begin{cases}\text{马哈拉诺比斯变换定理}\\\text{一元高斯分布特征函数}\\\text{特征函数性质}\end{cases}\Rightarrow\text{多元高斯分布特征函数}\end{align}$$2、当然也可以使用定义与变量代换定理一步到位。3、有了特征函数，我们就可以证明一系列关键定理4、我们讨论一下逆矩阵计算的问题，定义精度矩阵，重写了条件分布定理5、然后我们显然得出了高斯线性模型的结果。6、当然高斯模型还有非常多性质，我们将继续踏上征途。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/f42f3da0d7da5dc7a4ac62b0a0cdb640/如果您需要引用本文，请参考：引线小白. (Jan. 9, 2017). 《多元高斯分布》[Blog post]. Retrieved from https://www.limoncc.com/post/f42f3da0d7da5dc7a4ac62b0a0cdb640@online{limoncc-f42f3da0d7da5dc7a4ac62b0a0cdb640,title={多元高斯分布},author={引线小白},year={2017},month={Jan},date={9},url={\url{https://www.limoncc.com/post/f42f3da0d7da5dc7a4ac62b0a0cdb640}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习概论]]></title>
    <url>%2Fpost%2F72a5edcac04cd1a3e93a37ee90ac9259%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/72a5edcac04cd1a3e93a37ee90ac9259/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、机器学习若干符号解释我们在表达概念时，通常用集合论，空间之类的术。这个时候，我们注意元素和集合的区别。而在我们表达运算时，我们通常用矩阵的概念，这个时候你要注意维度、列、行的概念。多加练习，你很快就会掌握这个表达。 1、输入空间用矩阵表示数据集$\mathcal{D} : \boldsymbol{X}=\left[\begin{array}{c}\boldsymbol{x}_{1}^\text{T}\\\boldsymbol{x}_{2}^\text{T}\\\vdots\\\boldsymbol{x}_{n}^\text{T}\end{array}\right]=\left[\begin{array}{c}\boldsymbol{x}_{1,:}^\text{T}\\\boldsymbol{x}_{2,:}^\text{T}\\\vdots\\\boldsymbol{x}_{n,:}^\text{T}\end{array}\right]=\left[\begin{array}{ccc}x_{1,1} &amp; \dots &amp; x_{1,k} \\\vdots &amp; \dots &amp; \vdots \\x_{n,1} &amp; \dots &amp; x_{n,k}\end{array}\right]$。$\displaystyle \boldsymbol{x}_i=\boldsymbol{x}_{i,:} $表示用 $\displaystyle \boldsymbol{X} $的第i行转置构造向量 $\displaystyle \underbrace {\boldsymbol{x}_i}_{k\times1} $用matlab举个例子： 1234567891011121314%矩阵与机器学习&gt;&gt; X=[12, 14, 15; 1.5, 5.4 ,6.7;20,3.4,5]X = 12.0000 14.0000 15.0000 1.5000 5.4000 6.7000 20.0000 3.4000 5.0000&gt;&gt; x1=X(1,:)'x1 = 12 14 15&gt;&gt; x1(2,1)ans = 14 所以 $\displaystyle \boldsymbol{x_1} $表示包含k个维度的一次观测(示例)。我们用集合论的方式再叙述一遍有n个样本的数据集或者样本空间 $\displaystyle X=\{\boldsymbol{x}_1,\,\boldsymbol{x}_2,\, …,\,\boldsymbol{x}_i,\,…,\,\boldsymbol{x}_n\} \subseteq \mathcal{X}^n$ ,其中 $\displaystyle \boldsymbol{x}_i $是样本点(样本)，我们把输入的所有可能取值集合 $\displaystyle \mathcal{X} $叫做输入空间,无监督学习下也可以称为样本空间。显然 $\displaystyle X\subseteq\mathcal{X}^n $。 输入空间的矩阵表示和集合表示我们需要多加熟悉、灵活运用。这是我们思考多维问题的基础。 2、输出空间集合$\displaystyle Y=\{y_1\,,y_2\,,..\,,y_i\,,…\,,y_n\} \subseteq \mathcal{Y} $，其中输出空间 $\displaystyle \mathcal{Y} $、 输入样本$\displaystyle Y $、输入样本点 $\displaystyle y_i $。 矩阵表示： $\displaystyle \boldsymbol{y}=[y_1\,,y_2\,,..\,,y_i\,,…\,,y_n]^{\text{T}} $ 在有监督学习中，我们把集合 $\displaystyle \{(\boldsymbol{x}_i,y_i)\mid 1 \leqslant i\leqslant n\}$也叫做训练集$\mathcal{D}$ ，$\displaystyle (\boldsymbol{x}_i,y_i) $表示第i个样本。 符号$\displaystyle P(y\mid \boldsymbol{x})=\mathcal{N}(y\mid \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$与$\displaystyle y\mid \boldsymbol{x}\sim\mathcal{N}( \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$是同一个意思。注意 $\displaystyle \mid $的不同意思。[^1] 统计学中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{\beta}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{\beta}) $ 机器学习中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{w}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{w}) $ 3、假设空间：1、如果真实的世界的关系是 $\displaystyle y=h(\boldsymbol{x})$， 世界充满噪声。所以 $\displaystyle y=h(\boldsymbol{x})+e$。现在我们有一个样本或者数据集 $\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$。我们想通过这个数据集 $\displaystyle \mathcal{D}$估计出 $\displaystyle f(\boldsymbol{x})$来找到 $\displaystyle h(\boldsymbol{x})$。其实我们能找到的 $\displaystyle f$有很多，现在我们把它汇集起来： $\displaystyle \mathcal{H}=\{f_i\}$。我们的模型是 $\displaystyle y=f(\boldsymbol{x})+\varepsilon$。 现在我们换一个说法:1、世界是这样的： $\displaystyle p(y=h(\boldsymbol{x})\mid\boldsymbol{x})$2、我们观察到的世界是这样的：$\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$3、我们假设世界是这样的： $\displaystyle p(y=f(\boldsymbol{x)\mid }\boldsymbol{x},\mathcal{D},M)$[^1]，其中 $\displaystyle M$是模型(算法)。于是$\displaystyle \varepsilon=y-f=y-h+h-f=y-h+h-\mathrm{E}[f]+\mathrm{E}[f]-f$$\displaystyle \mathrm{E}[\varepsilon^2]=\mathrm{Var}[e]+\left(h-\mathrm{E}[f]\right)^2+\mathrm{E}\left[\left(f-\mathrm{E}[f]\right)^2\right]$$$ 平方损失期望=噪声方差+偏误^2+模型方差$$ 4、我们还可以写成：$\displaystyle \mathcal{H} =\{f\mid p(y=f(\boldsymbol{x})\mid\boldsymbol{x}, \mathcal{D})\}=\{f(\boldsymbol{\beta})\mid p(y=f(\boldsymbol{x};\boldsymbol{\beta})\mid \boldsymbol{x},\mathcal{D};\boldsymbol{\beta}),\boldsymbol{\beta}\in \mathbb{R}^k\}$。这里的$\displaystyle \mathcal{H} $是模型 $f$的集合。 2、这里的符号有一个重要的解释：$\displaystyle y $是一个随机变量，它的取值是 $\displaystyle y=y_i $。 $\displaystyle \boldsymbol{x} $表示的是 $\displaystyle n $个 $\displaystyle k $维输入数据。也就是说 $\displaystyle \boldsymbol{x} $也是一个变量，不过是向量的形式。它的取值是 $\displaystyle \boldsymbol{x}=\boldsymbol{x}_i $。 3、换一个程序员比较好理解的说法：$\displaystyle y,\boldsymbol{x} $是一个类。而 $\displaystyle y_i,\boldsymbol{x}_i $是一个实例。所以一个实例 $\displaystyle P(y_i\mid \boldsymbol{x}_i,\mathcal{D})$，又有 $\displaystyle P(y_{n+1}\mid \boldsymbol{x}_{n+1},\mathcal{D})$是一个数或者一个概率。$\displaystyle \hat{y},\hat{y}_i $也是类和实例的区别。 输出的最佳估计： $\displaystyle \hat{y}=\hat{f}(x)=\mathop {\text{argmax}}\limits_{\hat{y}}P(y=\hat{y}\mid \boldsymbol{x},\mathcal{D})$ 4、算法空间$\displaystyle \zeta\in\mathcal{L} $，它是算法的集合。 5、参数空间$\displaystyle \boldsymbol{\beta} \in\mathbb{R}^k $。这里的元素我们将 $\displaystyle \mathbb{R}^k$的k维有序组与向量矩阵$\mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}$等同，以方便表达。 6、概念总结有了这些基本概念，我们就可以建立起机器学习的基本框架。一张图搞定: 机器学习框架 7、指示函数，或者叫示性函数$\displaystyle \mathrm{I}_x(A)=\begin{cases}1&amp;\text{if }x\in A\\0&amp;\text{if }x\notin A\end{cases}$ 8、评论这段概论，大部分是站在频率学派的角度解释的，以后我们还会用贝叶斯学派的观点。 我们注意到符号与文字的转换要非常熟练，这就像英语，如果做到同声翻译的水平，这将有利于快速理解。所以一套好的数学符号是非常关键，好数学符号令人赏心悦目。但是每个人都有不同的风格，这就有点无语，以至于不同的书，符号不一样。TMD这是英语有了方言啊。有些书上的符号真是丑的不堪入目啊。严重影响阅读学习体验。 二、回归模型1、线性回归模型：模型的一些表示方法$ y_i=\boldsymbol{x}_{i}^T\boldsymbol{\beta}+\epsilon_i=\boldsymbol{x}_{i,:\,}^T\boldsymbol{\beta}+\epsilon_i$$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $模型矩阵解释：$$\displaystyle \mathop{\boldsymbol{y}}\limits_{(n\times 1)}=\underbrace{\mathop{\boldsymbol{X}}\limits_{(n\times k)} \mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}}_{n\times k} +\mathop{\boldsymbol{\epsilon}}\limits_{(n\times 1)} $$ 2、梯度下降算法：$\boldsymbol{\beta}: =\boldsymbol{\beta}-\alpha\nabla S$梯度下降算法的本质：可以使用相图的思想加以理解。例如有关系$\displaystyle F(x,y,t)=0$。如果我们画出$$\begin{cases}\dot{x}=-3x+5y\\\dot{y}=-5x-7\sin(y)\end{cases}$$动力系统的相图。那么如果是凸函数。相图上的曲线集就会流向平衡点。如图 相图所谓梯度就是图中的箭头乘以梯度的大小。代表了该点速度最快的方向。而 $\displaystyle\alpha_k$就是给梯度加了一个控制器。所以应该能够理解梯度下降算法了：$$\boldsymbol{\beta}_{k+1}=\boldsymbol{\beta}_{k}-\alpha_k\nabla S(\boldsymbol{\beta}_k)$$所以当系统比较复杂的时候，必然就面临问题。例如这种：$$\begin{cases}\dot{x}=-x+y\\\dot{y}=xy-1\end{cases}$$这个系统就非常复杂了。初始位置不同，我们将走向完全不同的结局。相图2 3、规范方程规范方程的本质可以如下理解： 线性回归几何解释 解决学习平方误差 $\displaystyle S$的最小化问题：$$\mathop {\min }\limits_\boldsymbol{\beta}S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 现在我们用线性空间的概念来加以理解：$\displaystyle \boldsymbol{X}$张成的空间,或者说超平面 $\displaystyle span(\boldsymbol{X})=span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,j},…,\boldsymbol{x}_{:,k})$ 这里的 $\displaystyle\boldsymbol{x}_{:,j}=\left[\begin{array}{c}x_{1,j} \\x_{2,j}\\\vdots\\x_{n,j}\end{array}\right]$。如图我们很容发现要使得 $\displaystyle\boldsymbol{\epsilon} $的欧式距离最短。那么$\displaystyle\boldsymbol{\epsilon} $必然与 $\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$垂直。即有如下方程。$$\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})=\boldsymbol{0}$$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$。所以 $\displaystyle \boldsymbol{y} $的最佳估计量 $\displaystyle \hat{\boldsymbol{y} }$是 $\displaystyle \boldsymbol{y} $在$\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$空间上的投影。 注释：[^1]: 如果 $\displaystyle \zeta$表示算法,可写为$\displaystyle P(y\mid \boldsymbol{x},\mathcal{D},\zeta) $ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/72a5edcac04cd1a3e93a37ee90ac9259/如果您需要引用本文，请参考：引线小白. (Jan. 8, 2017). 《机器学习概论》[Blog post]. Retrieved from https://www.limoncc.com/post/72a5edcac04cd1a3e93a37ee90ac9259@online{limoncc-72a5edcac04cd1a3e93a37ee90ac9259,title={机器学习概论},author={引线小白},year={2017},month={Jan},date={8},url={\url{https://www.limoncc.com/post/72a5edcac04cd1a3e93a37ee90ac9259}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大分布]]></title>
    <url>%2Fpost%2Ffcad1ed76dc00166a235da24da7c2799%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/fcad1ed76dc00166a235da24da7c2799/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$分布1、定义$$\begin{align}x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0\end{align}$$ 我们有时候也这样书写： $\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\exp(-vx)$。我们称之为等待时机分布，其中 $\displaystyle v$是速度参数。 $\displaystyle n$是次数参数。随机变量的含义是：事件 $\displaystyle A$出现后，再次出现 $\displaystyle n$次需要的时间。 2、$\displaystyle \Gamma$分布的特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 我们来求 $\displaystyle \Gamma$分布的特征函数：$$\begin{align}\displaystyle \varphi(t)&amp;=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\times\mathrm{Ga}(x\mid n,v)\mathrm{d}x\\&amp;=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\mathrm{d}x\\&amp;=\int_0^{\infty}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-(vx-\mathrm{i}t x)}\mathrm{d}x\,,y=vx-\mathrm{i}t x\\&amp;=\frac{v^n}{\Gamma(n)}\int_A \left(v- \mathrm{i}t\right)^{-n}y^{n-1}\mathrm{e}^{-y}\mathrm{d}y\\&amp;=\frac{v^n}{\Gamma(n)}\frac{\Gamma(n)}{\left(v- \mathrm{i}t\right)^n}\\&amp;=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 3、$\displaystyle \Gamma$分布的数字特征期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\left(-\mathrm{i}\right)^2n \frac{v^{n+1}}{\left(v- \mathrm{i}t\right)^{n+2}}\bigg|_{t=0}=\frac{n}{v}$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\frac{n}{v^2}$ 众数$\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{Ga}(x\mid n,v)=\frac{n-1}{v}$其中：$\displaystyle \frac{\partial }{\partial x}\mathrm{Ga}(x\mid n,v)=-C\mathrm{e}^{-vx} \left( x^{n-1}v-x^{n-2}n+x^{n-2} \right)=0\to\mathrm{mode}[x] $ 4、$\displaystyle \Gamma$分布的可加性【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从$\displaystyle \Gamma$分布 $\displaystyle x_i\sim \mathrm{Ga}(x_i\mid m_i,v)$。那么$$\begin{align} \zeta=x_1+x_2+\cdots+x_n\sim\mathrm{Ga}(\zeta\mid m_1+m_2+\cdots+m_n,v)\end{align}$$ 证明：我们有 $\displaystyle \Gamma$分布的特征函数： $\displaystyle \varphi(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^n$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特征函数之积。知道： $$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^{m_1+m_2+\cdots+m_n}\end{align}$$ 于是证明了结论。 二、$\displaystyle \chi^2(x\mid n)$分布1、定义$\displaystyle \chi^2(x\mid n)$分布$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 可以看出： $\displaystyle x\sim\chi^2(x\mid n)=\mathrm{Ga}(x\mid \frac{n}{2},\frac{1}{2}) $。 【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$是相互独立的标准高斯分布，那么随机变量：$$\begin{align}\xi=\chi^2=x_1^2+x_2^2+\cdots+x_n^2\sim\chi^2(\xi\mid n)\end{align}$$ 证明： 【引理】: 若 $\displaystyle x\sim \mathcal{N}(0,1)$,则 $\displaystyle x^2\sim\chi^2(x\mid 1)$ 有$\displaystyle y=x^2$的反函数：$$x=h(y)=\begin{cases} -\sqrt{y} \,,y&lt;0 \\\ \sqrt{y}\,,0\leqslant y&lt;+\infty \end{cases}$$容易知道导数的绝对值 $\displaystyle \left|h’(y)\right|=\frac{1}{2\sqrt{y}}$。由变量代换定理有：$$\begin{align}p(y)=\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}\mathrm{e}^{-\frac{y}{2}}=\frac{2^{-\frac{1}{2}}}{\Gamma \left(\frac{1}{2}\right)}y^{\frac{1}{2}-1}\mathrm{e}^{-y/2}\end{align}$$也就是说：$$\begin{align}p(y)=\chi^2(x\mid 1)=\mathrm{Ga}(x\mid \frac{1}{2},\frac{1}{2})\end{align}$$ 然后应用 $\displaystyle \Gamma$分布的可加性容易证明定理。 2、$\displaystyle \chi^2(x\mid n)$分布的特征函数和数字特征特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(1-2 \mathrm{i}t\right)^{-n/2}\end{align}$$ 期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=-\left(\mathrm{i}\right)^2\left( 1-2\,it \right) ^{-n/2-1}n\bigg|_{t=0}=n$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=-(\mathrm{i})^2 \left( 1-2\,it \right) ^{-n/2-2} \left( n+2 \right) n-n^2=2n$ 众数 $\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{\chi}^2(x\mid n)=n-2$其中：$\displaystyle \frac{\partial }{\partial x}\chi^2(x\mid n)=1/2C\,\mathrm{e}^{-x/2}\left( x^{n/2-2}n-2\,x^{n/2-2}-x^{n/2-1} \right) =0\to\mathrm{mode}[x] $ 3、$\displaystyle \chi^2(x\mid n)$分布的可加性质【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从 $\displaystyle \chi^2$分布 $\displaystyle x_i\sim \chi^2(x_i\mid m_i)$。那么$$\begin{align} x_1+x_2+\cdots+x_n\sim\chi^2(x\mid m_1+m_2+\cdots+m_n)\end{align}$$证明：我们有 $\displaystyle \chi^2$分布的特征函数： $\displaystyle \varphi(t)=\left(1-2 \mathrm{i}t\right)^{-n/2}$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特殊函数之积。知道：$$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)= \left(1-2 \mathrm{i}t\right)^{-\left(m_1+m_2+\cdots+m_n\right)/2}\end{align}$$于是证明了结论。 三、 $\displaystyle t(x\mid n)$分布1、定义$$\begin{align}x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}\end{align}$$ 【定理】若 $\displaystyle x\sim\mathcal{N}(0,1),y\sim\chi^2(n)$,且 $\displaystyle x$和 $\displaystyle y$相互独立,那么：$$\begin{align}\tau=\dfrac{x}{\sqrt{y/n}}\sim t(\tau\mid n)\end{align}$$ 证明：知：$$\begin{align}p(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\,,-\infty&lt;x&lt;+\infty\end{align}$$ 又知：$$\begin{align}p(y)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$于是由变量代换定理有 $\displaystyle z=\sqrt{y/n}$的密度：$$\begin{align}p(z)&amp;=p(nz^2)\times\left|2nz\right|=\frac{2nz}{2^{n/2}\Gamma \left(\frac{n}{2}\right)}\left(nz^2\right)^{n/2-1}\mathrm{e}^{-(nz^2)/2}\\&amp;=\frac{\sqrt{2n}}{\Gamma \left(\frac{n}{2}\right)}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2}\end{align}$$由随机变量商的分布知：$$\begin{align}p(\tau)&amp;=\int_{-\infty}^{+\infty}p_x(\tau z)\times p_z(z)\times \left|z\right|\mathrm{d}z\\&amp;=(n\pi)^{-1/2}\frac{1}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2 \left(1+\tau^2/n\right)}\,,s=(nz^2)/2 \left(1+\tau^2/n\right)\\&amp;=(n\pi)^{-1/2}\frac{\left(1+\frac{\tau^2}{n}\right)^{-(n+1)/2}}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}s^{(n+1)/2-1}\mathrm{e}^{-s}\mathrm{d}s\\&amp;=(n\pi)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+\tau^2/n\right)^{-(n+1)/2}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-\frac{n+1}{2}}\\\end{align}$$ 即有：$$\begin{align}\tau\sim t(\tau\mid n)=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-(n+1)/2}\end{align}$$ 证毕 2、$\displaystyle t(x\mid n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]\end{align}$$ 知：$\displaystyle t(-x\mid n)=t(x\mid n)$。所以：$$\begin{align}\mathrm{E}[x^k]=0,k\in\{2a\mid a\in \mathbb{R}\}\end{align}$$ 下面我们来考虑： $\displaystyle k\in\{2a+1\mid a\in \mathbb{R}\} $$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{-\infty}^{+\infty} x^kt(x\mid n)\mathrm{d}x=\int_{-\infty}^{+\infty} x^k\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{-\infty}^{+\infty} x^k\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\,,y=1+\frac{x^2}{n}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}2\int_{1}^{+\infty}n^{\frac{k}{2}}(y-1)^{\frac{k}{2}}y^{-\frac{n+1}{2}}\frac{1}{2}n^{\frac{1}{2}}(y-1)^{\frac{1}{2}}\mathrm{d}y\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{+\infty}(y-1)^{\frac{k-1}{2}}y^{-\frac{n+1}{2}}\mathrm{d}y\,,z=\frac{1}{y}\\&amp;=-\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{0}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{-\frac{k-1}{2}}z^{\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{\frac{n-k}{2}-1}\mathrm{d}z\\&amp;=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\\\end{align}$$ 即有：$$\begin{align}\mathrm{E}[x^k]=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)},k\in\{2a+1\mid a\in \mathbb{R}\}\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=0\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n)}{n-2}\,,n&gt;2$ 四、 $\displaystyle F(x\mid m,n)$分布1、定义$$\begin{align}x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0\end{align}$$ 【定理】若有随机变量 $\displaystyle x_1\sim \chi^2(m),x_2\sim\chi^2(n)$，且 $\displaystyle x_1$和 $\displaystyle x_2$相互独立，那么：$$\begin{align}\varphi= \frac{x_1/m}{x_2/n}= \frac{\chi^2(m)/m}{\chi^2(n)/n}\sim F(\varphi\mid m,n)\end{align}$$证明：首先我们来分析一下： $\displaystyle x/n$的分布, 我们知道：$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 令 $\displaystyle x=nz$，由变量代换定理知道： $$\begin{align} z\sim \frac{2^{-\frac{n}{2}}n}{\Gamma \left(\frac{n}{2}\right)}(nz)^{n/2-1}\mathrm{e}^{-nz/2}\,,z&gt;0 \end{align}$$于是有 $\displaystyle z_1=\varphi z_2$由随机变量商的分布知：$$\begin{align}p(\varphi)&amp;=\int_{0}^{+\infty}p_{z_1}(\varphi z_2)\times p_{z_2}(z_2)\times \left|z_2\right|\mathrm{d}z_2\\&amp;=C\int_{0}^{+\infty}(\varphi z_2)^{m/2-1}\mathrm{e}^{-m\varphi z_2/2}(z_2)^{n/2-1}\mathrm{e}^{-n z_2/2}z_2\mathrm{d}z_2\\&amp;=C\varphi^{m/2-1}\int_{0}^{+\infty}z_2^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}z_2(n+m\varphi)}\mathrm{d}z_2\,v=z_2(n+m\varphi)\\&amp;=C\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\int_{0}^{+\infty}v^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}v}\mathrm{d}v\\&amp;=C\Gamma\big(\frac{m+n}{2}\big)2^{(m+n)/2}\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\,,C=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}2^{-(m+n)/2}}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\varphi^{m/2-1}\left(n+m\varphi\right)^{-(m+n)/2}\,,\varphi&gt;0\end{align}$$ 2、$\displaystyle F(x\mid m,n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{0}^{+\infty}x^kF(x\mid m,n)\mathrm{d}x\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\int_{0}^{+\infty}x^kx^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=C\int_{0}^{+\infty}x^{k+m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=Cn^{-(m+n)/2}\int_{0}^{+\infty}x^{k+m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2}\mathrm{d}x\,,y=1+\frac{m}{n}x\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{+\infty}(y-1)^{k+m/2-1}y^{-(m+n)/2}\mathrm{d}y\,z=\frac{1}{y}\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{0}(\frac{1}{z}-1)^{k+m/2-1}z^{(m+n)/2}(-z^{-2})\mathrm{d}z\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{0}^{1}(1-z)^{m/2+k-1}z^{n/2-k-1}\mathrm{d}z\\&amp;=\big(\frac{n}{m}\big)^k\frac{\mathrm{B}\left(\frac{m}{2}+k,\frac{n}{2}-k\right)}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\,,2k&lt;n\\&amp;=\big(\frac{n}{m}\big)^k\frac{\Gamma\big(\frac{m}{2}+k\big)\Gamma\big(\frac{n}{2}-k\big)}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\,,2k&lt;n\\\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=\frac{n}{n-2}\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)^2}\,,n&gt;4$ 五、评述我们通过伽马函数引出伽马分布，当然这有点突兀，怎么就突然冒出来了伽马函数？后面还有一个贝塔函数，似乎有一种神秘力量在背后。这两个函数反复出现是有原因的。有机会我们专门会抽空补充一下，让它更加自然。事实上我们可以通过指数分布引入伽马分布。 伽马分布：$\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0$ 卡方分布：$\displaystyle x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0$ t分布：$\displaystyle x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}$ F分布：$\displaystyle x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/fcad1ed76dc00166a235da24da7c2799/如果您需要引用本文，请参考：引线小白. (Jan. 8, 2017). 《三大分布》[Blog post]. Retrieved from https://www.limoncc.com/post/fcad1ed76dc00166a235da24da7c2799@online{limoncc-fcad1ed76dc00166a235da24da7c2799,title={三大分布},author={引线小白},year={2017},month={Jan},date={8},url={\url{https://www.limoncc.com/post/fcad1ed76dc00166a235da24da7c2799}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>三大分布</tag>
        <tag>伽马分布</tag>
        <tag>卡方分布</tag>
        <tag>t分布</tag>
        <tag>F分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伽马函数]]></title>
    <url>%2Fpost%2Ff86d15ef1be4f2b424d4ad1ccc917c3d%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/f86d15ef1be4f2b424d4ad1ccc917c3d/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$函数$$\begin{align}\Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\,,x&gt;0\end{align}$$ 它有如下性质：1、对于 $\displaystyle x\in(0,+\infty)$，有 $\displaystyle \Gamma(x+1)=x \Gamma(x)$成立2、 $\displaystyle \Gamma(n+1)=n!\,,n=1,2,3,…$3、 $\displaystyle \log \Gamma$在 $\displaystyle (0,+\infty)$上是凸的 1、若干引理在证明上诉性质前：我们要证明 $\displaystyle \Gamma(x)$在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数。我们先说明反常积分柯西判别法的两个引理 【引理1】设 $\displaystyle f$定义在 $\displaystyle [a,+\infty)$，在任何有限区间 $\displaystyle [a,v]$上可积，且$$\begin{align}\lim_{x\to +\infty}x^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle p&gt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^{+\infty}f(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\leqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 与无界反常积分类似，也存在类似的瑕积分的判别方法【引理2】设 $\displaystyle f$定义在 $\displaystyle(a,b]$，其中 $\displaystyle a$位瑕点，在任何 $\displaystyle [v,b]\subset(a,b]$上可积，且$$\begin{align}\lim_{x\to a^+}(x-a)^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle 0&lt;p&lt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^b\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^bf(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\geqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 于是做如下分析：$$\begin{align}\Gamma(x)=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\end{align}$$ 根据引理1、2分析 $\displaystyle \Phi(x)，\Psi(x)$。我们容易得出 $\displaystyle \Gamma(x)$函数的定义域是： $\displaystyle (0,+\infty) $ 回忆一下：含参反常积分在 $\displaystyle E$上一致收敛概念： 然后我们归纳如下引理： 魏尔斯特拉斯 $\displaystyle M$判别法，以及含参反常积分一致收敛的可微性质。 【魏尔斯特拉斯 $\displaystyle M$判别法】 设有函数 $\displaystyle g(y)$，使得：$$\begin{align}\left|f(x,y)\right|\leqslant g(y)\,,(x,y)\in E\times[c,+\infty)\end{align}$$若 $\displaystyle \int_c^{+\infty}g(y)\mathrm{d}y$收敛。则 $\displaystyle \int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛 。【含参反常积分一致收敛的连续性质】设 $\displaystyle f(x.y)$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle [a,b]\in[c,+\infty)$上连续。 【含参反常积分一致收敛的可微性质】设 $\displaystyle f(x.y)$与 $\displaystyle \frac{\partial f}{\partial x}$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上收敛， $\displaystyle \int_c^{+\infty}\frac{\partial f}{\partial x}\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle E$上可微，且：$$\begin{align}\frac{\partial F}{\partial x}=\frac{\partial}{\partial x}\int_c^{+\infty}f(x,y)\mathrm{d}y=\int_c^{+\infty}\frac{\partial }{\partial x}f(x,y)\mathrm{d}y\end{align}$$ 2、$\displaystyle \Gamma(x)$是在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数下面我们来说明 $\displaystyle \Gamma(x)$的内闭一致收敛性质，对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有 $$\begin{align}\Gamma(x)&amp;=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\\&amp;\leqslant\int_0^1u^{a-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{b-1}\mathrm{e}^{-u} \mathrm{d}u&lt;\infty\end{align}$$同时考虑到 $\displaystyle u^{x-1}\mathrm{e}^{-u}&gt;0$且是连续函数。由魏尔斯特拉斯 $\displaystyle M$判别法和含参反常积分一致收敛的连续性质知道 $\displaystyle \Gamma(x)$在 $\displaystyle [a,b]\subset (0,+\infty)$一致收敛且在第一域上连续。 同时我们考虑到：$$\begin{align}\int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln u\mathrm{d}u\end{align}$$ 对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有$$\begin{align}&amp;\int_0^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u\right|\mathrm{d}y\\&amp;\leqslant\int_0^1\left|u^{x-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u\\&amp;\leqslant\int_0^1\left|u^{a-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{b-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u&lt;\infty\\\end{align}$$故 $\displaystyle \int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u$在 $\displaystyle [a,b]\subset (0,+\infty)$上一致收敛。于是由含参反常积分一致收敛的可微性质知道 $\displaystyle \Gamma(x)$在任意$\displaystyle [a,b]\subset (0,+\infty)$上可导，也是说在在定义域 $\displaystyle (0,+\infty)$上可导。且有：$$\begin{align}\Gamma^{(n)}(x)=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln^n u\mathrm{d}u\,,x&gt;0\end{align}$$容易知道： $\displaystyle \Gamma’’(x)&gt;0$。于是就证明了： $$\begin{align}\Gamma(x)是在定义域(0,+\infty)内连续且可导的凸函数。\end{align}$$证明中，一些细节并未详细说明，但是这是简单的。所以请注意。 3、其他性质的证明$$\begin{align}\int_0^a u^{x}\mathrm{e}^{-u}\mathrm{d}u&amp;=-u^x\mathrm{e}^{-u}\big|_0^a+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\&amp;=-a^x\mathrm{e}^{-a}+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\\end{align}$$令 $\displaystyle a\to+\infty$有：$$\begin{align}\Gamma(x+1)=x\Gamma(x)\end{align}$$ 若 $\displaystyle x\in \mathbb{Z}^+$有：$$\begin{align}\Gamma(n+1)=n(n-1)\cdots 2\cdot \Gamma(1)=n!\int_0^{+\infty}\mathrm{e}^{-u} \mathrm{d}u=n!\end{align}$$ 4、 $\displaystyle \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}$证明：令 $\displaystyle A=\Gamma\big(\frac{1}{2}\big)=\int_0^\infty u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u$，于是有：$$\begin{align}A&amp;=\int_0^{+\infty} u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u\,,u=t^2\\&amp;=2\int_0^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\\&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\end{align}$$ 同时有：$$\begin{align}A^2&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-x^2}\mathrm{d}x\int_{-\infty}^{+\infty}\mathrm{e}^{-y^2}\mathrm{d}y\\&amp;=\iint_{R^2}\mathrm{e}^{-(x^2+y^2)}\mathrm{d}x \mathrm{d}y\,,x=r\cos(\theta),y=r\sin(\theta)\\&amp;=\int_{0}^{2\pi}\int_{0}^{+\infty}r \mathrm{e}^{-r^2}\mathrm{d}r \mathrm{d}\theta=\frac{1}{2}\int_{0}^{2\pi}\mathrm{d}\theta=\pi\end{align}$$ 于是有：$$\begin{align} \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/f86d15ef1be4f2b424d4ad1ccc917c3d/如果您需要引用本文，请参考：引线小白. (Jan. 7, 2017). 《伽马函数》[Blog post]. Retrieved from https://www.limoncc.com/post/f86d15ef1be4f2b424d4ad1ccc917c3d@online{limoncc-f86d15ef1be4f2b424d4ad1ccc917c3d,title={伽马函数},author={引线小白},year={2017},month={Jan},date={7},url={\url{https://www.limoncc.com/post/f86d15ef1be4f2b424d4ad1ccc917c3d}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习书单]]></title>
    <url>%2Fpost%2F7b229b54afd86ff3877ec77e611f29a6%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/7b229b54afd86ff3877ec77e611f29a6/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在列出一些书单。若有错误，请大家指正。关键词: 机器学习,,书单,学习资源 机器学习资源因为大数据的兴起，机器学习大热。这一行业吸引了大量人才涌入。然后现状是学习机器学习需要较高的数学知识，导致人才供应不足。跟不上行业发展需求。较高的工资和技术时尚吸引大量的人们争相学习机器学习、人工智能。故收集网络整理出这些书籍： 网络上建议的入门顺序：1、斯坦福大学公开课 ：机器学习课程2、李航.统计学习方法3、Machine Learning in Action(机器学习实战） 解释一下这个入门：在这之前你可以简单了解一下，周志华：数据挖掘与机器学习，，斯坦福的公开课可以带你入门。《统计学习方法》可以带你了解基本理论和推导。《机器学习实战》可以让你了解一下实操。这个之后有两个路径，看大神Bengio的《deep learning》，或者继续深化机器学习的知识。当然你也可以参加这个：数据分析竞赛kaggle 附照片&lt;img src=http://oiol5pi05.bkt.clouddn.com/Yoshua_Bengio.jpg width=20%&gt;Yoshua BengioFull ProfessorDepartment of Computer Science and Operations ResearchCanada Research Chair in Statistical Learning Algorithms 解决了机器学习入门问题，我们接下来要进阶：1、PRML（Pattern Recognition and Machine Learning）2、ESL（The Elements of Statistical Learning ）3、MLAPP(Machine Learning: a Probabilistic Perspective) 4、Deep learning-author Yoshua Bengio 上面的书单应该是经典的四本书了。个人比较偏爱《MLAPP》，符号比较优美，叙述比较全面。当然你也可以看模式识别的书，例如《统计模式识别（第3版）Statistical Pattern Reco》，那个封面是豹子的书。 &lt;img src=http://oiol5pi05.bkt.clouddn.com/%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB.jpg width=”50%”&gt; 一些理论当然看这些书的时候，你也许会发现，凸优化，图模型，EM，MCMC之类的，你可以通过下面的书深入一下：1、Convex Optimization2、Probabilistic Graphical Models3、The EM Algorithm and Extensions4、Simulation Fifth Edition Sheldon M. Ross 实战编程1、Python for Data Analysis2、SciPy and NumPy3、Machine Learning for Hackers（这本是用R的）4、集体智慧编程（这个书名有点误导人） 网上搜索很容易找到。现在（2017）又出来很多新书，也发生了很多事情。就不一一列举。 好玩的一本书1、Bad Data Handbook 最后这些书我也还没看。。。。。我会把它们看完的！！！！^_^ 1我们还是来点别的吧： 神经网络学习机器学习之前，你应该先了解、学习神经网络。我知道现在媒体关注点在深度学习，机器学习上。网络上的学习路线也多少是从机器学习开始：从线性模型到深度学习。 可以是，你要知道Hinton的深度学习是挖掘了神经网络的潜能，Hinton是要把被抛弃、被侮辱、几起几落的神经网络再度复兴。在被人摒弃的10年中，加拿大多伦多大学的Geoffery Hinton教授依然坚守。2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。 Geoffery Hinton教授&lt;img src=http://oiol5pi05.bkt.clouddn.com/Geoffrey%20Hinton.jpg width=100%&gt; 与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。 他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。所以我觉得从神经网络到深度学习或者说从神经元到深度学习是会对人工智能有更好的认识。而不是机器学习。神经网络有着传奇的历史，如果你了解它，你将为之着迷。 Andrew Ng对神经网络的看法：&lt;img src=http://oiol5pi05.bkt.clouddn.com/Andrew%20Ng%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%81%E5%BA%A6.jpg width=100%&gt; 神经网络的书籍在这里：1、人工神经网络教程 第一版、第二版 韩立群2、神经网络原理3、神经网络设计3、神经网络与机器学习4、Matlab与神经网络，这种书籍就非常多了。 下面是我口号：$$\displaystyle Life=\int_{Birth}^{Death} (Learning+Working) \mathrm{d}t$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/7b229b54afd86ff3877ec77e611f29a6/如果您需要引用本文，请参考：引线小白. (Dec. 27, 2016). 《机器学习书单》[Blog post]. Retrieved from https://www.limoncc.com/post/7b229b54afd86ff3877ec77e611f29a6@online{limoncc-7b229b54afd86ff3877ec77e611f29a6,title={机器学习书单},author={引线小白},year={2016},month={Dec},date={27},url={\url{https://www.limoncc.com/post/7b229b54afd86ff3877ec77e611f29a6}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行列式]]></title>
    <url>%2Fpost%2F3fe19b4e56c4dfbdfa57a7f8da13e428%2F</url>
    <content type="text"><![CDATA[摘要：本文意在理清行列式的基础问题。若有错误，请大家指正。关键词: 行列式,机器学习 一、定义函数集合$\displaystyle V=\{f\mid f$是矩阵$M_n(K)$上的数量函数$\}$ $\displaystyle V_1=\{f\mid f\in V,$且满足列线性性$\} $ $\displaystyle V_2=\{f\mid f\in V_1,$且满足列反对称性$\}$ $\displaystyle V_3=\{f:f\in V_2,$且满足规范性$\} $ 容易验证， $\displaystyle V,V_1,V_2$关于加法和数乘运算构成函数空间。 二、性质性质1$\displaystyle V$是 $\displaystyle K$上的 $\displaystyle n^2$元函数空间，所以有 $\displaystyle \dim(V)=\infty$ 性质2$\displaystyle \dim(V_1)=n^n$，其一组基底是 $\displaystyle f_{i_1i_2\cdots i_n}(\varepsilon_{j_1}，\varepsilon_{j_2},\cdots,\varepsilon_{j_n})=\delta_{i_1j_1}\delta_{i_2j_2}\cdots\delta_{i_nj_n}$ 性质3$\displaystyle \dim(V_2)=1，V_2=span(\det(\cdot))$，其中 $\displaystyle \det(\cdot)$为行列式函数。 性质4$\displaystyle V_3=\{\det(\cdot)\}$ 三、证明性质2的证明定义 $\displaystyle n^n$维空间 $\displaystyle L=\{(c_{i_1i_2\cdots i_n}):1\leq i_1,i_2,\cdots,i_n \leq n \}$ 定义映射空间 $\displaystyle V_1$到 $\displaystyle L$的线性映射，从而有 $$\begin{align}\dim(V_1)=\dim(L)=n^n\end{align}$$ 1.【H是单射】 若 $\displaystyle f，g \in V_1且H(f)=H(g)=(c_{i_1i_2\cdots i_n})$则有 $$\begin{align}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=c_{i_1i_2\cdots i_n}\end{align}$$又$$\begin{align}f\big(\alpha_1,\alpha_2,\cdots,\alpha_n)= f(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$ $$\begin{align}g\big(\alpha_1,\alpha_2,\cdots,\alpha_n\big)= g\big(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$所以有 $$\begin{align}f=g\end{align}$$ 2.【H是满射】 同时任意给定 $\displaystyle (c_{i_1i_2\cdots i_n})\in L$，定义函数 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}c_{i_1i_2\cdots i_n}\end{align}$$ 可以验证 $\displaystyle f \in V_1$ ，此时有$$\begin{align}H(f)=(c_{i_1i_2\cdots i_n})\end{align}$$所以 $\displaystyle H$是满射。根据同态映射 $\displaystyle H$我们不难找到 $\displaystyle V_2$的一组基底 性质3的证明若 $\displaystyle f\in V_2$，由列反对称性，有 $$\begin{align}f(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n})=\begin{cases}0 &amp;\exists s,t,\to i_s=i_t\\(-1)^{\tau(i_1i_2\cdots i_n)}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)&amp; i_1,i_2,\cdots ,i_n \textit{ pairwise unequal }\end{cases}\end{align}$$ 其中 $\displaystyle \tau(i_1i_2\cdots i_n)$为排列 $\displaystyle i_1.i_2,\cdots,i_n$的逆序数。所以我们有 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{i_1,i_2,\cdots,i_n\textit{ pairwise unequal } }(-1)^{\tau(i_1i_2\cdots i_n)}a_{i_1}a_{i_2}\cdots a_{i_n}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)\end{align}$$从而有 $$\begin{align}f(A)=f(E)\cdot \det(A)\end{align}$$ 容易验证具有该表达式的函数 $\displaystyle f$属于 $\displaystyle V_2$ 性质4的证明若 $\displaystyle f\in V_3$,则有 $$\begin{align}f(A)=f(E)\cdot \det(A)=\det(A)\end{align}$$]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学分析笔记-数的建立]]></title>
    <url>%2Fpost%2Fe16e96fd600d84099331bdee7ae39758%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/e16e96fd600d84099331bdee7ae39758/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清数的问题。若有错误，请大家指正。关键词: 有理数,实数 这是测试文章： 一、数的建立1、有理数的缝隙数的建立是数学分析的基础。实数的最小上界性质是我们开启现代数学的钥匙。 关于$p=\sqrt{2}$的重要定律： 构造一个数 $\displaystyle z=\frac{2x+2}{x+2}=x-\frac{x^2-2}{x+2}$ 同时令 $\displaystyle A=\{a\mid a^2&lt;2,a\in \mathbb{Q}^+\}$ 又有 $\displaystyle B=\{b\mid b^2&gt;2, b \in \mathbb{Q}^+\}$ 我们有 $\displaystyle z^2-2=\frac{2(x^2-2)}{(p+2)^2}$ 即可证明$\displaystyle A$ 里面没有最大的数$$\begin{align}x \in A,\to z \in A\,, z&gt;x\end{align}$$ $\displaystyle B$ 里面没有最小的数$$\begin{align}\,x \in B\to z \in B\,,0&lt;z&lt;x\end{align}$$ 这说明尽管有理数之间还有有理数，但是有理数系还是有空隙。而实数系填满了这些空隙，这就是实数系能在分析学中能起基础作用的主要原因。 $\displaystyle \ell \mathbb{ABC}$ 二、行列式行列式定义： $\displaystyle det: M_n(F) \to \Bbb{R} $ 行列式表达式 $$\begin{align}det(A)=\sum _{\sigma \in S_n}sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }\end{align}$$ $$\displaystyledet(A)=\sum _{\sigma \in S_n}sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }$$ 其中$\sigma$是集合$X=\{1,2,\,…\,n\}$上的置换：$\sigma: X \to X$。$S_n$是置换$\sigma$的集合,易知$S_n$是一个对称群。$\tau(\sigma)$是$\sigma$的逆序数。$\displaystyle sgn(\sigma)=\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)=(-1)^{\tau(\sigma)}$是置换的符号函数。于是： $$\displaystyle det(A)=\sum _{\sigma \in S_n}\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)\prod _{i=1}^n A_{\sigma(i)i }=\sum _{\sigma \in S_n}(-1)^{\tau(\sigma)}\prod _{i=1}^n A_{\sigma(i)i }$$ 简记为：$$det(A)=\sum _{\sigma \in S_n} sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }$$ 三、【函数的极限】令$X$和$Y$是度量空间，假设$E \subseteq X$、$\bm{f}$将$E$映入$Y$内、且$\bm{p}$是$E$的极限点。如果 $\forall \epsilon\,,\exists \delta&gt;0$，对于 $\{\bm{x} \in E\mid 0&lt;d_{X}(\bm{x},\bm{p})&lt;\delta\}$中一切点 $\bm{x}$，使得 $\displaystyle d_{Y}(\bm{f}(\bm{x}),\bm{q})&lt;\epsilon,\,\bm{q}\in Y$成立。就说: $$\lim_{\bf{x} \to\bm{p}} \bm{f}(\bm{x})=\bm{q}$$ 卷积$\displaystyle \mathbf{C}=\mathbf{X}*\mathbf{W}$ $\displaystyle \bm{C}=\bm{X}*\bm{W}$ 分段函数$$\displaystyle f(n)= \left\{\begin{matrix} n/2, &amp; \text {if $n$ is even} \\\ 3n+1, &amp; \text{if $n$ is odd} \end{matrix}\right. \\$$ $\displaystyle \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4 \end{bmatrix}$ 我们可以看到 session.run专业我们就可以Variable 1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello).decode('utf-8')) 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/e16e96fd600d84099331bdee7ae39758/如果您需要引用本文，请参考：引线小白. (Dec. 23, 2016). 《数学分析笔记-数的建立》[Blog post]. Retrieved from https://www.limoncc.com/post/e16e96fd600d84099331bdee7ae39758@online{limoncc-e16e96fd600d84099331bdee7ae39758,title={数学分析笔记-数的建立},author={引线小白},year={2016},month={Dec},date={23},url={\url{https://www.limoncc.com/post/e16e96fd600d84099331bdee7ae39758}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
        <tag>数学</tag>
        <tag>现代数学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyhton-notebook主题颜色配置]]></title>
    <url>%2Fpost%2Fae12247dac6e96d2796b3e6769150a6c%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/ae12247dac6e96d2796b3e6769150a6c/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、安装jupyter-themer插件12345678910111213➜ ~ cd anaconda➜ anaconda sudo pip install jupyter-themerPassword:The directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting jupyter-themer Downloading jupyter-themer-0.3.0.tar.gz (40kB) 100% |████████████████████████████████| 40kB 35kB/sRequirement already satisfied: jupyter in ./lib/python3.6/site-packages (from jupyter-themer)Requirement already satisfied: notebook in ./lib/python3.6/site-packages (from jupyter-themer)Installing collected packages: jupyter-themer Running setup.py install for jupyter-themer ... doneSuccessfully installed jupyter-themer-0.3.0 二、设定主题1、语法介绍：1234567%语法格式如下usage: jupyter-themer [-c COLOR, --color COLOR] [-l LAYOUT, --layout LAYOUT] [-t TYPOGRAPHY, --typography TYPOGRAPHY] [-f CODE_FONT, --font CODE_FONT] [-b BACKGROUND, --background BACKGROUND] [-s OPTION, --show OPTION] 2、显示所有可选颜色主题：12345678910111213141516171819202122232425262728293031323334353637383940414243➜ anaconda jupyter-themer --show color3024-day3024-nightabcdefambiancebase16-darkbase16-lightblackboardcobaltcolorforthdraculaeclipseeleganterlang-darkicecoderlesser-darkliquibytematerialmbomdn-likemidnightmonokaineatneonightparaiso-darkparaiso-lightpastel-on-darkrubybluesetisolarized-darksolarized-lightthe-matrixtomorrow-night-brighttomorrow-night-eightiesttcntwilightvibrant-inkxq-darkxq-lightyetizenburn➜ anaconda 3、设定monokai主题12➜ anaconda jupyter-themer -c monokaiCustom jupyter notebook theme created - refresh any open jupyter notebooks to apply theme. 效果如下 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/ae12247dac6e96d2796b3e6769150a6c/如果您需要引用本文，请参考：引线小白. (May. 1, 2016). 《pyhton-notebook主题颜色配置》[Blog post]. Retrieved from https://www.limoncc.com/post/ae12247dac6e96d2796b3e6769150a6c@online{limoncc-ae12247dac6e96d2796b3e6769150a6c,title={pyhton-notebook主题颜色配置},author={引线小白},year={2016},month={May},date={1},url={\url{https://www.limoncc.com/post/ae12247dac6e96d2796b3e6769150a6c}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>工程问题</tag>
        <tag>python</tag>
        <tag>notebook</tag>
        <tag>计算环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夜晓风]]></title>
    <url>%2Fpost%2F230d6caa59e2433ba58fd99c2fca28b5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/230d6caa59e2433ba58fd99c2fca28b5/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夜晓风皎月榕树头，夜晓风。 一曲江畔，感似岁月悠悠， 山河在，壮志未酬。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/230d6caa59e2433ba58fd99c2fca28b5/如果您需要引用本文，请参考：引线小白. (Sep. 19, 2015). 《夜晓风》[Blog post]. Retrieved from https://www.limoncc.com/post/230d6caa59e2433ba58fd99c2fca28b5@online{limoncc-230d6caa59e2433ba58fd99c2fca28b5,title={夜晓风},author={引线小白},year={2015},month={Sep},date={19},url={\url{https://www.limoncc.com/post/230d6caa59e2433ba58fd99c2fca28b5}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[穗夜]]></title>
    <url>%2Fpost%2F8e17e263980fd8624def9829d39bff00%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/8e17e263980fd8624def9829d39bff00/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 穗夜夜声碎月光如练，榕树头江畔愁眠。 桥头浪卷已三更，沉沉幕夏雨如注。 引线小白写于2014-6-19 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/8e17e263980fd8624def9829d39bff00/如果您需要引用本文，请参考：引线小白. (Jun. 19, 2014). 《穗夜》[Blog post]. Retrieved from https://www.limoncc.com/post/8e17e263980fd8624def9829d39bff00@online{limoncc-8e17e263980fd8624def9829d39bff00,title={穗夜},author={引线小白},year={2014},month={Jun},date={19},url={\url{https://www.limoncc.com/post/8e17e263980fd8624def9829d39bff00}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>穗夜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再回大学城]]></title>
    <url>%2Fpost%2F53f3c314a38bb844555c0deacf1de635%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/53f3c314a38bb844555c0deacf1de635/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 再回大学城沉浸校园曲风气爽清秋节，叶至桂香来，小湖椰影廊桥，曾记否，谷围晓月，灯影朦胧，勘回首，沉浸校园曲风，且看从容，壮志未酬笑谈中。 引线小白写于2011-5-20 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/53f3c314a38bb844555c0deacf1de635/如果您需要引用本文，请参考：引线小白. (May. 20, 2011). 《再回大学城》[Blog post]. Retrieved from https://www.limoncc.com/post/53f3c314a38bb844555c0deacf1de635@online{limoncc-53f3c314a38bb844555c0deacf1de635,title={再回大学城},author={引线小白},year={2011},month={May},date={20},url={\url{https://www.limoncc.com/post/53f3c314a38bb844555c0deacf1de635}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>大学城</tag>
        <tag>诗歌</tag>
        <tag>小谷围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夏雨急]]></title>
    <url>%2Fpost%2Fa2b4f020470172a5bbc0f920189b330c%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/a2b4f020470172a5bbc0f920189b330c/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夏雨急夏雨急，落如珠， 今朝且看风疾。 迎地扫，路人急， 此地空余。。。 小白写于2011-4-30日 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/a2b4f020470172a5bbc0f920189b330c/如果您需要引用本文，请参考：引线小白. (Apr. 30, 2011). 《夏雨急》[Blog post]. Retrieved from https://www.limoncc.com/post/a2b4f020470172a5bbc0f920189b330c@online{limoncc-a2b4f020470172a5bbc0f920189b330c,title={夏雨急},author={引线小白},year={2011},month={Apr},date={30},url={\url{https://www.limoncc.com/post/a2b4f020470172a5bbc0f920189b330c}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏雨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初夏里]]></title>
    <url>%2Fpost%2F914b8d8546baa59ec810b57e59a93b32%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/914b8d8546baa59ec810b57e59a93b32/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 初夏里清风夜，晓月枝头，照斑驳绿影初夏里，世事心愁，思长夜忧忧叹悲歌，一曲无穷，怎泪眼朦胧自惆怅，知与谁同，独望江水流 引线小白写于2011-4-27广州 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/914b8d8546baa59ec810b57e59a93b32/如果您需要引用本文，请参考：引线小白. (Apr. 27, 2011). 《初夏里》[Blog post]. Retrieved from https://www.limoncc.com/post/914b8d8546baa59ec810b57e59a93b32@online{limoncc-914b8d8546baa59ec810b57e59a93b32,title={初夏里},author={引线小白},year={2011},month={Apr},date={27},url={\url{https://www.limoncc.com/post/914b8d8546baa59ec810b57e59a93b32}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>伤感</category>
      </categories>
      <tags>
        <tag>初夏里</tag>
        <tag>引线小白</tag>
        <tag>诗词</tag>
        <tag>诗集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[烈日炎炎]]></title>
    <url>%2Fpost%2Fa9de6d958ed910a02f8a56b87d2a28b2%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：httpss://www.limoncc.com/post/a9de6d958ed910a02f8a56b87d2a28b2/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 烈日炎炎日炎炎，清风绝。惜雨荫少，浪浪绵绵。汗易求冰难接。是个酷热天！ 引线小白写于2010-8-4 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ https://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接httpss://www.limoncc.com/post/a9de6d958ed910a02f8a56b87d2a28b2/如果您需要引用本文，请参考：引线小白. (Aug. 4, 2010). 《烈日炎炎》[Blog post]. Retrieved from https://www.limoncc.com/post/a9de6d958ed910a02f8a56b87d2a28b2@online{limoncc-a9de6d958ed910a02f8a56b87d2a28b2,title={烈日炎炎},author={引线小白},year={2010},month={Aug},date={4},url={\url{https://www.limoncc.com/post/a9de6d958ed910a02f8a56b87d2a28b2}},} window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏天</tag>
        <tag>日日炎炎</tag>
      </tags>
  </entry>
</search>
