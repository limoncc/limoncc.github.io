<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PyTorch安装]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2018-03-07-PyTorch%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2018-03-07-PyTorch安装/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在记录了安装PyTorch的过程。若有错误，请大家指正。关键词: 深度学习,PyTorch 一、安装PyTorch1234567891011➜ ~ cd anaconda➜ anaconda pip3 install http://download.pytorch.org/whl/torch-0.3.1-cp36-cp36m-macosx_10_7_x86_64.whlzsh: command not found: pip3➜ anaconda pip install http://download.pytorch.org/whl/torch-0.3.1-cp36-cp36m-macosx_10_7_x86_64.whlCollecting torch==0.3.1 from http://download.pytorch.org/whl/torch-0.3.1-cp36-cp36m-macosx_10_7_x86_64.whl Downloading http://download.pytorch.org/whl/torch-0.3.1-cp36-cp36m-macosx_10_7_x86_64.whl (7.0MB) 100% |████████████████████████████████| 7.0MB 118kB/sRequirement already satisfied: numpy in ./lib/python3.6/site-packages (from torch==0.3.1)Requirement already satisfied: pyyaml in ./lib/python3.6/site-packages (from torch==0.3.1)Installing collected packages: torchSuccessfully installed torch-0.3.1 二、安装Torchvision1234567891011121314151617➜ anaconda pip install torchvisionCollecting torchvision Downloading torchvision-0.2.0-py2.py3-none-any.whl (48kB) 100% |████████████████████████████████| 51kB 68kB/sRequirement already satisfied: torch in ./lib/python3.6/site-packages (from torchvision)Collecting pillow&gt;=4.1.1 (from torchvision) Downloading Pillow-5.0.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.5MB) 100% |████████████████████████████████| 3.5MB 104kB/sRequirement already satisfied: numpy in ./lib/python3.6/site-packages (from torchvision)Requirement already satisfied: six in ./lib/python3.6/site-packages (from torchvision)Requirement already satisfied: pyyaml in ./lib/python3.6/site-packages (from torch-&gt;torchvision)Installing collected packages: pillow, torchvision Found existing installation: Pillow 4.0.0 Uninstalling Pillow-4.0.0: Successfully uninstalled Pillow-4.0.0Successfully installed pillow-5.0.0 torchvision-0.2.0➜ anaconda 三、测试1234567891011121314151617181920➜ ~ cd anaconda➜ anaconda pythonPython 3.6.2 |Anaconda custom (x86_64)| (default, Jul 20 2017, 13:14:59)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import torch&gt;&gt;&gt; # 定义未初始化的5x3张量&gt;&gt;&gt; x = torch.Tensor(5, 3)&gt;&gt;&gt; # 随机初始化的5x3张量&gt;&gt;&gt; x = torch.rand(5, 3)&gt;&gt;&gt; print(x) 0.4774 0.7832 0.3552 0.1281 0.1750 0.0092 0.5813 0.8938 0.3146 0.7202 0.0426 0.7709 0.3413 0.5874 0.6588[torch.FloatTensor of size 5x3]&gt;&gt;&gt; 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2018-03-07-PyTorch安装/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装MXnet]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2018-03-07-%E5%AE%89%E8%A3%85MXnet%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2018-03-07-安装MXnet/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在记录安装MXnet的过程。若有错误，请大家指正。关键词: 深度学习,MXnet 一、安装MXnet1234567891011121314151617181920➜ anaconda sudo pip install mxnetThe directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting mxnet Downloading mxnet-1.1.0-cp36-cp36m-macosx_10_12_x86_64.whl (9.3MB) 100% |████████████████████████████████| 9.3MB 71kB/sRequirement already satisfied: requests==2.18.4 in ./lib/python3.6/site-packages (from mxnet)Collecting numpy&lt;=1.13.3 (from mxnet) Downloading numpy-1.13.3-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.5MB) 100% |████████████████████████████████| 4.6MB 125kB/sRequirement already satisfied: graphviz==0.8.1 in ./lib/python3.6/site-packages (from mxnet)Requirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in ./lib/python3.6/site-packages (from requests==2.18.4-&gt;mxnet)Requirement already satisfied: idna&lt;2.7,&gt;=2.5 in ./lib/python3.6/site-packages (from requests==2.18.4-&gt;mxnet)Requirement already satisfied: certifi&gt;=2017.4.17 in ./lib/python3.6/site-packages (from requests==2.18.4-&gt;mxnet)Requirement already satisfied: urllib3&lt;1.23,&gt;=1.21.1 in ./lib/python3.6/site-packages (from requests==2.18.4-&gt;mxnet)Installing collected packages: numpy, mxnet Found existing installation: numpy 1.14.1 Uninstalling numpy-1.14.1: Successfully uninstalled numpy-1.14.1Successfully installed mxnet-1.1.0 numpy-1.13.3 第二步、Graphviz1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495➜ anaconda brew install graphvizUpdating Homebrew...==&gt; Auto-updated Homebrew!Updated 1 tap (homebrew/core).==&gt; Updated Formulaeapp-engine-java mesonbit mongo-cxx-driverburp muttcalabash nanopb-generatorcayley ncoconan nsprdub odpieralchemy offlineimapfeh openimageiofluent-bit plankflyway pre-commitfn protobuffolly pyqtfuse-emulator qpdfgeth qpid-protonglade qscintilla2gomplate s-nailgsoap saxongutenberg sbclgxml selenium-server-standaloneigv sipimagemagick sparkeyimagemagick@6 syncthingjbake tile38jenkins twarcjust vnstatkerl wireguard-toolslibvirt yaml-cpp==&gt; Installing dependencies for graphviz: libtool, libpng, freetype, fontconfig, jpeg, libtiff, webp, gd==&gt; Installing graphviz dependency: libtool==&gt; Downloading https://homebrew.bintray.com/bottles/libtool-2.4.6_1.high_sierra==&gt; Downloading from https://akamai.bintray.com/eb/ebb50367eb2336ee317841587e246######################################################################## 100.0%==&gt; Pouring libtool-2.4.6_1.high_sierra.bottle.tar.gz==&gt; CaveatsIn order to prevent conflicts with Apple's own libtool we have prepended a "g"so, you have instead: glibtool and glibtoolize.==&gt; Summary🍺 /usr/local/Cellar/libtool/2.4.6_1: 71 files, 3.7MB==&gt; Installing graphviz dependency: libpng==&gt; Downloading https://homebrew.bintray.com/bottles/libpng-1.6.34.high_sierra.b==&gt; Downloading from https://akamai.bintray.com/d3/d38a64089526ecc1413acbc223738######################################################################## 100.0%==&gt; Pouring libpng-1.6.34.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/libpng/1.6.34: 26 files, 1.2MB==&gt; Installing graphviz dependency: freetype==&gt; Downloading https://homebrew.bintray.com/bottles/freetype-2.9.high_sierra.bo==&gt; Downloading from https://akamai.bintray.com/29/2978dbec18cf06827ddc93ee04262######################################################################## 100.0%==&gt; Pouring freetype-2.9.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/freetype/2.9: 60 files, 2.7MB==&gt; Installing graphviz dependency: fontconfig==&gt; Downloading https://homebrew.bintray.com/bottles/fontconfig-2.12.6.high_sier==&gt; Downloading from https://akamai.bintray.com/8c/8c9ff65654be03a4003d0e0d9e27f######################################################################## 100.0%==&gt; Pouring fontconfig-2.12.6.high_sierra.bottle.tar.gz==&gt; Regenerating font cache, this may take a while==&gt; /usr/local/Cellar/fontconfig/2.12.6/bin/fc-cache -frv🍺 /usr/local/Cellar/fontconfig/2.12.6: 493 files, 3.2MB==&gt; Installing graphviz dependency: jpeg==&gt; Downloading https://homebrew.bintray.com/bottles/jpeg-9c.high_sierra.bottle.==&gt; Downloading from https://akamai.bintray.com/17/178200fd8aa50d5db22c5faa4ca40######################################################################## 100.0%==&gt; Pouring jpeg-9c.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/jpeg/9c: 21 files, 724.5KB==&gt; Installing graphviz dependency: libtiff==&gt; Downloading https://homebrew.bintray.com/bottles/libtiff-4.0.9_2.high_sierra==&gt; Downloading from https://akamai.bintray.com/b2/b25a0893acdffc8fcbb1f9d0a2f1e######################################################################## 100.0%==&gt; Pouring libtiff-4.0.9_2.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/libtiff/4.0.9_2: 246 files, 3.5MB==&gt; Installing graphviz dependency: webp==&gt; Downloading https://homebrew.bintray.com/bottles/webp-0.6.1.high_sierra.bott==&gt; Downloading from https://akamai.bintray.com/f4/f42744b43febbc4a5d8cac83c87c0######################################################################## 100.0%==&gt; Pouring webp-0.6.1.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/webp/0.6.1: 38 files, 2.0MB==&gt; Installing graphviz dependency: gd==&gt; Downloading https://homebrew.bintray.com/bottles/gd-2.2.5.high_sierra.bottle==&gt; Downloading from https://akamai.bintray.com/ff/ff7aa2d452c6c05f8d41dee63bbd1######################################################################## 100.0%==&gt; Pouring gd-2.2.5.high_sierra.bottle.tar.gz🍺 /usr/local/Cellar/gd/2.2.5: 35 files, 1.1MB==&gt; Installing graphviz==&gt; Downloading https://homebrew.bintray.com/bottles/graphviz-2.40.1.high_sierra==&gt; Downloading from https://akamai.bintray.com/b5/b592ce51c2a929c3da82e96ec8565######################################################################## 100.0%==&gt; Pouring graphviz-2.40.1.high_sierra.bottle.1.tar.gz🍺 /usr/local/Cellar/graphviz/2.40.1: 500 files, 11.2MB 关于graphviz可以点击http://www.graphviz.org 第三步、验证安装正确123456789101112➜ ~ cd anaconda➜ anaconda pythonPython 3.6.2 |Anaconda custom (x86_64)| (default, Jul 20 2017, 13:14:59)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import mxnet as mx&gt;&gt;&gt; a = mx.nd.ones((2, 3))&gt;&gt;&gt; b = a * 2 + 1&gt;&gt;&gt; b.asnumpy()array([[ 3., 3., 3.], [ 3., 3., 3.]], dtype=float32)&gt;&gt;&gt; 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2018-03-07-安装MXnet/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>MXnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google机器学习-模型训练与评估]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2018-03-07-Google%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2018-03-07-Google机器学习-模型训练与评估/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清机器学习的基础问题。若有错误，请大家指正。关键词: tensorflow,模型训练,模型评估 我们首先加载并准备数据。这一次，我们将使用多个特征，因此我们会将逻辑模块化，以对特征进行预处理。我们将使用加利福尼亚州住房数据集，尝试根据1990年的人口普查数据在城市街区级别预测房价中位数 median_house_value。 一、数据处理1.1、载入数据123456789101112131415161718192021222324252627282930313233343536373839404142# 忽略警告，但是好像没有什么用。import warningswarnings.filterwarnings("ignore")import mathfrom IPython import displayfrom matplotlib import cmfrom matplotlib import gridspecfrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdfrom sklearn import metricsimport tensorflow as tf# 设置工具基本参数，含义依次是：# 输出训练过程中的loss信息，代码直译的意思是：日志信息显示设置(误差日志)# pd数据显示设置最多显示10行。# 显示小数点后1位。tf.logging.set_verbosity(tf.logging.ERROR)pd.options.display.max_rows = 10pd.options.display.float_format = '&#123;:.1f&#125;'.format#==============================# 读取数据#==============================california_housing_dataframe = pd.read_csv( "california_housing_train.csv", sep=",")# 参看前五条数据california_housing_dataframe.head(5)# 对数据集进行随机化处理# np的随机排列函数可以对array(向量)的元素随机重新安排位置。# pd的dataframe数据类型的reindex函数可以通过输入重新安排顺序的索引，来对数据重新排序，但是索引是不变的。# 记住这不会改变原来的数据顺序。所以写法上，我们再次用原来的变量名赋值了一次california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index)) 我们来解释一下数据： 序号 经度 纬度 房龄中位数 房间总数 卧室总数 人口 户数 收入中位数 房价中位数 0 -114.3 34.2 15.0 5612.0 1283.0 1015.0 472.0 1.5 66900.0 1 -114.5 34.4 19.0 7650.0 1901.0 1129.0 463.0 1.8 80100.0 1.2、预处理数据另外我们需要处理数据，以便让数据适用于我们的模型。下面我们通过自定义一些函数，来实现这一点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#==============================# 定义两个数据预处理函数#==============================def preprocess_features(california_housing_dataframe): # 我们通过输入dataframe格式的数据，来生成一个同样是dataframe格式的数据， # 其中我们会合成一个特征：人均房间数，以适用于我们的模型。 # 函数是这样来写的：定义个选择特征变量数据集，然后生成一个预处理特征变量数据集 # 输入： # california_housing_dataframe：dataframe格式数据集 # 输出： # processed_features：预处理特征变量数据集 # 疑问解决: Email：limoncc@icloud.com selected_features = california_housing_dataframe[ ["latitude", "longitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income"]] processed_features = selected_features.copy() # 创建一个混合特征:人均房间数 processed_features["rooms_per_person"] = ( california_housing_dataframe["total_rooms"] / california_housing_dataframe["population"]) return processed_features # end def of preprocess_featuresdef preprocess_targets(california_housing_dataframe): # 改变一下预测变量单位：房价中位数单位改为千美元 thousand dollars。即放缩变量。 # 同时准备一个单独的输出变量数据集，格式同样是dataframe。 # 函数是这样来写的：生成一个空的 dataframe，然后赋值。 # 疑问解决: Email：limoncc@icloud.com output_targets = pd.DataFrame() # 放缩变量，房价中位数单位改为千美元. output_targets["median_house_value"] = ( california_housing_dataframe["median_house_value"] / 1000.0) return output_targets # end def of preprocess_targets 1.3、切分数据集为训练集、验证集对于训练集，我们从共 17000 个样本中选择前 12000 个样本。对于验证集，我们从共 17000 个样本中选择后 5000 个样本。 1234567891011#==============================# 切分数据集为训练集，验证集#==============================# 从共 17000 个样本中选择前 12000 个样本作为训练集training_examples = preprocess_features( california_housing_dataframe.head(12000))print("训练特征集概览")print(training_examples.describe()) 训练特征集概览 经度 纬度 房龄中位数 房间总数 卧室总数 人口 户数 收入中位数 人均房间数 count 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 mean 35.6 -119.6 28.5 2655.5 541.8 1435.4 503.5 3.9 2.0 std 2.1 2.0 12.6 2207.5 425.8 1163.5 388.4 1.9 1.2 min 32.5 -124.3 1.0 8.0 1.0 3.0 1.0 0.5 0.0 25% 33.9 -121.8 18.0 1465.0 299.0 792.0 282.8 2.6 1.5 50% 34.2 -118.5 28.0 2132.5 434.0 1165.0 409.0 3.5 1.9 75% 37.7 -118.0 37.0 3165.0 652.0 1726.0 608.0 4.8 2.3 max 42.0 -114.3 52.0 37937.0 5471.0 35682.0 5189.0 15.0 52.0 123training_targets = preprocess_targets(california_housing_dataframe.head(12000))print("训练输出集概览")training_targets.describe() 训练输出集概览 median_house_value count 12000.0 mean 208.0 std 116.6 min 15.0 25% 119.8 50% 180.9 75% 266.0 max 500.0 1234# 从共 17000 个样本中选择后 5000 个样本作为验证集validation_examples = preprocess_features(california_housing_dataframe.tail(5000))print("验证特征集概览")validation_examples.describe() 验证特征集概览 经度 纬度 房龄中位数 房间总数 卧室总数 人口 户数 收入中位数 人均房间数 count 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 mean 35.6 -119.6 28.8 2615.3 533.6 1415.6 495.6 3.9 2.0 std 2.1 2.0 12.6 2112.2 410.9 1109.5 375.0 1.9 1.2 min 32.5 -124.3 1.0 2.0 2.0 6.0 2.0 0.5 0.1 25% 33.9 -121.8 18.0 1452.0 293.0 785.0 279.0 2.5 1.5 50% 34.2 -118.5 29.0 2112.0 434.0 1169.0 409.0 3.5 1.9 75% 37.7 -118.0 37.0 3120.0 640.0 1712.2 596.0 4.7 2.3 max 41.9 -114.5 52.0 32627.0 6445.0 28566.0 6082.0 15.0 55.2 123validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))print("验证输出集概览")validation_targets.describe() 验证输出集概览 median_house_value count 12000.0 mean 208.0 std 116.6 min 15.0 25% 119.8 50% 180.9 75% 266.0 max 500.0 1.4、观察数据有没有异常12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#==============================# 观察数据有没有异常#==============================# 让图表能在ipython 中可以显示。# % matplotlib inline# figure函数# figure(num=None, figsize=None, dpi=None,# facecolor=None, edgecolor=None, frameon=True, FigureClass=&lt;class 'matplotlib.figure.Figure'&gt;, clear=False, **kwargs)# 创建一个figure对象，并设置大小为13x8英寸plt.figure(figsize=(13, 8))# 在figure对象创建1行2列的第一个图。ax = plt.subplot(1, 2, 1)# 设置1行2列的第一个图ax.set_title("Validation Data")ax.set_autoscaley_on(False)ax.set_ylim([32, 43])ax.set_autoscalex_on(False)ax.set_xlim([-126, -112])# 绘制一经纬度为坐标，以中位数房价与验证集中位数房价比例绘制颜色。来作散点图plt.scatter(validation_examples["longitude"], validation_examples["latitude"], cmap="coolwarm", c=validation_targets["median_house_value"] / validation_targets["median_house_value"].max())# 以下命令相同。ax = plt.subplot(1, 2, 2)ax.set_title("Training Data")ax.set_autoscaley_on(False)ax.set_ylim([32, 43])ax.set_autoscalex_on(False)ax.set_xlim([-126, -112])plt.scatter(training_examples["longitude"], training_examples["latitude"], cmap="coolwarm", c=training_targets["median_house_value"] / training_targets["median_house_value"].max())# 下面这句可以去掉ipython的out信息。让ipython看上去更美观点。_ = plt.plot() 现在应该已经呈现出一幅不错的加利福尼亚州地图了，其中旧金山和洛杉矶等住房成本高昂的地区用红色表示。根据训练集呈现的地图有几分像真正的地图，但根据验证集呈现的明显不像。训练数据和验证数据之间的特征或目标分布明显不同。 这一事实表明我们创建训练集和验证集的拆分方式很可能存在问题。同时我们也会发现：机器学习中的调试通常是数据调试 而不是代码调试。如果我们在创建训练集和验证集之前，没有对数据进行正确的 随机化处理，那么以某种特定顺序接收数据可能会导致出现问题（似乎就是此时的问题）。 我们在一开始应该加上如下代码：123# 对数据集进行随机化处理california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index)) 二、训练模型花费约 5 分钟的时间尝试不同的超参数设置。尽可能获取最佳验证效果。然后，我们会使用数据集中的所有特征训练一个线性回归器，看看其表现如何。我们来定义一下以前将数据加载到 TensorFlow 模型中时所使用的同一输入函数。 1234567891011121314151617181920212223242526272829303132333435363738394041#==============================# 训练模型#==============================def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None): # 定义一个满足线性回归模型输入特征 # 输入: # features: DataFrame格式的特征 # targets: DataFrame格式的目标输出 # batch_size: 通过模型的批量大小 # shuffle: True or False. 是否打乱数据 # num_epochs: 需要重复数据的次数。None = 无限重复 # 输出: # features：下个批量数据的元组(特征) # labels：下个批量数据的元组(标签) # 疑问解决: Email：limoncc@icloud.com # 转换数据为np中arrays的字典格式 features = &#123;key: np.array(value) for key, value in dict(features).items()&#125; # 构建数据集，并设置批量和重复次数 ds = tf.data.Dataset.from_tensor_slices( (features, targets)) # warning: 2GB limit ds = ds.batch(batch_size).repeat(num_epochs) # Shuffle the data, if specified 如果需要，打乱数据。它维护一个固定大小的缓冲区，并从缓冲区中随机选择下一个元素。 # 从Dataset中消耗(consume)值的最常见的方法是创建一个iterator对象，该对象对数据集一次只提供一个元素访问。 if shuffle: ds = ds.shuffle(10000) # 返回数据的下一个批量 # 从Dataset中消耗值的最常见的方法是创建一个iterator对象，该对象对数据集一次只提供一个元素访问。 features, labels = ds.make_one_shot_iterator().get_next() return features, labels # end 结束my_input_fn 由于我们现在使用的是多个输入特征，因此需要把用于将特征列配置为独立函数的代码模块化。（目前此代码相当简单，因为我们的所有特征都是数值，但当我们在今后的练习中使用其他类型的特征时，会基于此代码进行构建。 1234567891011def construct_feature_columns(input_features): # 构建一个Tensorflow的特征列对象(Feature Columns)。 # 输入: # input_features: 数字输入特征的名字 # 输出: # 特征列集合 # 疑问解决: Email：limoncc@icloud.com return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features]) # end 结束标记 接下来，继续完成下面的 train_model() 代码，以设置输入函数和计算预测。注意：可以参考以前的练习中的代码，但要确保针对相应数据集调用 predict()。比较训练数据和验证数据的损失。使用一个原始特征时，我们得到的最佳均方根误差 (RMSE) 约为 180。现在我们可以使用多个特征，不妨看一下可以获得多好的结果。使用我们之前了解的一些方法检查数据。这些方法可能包括：1、比较预测值和实际目标值的分布情况2、绘制预测值和目标值的散点图3、使用 latitude 和 longitude 绘制两个验证数据散点图：4、一个散点图将颜色映射到实际目标 median_house_value5、另一个散点图将颜色映射到预测的 median_house_value，并排进行比较。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#==============================# 训练模型#==============================def train_model( learning_rate, stepsbatch_size, training_examples, training_targets, validation_examples, validation_targets): # 训练一个特征的线性模型 # 除了训练，这个函数还将打印训练进度信息， # 以及一段时间内的训练损失和验证损失。 # 输入: # learning_rate：训练速度，是一个浮点数 # steps：训练步骤的总数，是一个非零的整数。训练步骤由一个使用单一批量的前后和向后传递组成。 # batch_size：批量大小一个非零的整数，是一个非零的整数 # training_examples：一个dataframe数据格式对象，包含了来自‘california_housing_dataframe’数据的一个列或多列，用在训练用的输入特征 # training_targets：一个dataframe数据格式对象，包含了来自‘california_housing_dataframe’数据的恰好一列，用作训练目标 # validation_examples：一个dataframe数据格式对象，包含了来自‘california_housing_dataframe’数据的一个列或多列，用作验证的输入特征 # validation_targets: 一个dataframe数据格式对象，包含了来自‘california_housing_dataframe’数据的恰好一列，用作验证目标 # 输出: # 返回一个在训练数据上训练的'LinearRegressor'对象 # 疑问解决: Email：limoncc@icloud.com periods = 10 steps_per_period = steps / periods # 创建线性回归对象 my_optimizer = tf.train.GradientDescentOptimizer( learning_rate=learning_rate) my_optimizer = tf.contrib.estimator.clip_gradients_by_norm( my_optimizer, 5.0) linear_regressor = tf.estimator.LinearRegressor( feature_columns=construct_feature_columns(training_examples), optimizer=my_optimizer) # 创建输入函数 def training_input_fn(): return my_input_fn( training_examples, training_targets["median_house_value"], batch_size=batch_size) def predict_training_input_fn(): return my_input_fn( training_examples, training_targets["median_house_value"], num_epochs=1, shuffle=False) def predict_validation_input_fn(): return my_input_fn( validation_examples, validation_targets["median_house_value"], num_epochs=1, shuffle=False) # 训练模型，但要在循环中运行，这样我们就可以周期性地进行评估。 # 损失指标(loss metrics) print("Training model...") print("RMSE (on training data):") training_rmse = [] validation_rmse = [] for period in range(0, periods): # 从初始状态开始训练模型 linear_regressor.train(input_fn=training_input_fn,steps=steps_per_period,) # 暂停，然后计算训练集和验证集的预测，并转化为np的array格式 training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn) training_predictions = np.array([item['predictions'][0] for item in training_predictions]) validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn) validation_predictions = np.array([item['predictions'][0] for item in validation_predictions]) # 计算训练集合验证的残差 training_root_mean_squared_error = math.sqrt(metrics.mean_squared_error(training_predictions, training_targets)) validation_root_mean_squared_error = math.sqrt(metrics.mean_squared_error(validation_predictions, validation_targets)) # 偶尔打印当前损失。 print(" period %02d : %0.2f" %(period, training_root_mean_squared_error)) # 将此期间的损失指标添加到我们的列表中。 training_rmse.append(training_root_mean_squared_error) validation_rmse.append(validation_root_mean_squared_error) print("Model training finished.") # 输出一段时间内损失指标的图表。 plt.ylabel("RMSE") plt.xlabel("Periods") plt.title("Root Mean Squared Error vs. Periods") plt.tight_layout() plt.plot(training_rmse, label="training") plt.plot(validation_rmse, label="validation") plt.legend() return linear_regressor # end 结束标记 123456789101112#==============================# 运行模型#==============================linear_regressor = train_model( learning_rate=0.00003, steps=500, batch_size=5, training_examples=training_examples, training_targets=training_targets, validation_examples=validation_examples, validation_targets=validation_targets) Training model... RMSE (on training data): period 00 : 218.32 period 01 : 200.74 period 02 : 187.11 period 03 : 177.11 period 04 : 171.24 period 05 : 168.59 period 06 : 168.07 period 07 : 168.07 period 08 : 168.74 period 09 : 170.20 Model training finished. 三、测试模型1234567891011121314151617181920212223#==============================# 测试模型#==============================california_housing_test_data = pd.read_csv( "https://storage.googleapis.com/ml_universities/california_housing_test.csv", sep=",")test_examples = preprocess_features(california_housing_test_data)test_targets = preprocess_targets(california_housing_test_data)def predict_test_input_fn(): return my_input_fn( test_examples, test_targets["median_house_value"], num_epochs=1, shuffle=False)test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)test_predictions = np.array([item['predictions'][0]for item in test_predictions])root_mean_squared_error = math.sqrt(metrics.mean_squared_error(test_predictions, test_targets))print("Final RMSE (on test data): %0.2f" % root_mean_squared_error) Final RMSE (on test data): 221.57 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2018-03-07-Google机器学习-模型训练与评估/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型的维特比算法]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2018-01-09-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2018-01-09-隐马尔可夫模型的维特比算法/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清维特比算法的基础问题。本文通过清晰的数学符号，说明问题，大大易于理解。若有错误，请大家指正。关键词: 隐马尔可夫模型,维特比算法,动态规划 一、问题描述定义隐变量数据集 $\displaystyle \mathcal{D}_T^z$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}$,完全数据集 $\displaystyle \mathcal{D}^+_T$。维特比算法要解决的是求隐变量最可能状态序列。可以理解为：已知观测变量数据集，推断隐变量数据集：$$\begin{align}\hat{\mathcal{D}_T^\bm{z}}=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^\bm{z}}\,p \big(\mathcal{D}_T^\bm{z}\mid\mathcal{D}_T^\bm{x}\big)\end{align}$$注意到：$$\begin{align}p \big(\mathcal{D}_{t}^z\mid \mathcal{D}_t^\bm{x}\big)=p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\big/p \big(\mathcal{D}_t^\bm{x}\big)\propto p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\end{align}$$相差一个常数 $ p \big(\mathcal{D}_t^\bm{x}\big)$，通过直接优化完全数据集会简化问题。我们选择最大化下式：$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{\mathcal{D}_t^z}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\\end{align}$$其中完全数据集的概率 $\displaystyle \pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]$，写成对数形式有：$$\begin{align}\ln\pi[i_1]+\ln\rho_1[i_1]+\sum_{t=2}^T \bigg[\ln A_t[i_{t-1},i_{t}]+\ln \rho_t[i_t]\bigg]\end{align}$$这个形式有利于我们着手分析。 二、前向计算我们可以拆分隐变量数据集 $ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t\}$，拆分的关键直觉是时刻 $ t$的最可能路径必须有是由时刻 $ t-1$的最可能路径组成。问题变为$$\begin{align}\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)=\max_{z_t}\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$追寻这一关键思想，下面来具体化：我们假设 $ t$时刻的状态为 $ i_t$，进而定义路径$ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t=i_t\}$的最大概率(权重)：$$\begin{align}\delta_t[i_t]=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\end{align}$$为了充分利用隐马尔可夫模型的条件独立性质和动态规划思想，假设 $ t-1$的状态为 $ i_{t-1}$。继续拆分数据集于是有：$$\begin{align}\delta_{t}[i_t]&amp;=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\\&amp;=\max_{\mathcal{D}_{t-2}^z,z_{t-1}=i_{t-1}}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\&amp;=\max_{i_{t-1}}\bigg[p\big(z_t=i_t\mid z_{t-1}=i_{t-1}\big)p\big(\bm{x}_t\mid z_t=i_t\big)\max_{\mathcal{D}_{t-2}^z} p \big(\mathcal{D}_{t-2}^\bm{x},\mathcal{D}_{t-1}^z,z_{t-1}=i_{t-1}\big)\bigg]\\&amp;=\max_{i_{t-1}} \delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\\\end{align}$$也就是说：时刻 $ t$行至状态 $ i_t$的最可能路径必须有是由时刻 $ t-1$ 行至其他状态 $ i_{t-1}$的最可能路径组成。$$\begin{align}\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$定义初始状态 $ \delta_1[i_1]=\pi[i_1]\rho_1[i_1]$。利用这公式，已知观测变量数据集 $ \mathcal{D}_T^\bm{x}$，可以求时刻 $ t$状态为 $ i_t$的最大概率，有$$\begin{align}\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\end{align}$$ 三、后向回溯回到我们的问题：已知观测变量数据集，推断隐变量数据集。最大化联合概率问题变为$$\begin{align}\max_{\mathcal{D}_T^z} p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=\max_{i_T}\max_{i_{T-1}}A[i_{T-1},i_t] \rho_T[i_T]\cdots\max_{i_1}\delta_1[i_1]A[i_{1},i_2] \rho_2[i_2]\end{align}$$这样一种转换称为 max-product操作。 为了解决这个问题，回顾动态规划思想：最优路径 $ \hat{\mathcal{D}}_{1:T}^z$的一部分 $ \hat{\mathcal{D}}_{t:T}^z$对于 $ t:T$的所有可能路径 $ \mathcal{D}_{t:T}^z$必然是最优。如果存在另外一条路径 $ \tilde{\mathcal{D}}_{t:T}^z$是最优的，那么会出现矛盾 $ \hat{\mathcal{D}}_{1:t}^z\cup \tilde{\mathcal{D}}_{t:T}^z\neq \hat{\mathcal{D}}_{1:T}^z $，所以 $ \hat{\mathcal{D}}_{t:T}^z$ 必须是最优的。根据这一思想，我们定义回溯操作 traceback: $ \omega_t[\cdot]$，来从后向前还原最优状态序列。$$\begin{align}\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$定义 $ T$时刻最优状态 $\displaystyle \hat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]$。应用回溯操作，得到最优路径：$$\begin{align}\hat{\mathcal{D}}_{T}^z=\{\hat{z}_{t-1}=\omega_t[\hat{z}_{t}]\}_{t=T}^1\end{align}$$ 为了解决数据下溢问题，我们可以取对数$$\begin{align}&amp;\ln\delta_{t}[i_t]=\max_{i_{t-1}}\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\end{align}$$ 算法：维特比算法 $1\displaystyle \bm{\delta}_1=\bm{\pi}^\text{T}\bm{A}$$2\displaystyle \text{ for }\,t=2:T$$\displaystyle\quad\begin{array}{|lc}\text{ for }\,i_{t}=1:K \\\quad\begin{array}{|lc}\displaystyle \big[\ln\delta_t[i_t],\omega_{t-1}[i_t]\big]=\max_{i_{t-1}}\ln \bm{\delta}_{t-1}\odot \ln\bm{A}+\bm{I}\ln B_{t}[i_{t},x_t]\big]\end{array}\\\text{ end}\\\end{array}$3end$\displaystyle [\ln\delta_T,\hat{z}_{T}]=\max_{i_T}\,\ln\bm{\delta}_T$$4\displaystyle \text{ for }\,t=T:2$ $\displaystyle\quad\begin{array}{|lc}\hat{z}_{t-1}=\omega_{t}[\hat{z}_{t}]\end{array}$5end$6\displaystyle \hat{\bm{z}}$ 四、评述1、在切分数据集后，同过充分利用隐马尔可夫模型的条件独立假设，我们得到迭代方程和回溯方程。$$\begin{align}&amp;\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\\&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]\end{align}$$2、使用 max-product，我们认识到寻找最优隐状态序列问题是个动态规划问题。3、然后很自然，使用动态规划，解决了问题。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2018-01-09-隐马尔可夫模型的维特比算法/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>维特比算法</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类分布大意]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-05-06-%E5%88%86%E7%B1%BB%E5%88%86%E5%B8%83%E5%A4%A7%E6%84%8F%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清分类分布的基础问题。若有错误，请大家指正。关键词: multinoulli,Categorical,softmax 一、分类分布的若干形式1.1、分类分布指示形式第一，回顾一下猫猫分布 (multinoulli distribution)或者叫 (Categorical distribution)。因为有英文 cat，我又叫它猫猫分布🐈：$$\begin{align}\mathrm{Cat}(x\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{\mathbb{I}(x=c)},x\in\{1,…,C\}\end{align}$$其中$\displaystyle \bm{\mu}=[\mu_1,\mu_2,…,\mu_C]^\text{T}\,,\sum_{c=1}^{C}\mu_c=\bm{1}^\text{T}\bm{\mu}=1$ 1.2、分类分布0-1编码形式第二、特别的，我们使用0-1编码来表示分类时，我们有：$$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{x_c}\,,\bm{x}=[x_1,x_2,…,x_C]^\text{T}\,,\bm{x}\in\{0,1\}^C\end{align}$$其中 $\displaystyle \sum_{c=1}^{C}x_c=\bm{1}^\text{T}\bm{x}=1$ 1.3、分类分布指数族形式第三、继续使用0-1编码，现在我们使用指数族的思想，对分类分布，也就是这个猫猫加以变形，这形式是我们要经常用到的形式： $$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})&amp;=\prod_{c=1}^{C}\mu_c^{x_c}=\exp\bigg[\sum_{c=1}^C x_c\ln\mu_c\bigg]=\exp\bigg[\bm{x}^\text{T}\ln \bm{\mu}\bigg]\\&amp;=\exp\bigg[\bm{x}^\text{T}\ln\frac{\bm{\mu}}{\mu_C}+\ln \mu_C\bigg]\end{align}$$我们定义一个新的向量 $\displaystyle \bm{\eta}=\ln\frac{\bm{\mu}}{\mu_C}$，于是有：$$\begin{align}\bm{\mu}=\mathcal{S}\big(\bm{\eta}\big)=\mathrm{softmax}\big(\bm{\eta}\big)=\Bigg[\frac{\mathrm{e}^{\eta_c}}{\sum_{c=1}^{C}\mathrm{e}^{\eta_c}}\Bigg]_{1\times C}=\frac{\mathrm{e}^\bm{\eta}}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\end{align}$$亦有$$\begin{align}\bm{x}\sim \mathrm{Cat}\big(\bm{x}\mid \mathcal{S}\big(\bm{\eta}\big)\big)=\exp \big[\bm{x}^\text{T}\bm{\eta}-\ln [\bm{I}^\text{T}\mathrm{e}^\bm{\eta}]\big]\end{align}$$ 二、分类分布、sofxmax和多元logistic回归。2.1、多元logistic回归模型考虑如下广义线性模型连接函数 $\displaystyle g \big(\bm{\mu}\big)=\bm{\eta}=\bm{W}\bm{x}\to \bm{\mu}=\mathcal{S}\big(\bm{W}\bm{x}\big)$。这一模型的形式具有十分重要的意义，首先让我们把诸多特征与与分类建立的了数学模型，其次分类分布的期望是一组归一化的概率，直接代表了我们对一次特征观测应该对应于哪个分类的信心。辅助以决策论，很容易做出推断。$$\begin{align}\bm{y}\sim \mathrm{Cat}\big(\bm{y}\mid \mathcal{S}\big(\bm{W}\bm{x}\big)\big)\end{align}$$我们有对数似然：$$\begin{align}\ell \big(\mathcal{D}\mid \bm{W}\big)&amp;=\ln \prod_{i=1}^N\prod_{c=1}^{C}\mathcal{S}_{ic}^{y_{ic}}=\sum_{i=1}^N \bm{y}_i ^\text{T}\ln \mathcal{S}\big(\bm{W}\bm{x}_i\big)=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{W}\bm{x}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{W}\bm{x}_i\big]\bigg)\bigg]\\&amp;=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{\eta}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]\bigg)\bigg]\end{align}$$ 2.2、多元logistic回归的梯度我们有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\sum_{i=1}^N \Bigg[\bm{y}_i- \frac{\mathrm{diag}\big[\mathrm{e}^{\bm{\eta}_i}\big]\bm{I}}{\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]}\Bigg]=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\end{align}$$有如下微分$$\begin{align}\mathrm{d}\ell=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i=\mathrm{tr}\bigg(\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i\bigg)\end{align}$$注意到 $\displaystyle \bm{\eta}_i=\bm{W}\bm{x}_i\to \mathrm{d}\bm{\eta}_i=\mathrm{d}\bm{W}\bm{x}_i$于是根据数量函数与矩阵微分的地定义有 ：$$\begin{align}&amp;\mathrm{d}\ell=\mathrm{tr}\bigg(\frac{\partial \ell}{\partial \bm{\eta}_i ^\text{T}}\cdot\mathrm{d}\bm{\eta}_i\bigg)=\sum_{i=1}^N\mathrm{tr}\Big( \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\bm{x}_i\Big)=\sum_{i=1}^N\mathrm{tr}\Big( \bm{x}_i\big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\Big)\\&amp;\Longrightarrow \frac{\partial \ell}{\partial \bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}\end{align}$$也就是说我们有梯度$$\begin{align}\nabla_{\bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}=\sum_{i=1}^N \big[\bm{y}_i- \bm{\mu}_i\big]\bm{x}_i^\text{T}\end{align}$$ 2.3、多元logistic回归的海赛矩阵注意到梯度的维数是 $\displaystyle D\times D$，应用 $\displaystyle \frac{\partial \big[a(\bm{x})\bm{f}(\bm{x})\big]}{\partial \bm{x}^\text{T}}=a\frac{\partial \bm{f}}{\partial \bm{x}^\text{T}}+\bm{f}\frac{\partial a}{\partial \bm{x}^\text{T}}$，于是我们注意到有如下结论$$\begin{align}\dot{\mathcal{S}}&amp;=\frac{\partial \mathcal{S}\big(\bm{\eta}\big)}{\partial \bm{\eta}^\text{T}}=\frac{1}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]-\mathrm{e}^\bm{\eta}\left[\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]\bm{I}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}\right]^\text{T}\\&amp;=\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}-\frac{ \mathrm{e}^\bm{\eta}\big[\mathrm{e}^\bm{\eta}\big]^\text{T}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}=\mathrm{diag}\big[\bm{\mu}\big]-\bm{\mu}\bm{\mu}^\text{T}\end{align}$$现在我们更进一步求这个矩阵梯度的微分，同时注意到有( $\displaystyle \mathrm{vec}\big(\bm{A}\bm{X}\bm{B}\big)=\big[\bm{B}^\text{T}\otimes \bm{A}\big]\mathrm{vec}\big(\bm{X}\big)$)，于是我们可以导出矩阵导数：$$\begin{align}&amp;\mathrm{d}\nabla=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{\eta}_i \cdot\bm{x}_i ^\text{T}=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\\\Longrightarrow&amp;\mathrm{vec}\big(\mathrm{d}\nabla\big)=\mathrm{vec}\bigg(\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\bigg)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N \mathrm{vec}\big(\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\big)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N\big[\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\big]\mathrm{d}\mathrm{vec}\big(\bm{W}\big)\\\Longrightarrow&amp;\bm{H}=\frac{\partial \mathrm{vec}\big(\nabla\big)}{\partial \mathrm{vec}^\text{T}\big(\bm{W}\big)}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{align}$$为了方便计算，我们把梯度也向量化，于是我们有$$\begin{align}\begin{cases}\displaystyle\nabla=\mathrm{vec}\big(\nabla_{\bm{W}}\big)=\sum_{i=1}^N\bm{x}_i \otimes\big[\bm{y}_i- \bm{\mu}_i\big]\\\displaystyle\bm{H}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{cases}\end{align}$$牛顿-拉弗迭代法有：$$\begin{align}\mathrm{vec}\big(\bm{W}\big):=\mathrm{vec}\big(\bm{W}\big)-\bm{H}^{-1}\nabla\end{align}$$当然各种优化方法都可以加以应用。 三、评述1、分类分布，有人又叫多项式分布，这里n=1，不过我不太赞成这个说法，使用分类分布更加恰当。当然更萌(๑•ᴗ•๑)一点可以叫猫猫分布。2、分类分布是一种很特殊的分布：对任意一个连续分布，如果我们将其离散化，我们都可以归结为一个分类分布。3、分类问题可以归结为：对特征向量分类，或者说对特征空间进行划分。由于与分类分布的关系使得 softmax函数经常出现，一般把它放在神经网络的最后一层来输出概率。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>分类分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac安装tensorflow1.2.0rc2与tensorboard]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2017-05-05-mac%E5%AE%89%E8%A3%85tensorflow1-2-0rc2%E4%B8%8Etensorboard%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2017-05-05-mac安装tensorflow1-2-0rc2与tensorboard/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、前期准备首先你应该去安装anaconda：https://www.continuum.io/downloads/，这个非常容易，和平常安装软件是一样的。然后，我们建议你最好安装Oh My Zsh：http://ohmyz.sh。 这是一个终极shell工具。当然如果你从来不知道命令行为何物，理解可能需要花点时间。 首先卸载：1➜ anaconda sudo pip uninstall tensorflow 1、安装Java SE Development Kit2、切换到根目录usr，安装Homebrew123➜ /usr bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"➜ /usr brew update 3、然后安装bazel。1➜ /usr brew install bazel 二、开始编译安装1、指定目录1➜ ~ cd anaconda 2、克隆下来1➜ anaconda git clone https://github.com/tensorflow/tensorflow 3、指定版本12➜ ~ cd anaconda/tensorflow➜ tensorflow git checkout r1.2.0rc2 4、配置tensorflow1➜ tensorflow git:(ce1d6ec49) ./configure 5、生成一个pip的安装包,mac和lunix系统，默认会编译指令集，win的好像要手动设置1➜ tensorflow git:(ce1d6ec49) sudo bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package 6、创建whl安装包1➜ tensorflow git:(ce1d6ec49) bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 7、用pip安装whl安装包1➜ anaconda sudo pip install /tmp/tensorflow_pkg/tensorflow-1.2.0rc2-cp36-cp36m-macosx_10_7_x86_64.whl 注意：win的好像要手动设置1bazel build -c opt --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package 在GitHub上，有人编译好了，地址是 https://github.com/lakshayg/tensorflow-build 三、运行一个经典的手写识别例子打开终端,cd到指定目录。123456789101112131415161718192021➜ ~ cd anaconda➜ anaconda python tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.pySuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gzSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gzSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gzSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gzAccuracy at step 0: 0.1209Accuracy at step 10: 0.6857Accuracy at step 20: 0.839......Accuracy at step 970: 0.9693Accuracy at step 980: 0.97Accuracy at step 990: 0.9694Adding run metadata for 999➜ anaconda tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summariesStarting TensorBoard b'54' at http://xiaobai-mac.local:6006(Press CTRL+C to quit) 在浏览器中打开如下： 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2017-05-05-mac安装tensorflow1-2-0rc2与tensorboard/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下Tensorflow的安装]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2017-05-04-Mac%E4%B8%8BTensorflow%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2017-05-04-Mac下Tensorflow的安装/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、用conda创建tensorflow1、库源问题参照官方说明我们在anaconda环境下，用conda来安装tensorflow。结果遇到源无法连接的情况。所谓源的概念请参考iphone越狱你添加软件源的情形。 12345678910111213141516171819202122232425➜ ~ cd anaconda➜ anaconda sudo conda create -n tensorflowPassword:Fetching package metadata ....WARNING: The remote server could not find the noarch directory for therequested channel with url: http://pypi.douban.comIt is possible you have given conda an invalid channel. Please double-checkyour conda configuration using `conda config --show`.If the requested url is in fact a valid conda channel, please request that thechannel administrator create `noarch/repodata.json` and associated`noarch/repodata.json.bz2` files, even if `noarch/repodata.json` is empty.$ mkdir noarch$ echo '&#123;&#125;' &gt; noarch/repodata.json$ bzip2 -k noarch/repodata.json.CondaHTTPError: HTTP None None for url &lt;http://pypi.python.com/osx-64/repodata.json&gt;Elapsed: NoneAn HTTP error occurred when trying to retrieve this URL.HTTP errors are often intermittent, and a simple retry will get you on your way.ConnectionError(MaxRetryError("HTTPConnectionPool(host='pypi.python.com', port=80): Max retries exceeded with url: /osx-64/repodata.json (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x1119ee710&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",),)➜ ~ cd anaconda 2、然后我们这样解决修改源地址，主要在 “.condarc”这个文件里面。我们添加清华源：https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/1234567891011121314151617181920➜ ~ cd anaconda➜ anaconda sudo conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/Password:➜ anaconda sudo conda config --set show_channel_urls yes➜ anaconda sudo conda create -n tensorflowFetching package metadata .....Solving package specifications:Package plan for installation in environment /Users/xiaobai/anaconda/envs/tensorflow:Proceed ([y]/n)? y## To activate this environment, use:# &gt; source activate tensorflow## To deactivate this environment, use:# &gt; source deactivate tensorflow#➜ ~ cd anaconda 我们最好是直接去操作 “.condarc”的文件，将地址改为国内的地址。然后sudo conda config –set show_channel_urls yes。 3、然后进入我们创建的tensorflow的环境，来安装tensorflow。1234567891011121314151617181920212223242526272829303132333435➜ anaconda source activate tensorflow(tensorflow) ➜ anaconda sudo pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.1.0-py3-none-any.whlThe directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting tensorflow==1.1.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.1.0-py3-none-any.whl Downloading https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.1.0-py3-none-any.whl (30.9MB) 100% |████████████████████████████████| 30.9MB 21kB/sCollecting six&gt;=1.10.0 (from tensorflow==1.1.0) Downloading six-1.10.0-py2.py3-none-any.whlCollecting wheel&gt;=0.26 (from tensorflow==1.1.0) Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB) 100% |████████████████████████████████| 71kB 156kB/sCollecting werkzeug&gt;=0.11.10 (from tensorflow==1.1.0) Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312kB) 100% |████████████████████████████████| 317kB 140kB/sCollecting numpy&gt;=1.11.0 (from tensorflow==1.1.0) Downloading numpy-1.12.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.4MB) 100% |████████████████████████████████| 4.4MB 49kB/sCollecting protobuf&gt;=3.2.0 (from tensorflow==1.1.0) Downloading protobuf-3.3.0.tar.gz (271kB) 100% |████████████████████████████████| 276kB 97kB/sCollecting setuptools (from protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0) Downloading setuptools-35.0.2-py2.py3-none-any.whl (390kB) 100% |████████████████████████████████| 399kB 121kB/sCollecting appdirs&gt;=1.4.0 (from setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0) Downloading appdirs-1.4.3-py2.py3-none-any.whlCollecting packaging&gt;=16.8 (from setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0) Downloading packaging-16.8-py2.py3-none-any.whlCollecting pyparsing (from packaging&gt;=16.8-&gt;setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0) Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56kB) 100% |████████████████████████████████| 61kB 56kB/sInstalling collected packages: six, wheel, werkzeug, numpy, appdirs, pyparsing, packaging, setuptools, protobuf, tensorflow Running setup.py install for protobuf ... doneSuccessfully installed appdirs-1.4.3 numpy-1.12.1 packaging-16.8 protobuf-3.3.0 pyparsing-2.2.0 setuptools-35.0.2 six-1.10.0 tensorflow-1.1.0 werkzeug-0.12.2 wheel-0.29.0➜ anaconda 4、测试成功不过一些加快cpu指令不能执行，因为我们使用的pip的安装方式，为了填这个坑，不得不卸载，采用源安装。。。。坑啊！白激动了。1234567891011121314➜ anaconda pythonPython 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()2017-05-20 23:19:36.415562: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.2017-05-20 23:19:36.415585: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.2017-05-20 23:19:36.415591: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.2017-05-20 23:19:36.415596: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.2017-05-20 23:19:36.415600: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.&gt;&gt;&gt; print(sess.run(hello))b'Hello, TensorFlow!' 二、源码安装tensorflow1、现在我们要卸载tensorflow12345678910➜ anaconda sudo pip uninstall tensorflow /Users/xiaobai/anaconda/lib/python3.6/site-packages/tensorflow/tools/tfprof/__pycache__/tfprof_output_pb2.cpython-36.pyc .... /Users/xiaobai/anaconda/lib/python3.6/site-packages/tensorflow/tools/tfprof/tfprof_log_pb2.py /Users/xiaobai/anaconda/lib/python3.6/site-packages/tensorflow/tools/tfprof/tfprof_options_pb2.py /Users/xiaobai/anaconda/lib/python3.6/site-packages/tensorflow/tools/tfprof/tfprof_output_pb2.pyProceed (y/n)? y Successfully uninstalled tensorflow-1.1.0The directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.➜ anaconda 2、安装Java SE Development Kit3、切换到根目录usr，安装Homebrew123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104➜ /usr bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"==&gt; This script will install:/usr/local/bin/brew/usr/local/share/doc/homebrew/usr/local/share/man/man1/brew.1/usr/local/share/zsh/site-functions/_brew/usr/local/etc/bash_completion.d/brew/usr/local/Homebrew==&gt; The following new directories will be created:/usr/local/Homebrew/usr/local/FrameworksPress RETURN to continue or any other key to abort==&gt; /usr/bin/sudo /bin/mkdir -p /usr/local/Homebrew /usr/local/FrameworksPassword:==&gt; /usr/bin/sudo /bin/chmod g+rwx /usr/local/Homebrew /usr/local/Frameworks==&gt; /usr/bin/sudo /bin/chmod 755 /usr/local/share/zsh /usr/local/share/zsh/site-functions==&gt; /usr/bin/sudo /usr/sbin/chown xiaobai /usr/local/Homebrew /usr/local/Frameworks==&gt; /usr/bin/sudo /usr/bin/chgrp admin /usr/local/Homebrew /usr/local/Frameworks==&gt; /usr/bin/sudo /bin/mkdir -p /Library/Caches/Homebrew==&gt; /usr/bin/sudo /bin/chmod g+rwx /Library/Caches/Homebrew==&gt; /usr/bin/sudo /usr/sbin/chown xiaobai /Library/Caches/Homebrew==&gt; Downloading and installing Homebrew...remote: Counting objects: 4267, done.remote: Compressing objects: 100% (2809/2809), done.remote: Total 4267 (delta 2206), reused 2751 (delta 1310), pack-reused 0Receiving objects: 100% (4267/4267), 2.44 MiB | 54.00 KiB/s, done.Resolving deltas: 100% (2206/2206), done.From https://github.com/Homebrew/brew * [new branch] master -&gt; origin/master * [new tag] 0.1 -&gt; 0.1 * [new tag] 0.2 -&gt; 0.2 * [new tag] 0.3 -&gt; 0.3 * [new tag] 0.4 -&gt; 0.4 * [new tag] 0.5 -&gt; 0.5 * [new tag] 0.6 -&gt; 0.6 * [new tag] 0.7 -&gt; 0.7 * [new tag] 0.7.1 -&gt; 0.7.1 * [new tag] 0.8 -&gt; 0.8 * [new tag] 0.8.1 -&gt; 0.8.1 * [new tag] 0.9 -&gt; 0.9 * [new tag] 0.9.1 -&gt; 0.9.1 * [new tag] 0.9.2 -&gt; 0.9.2 * [new tag] 0.9.3 -&gt; 0.9.3 * [new tag] 0.9.4 -&gt; 0.9.4 * [new tag] 0.9.5 -&gt; 0.9.5 * [new tag] 0.9.8 -&gt; 0.9.8 * [new tag] 0.9.9 -&gt; 0.9.9 * [new tag] 1.0.0 -&gt; 1.0.0 * [new tag] 1.0.1 -&gt; 1.0.1 * [new tag] 1.0.2 -&gt; 1.0.2 * [new tag] 1.0.3 -&gt; 1.0.3 * [new tag] 1.0.4 -&gt; 1.0.4 * [new tag] 1.0.5 -&gt; 1.0.5 * [new tag] 1.0.6 -&gt; 1.0.6 * [new tag] 1.0.7 -&gt; 1.0.7 * [new tag] 1.0.8 -&gt; 1.0.8 * [new tag] 1.0.9 -&gt; 1.0.9 * [new tag] 1.1.0 -&gt; 1.1.0 * [new tag] 1.1.1 -&gt; 1.1.1 * [new tag] 1.1.2 -&gt; 1.1.2 * [new tag] 1.1.3 -&gt; 1.1.3 * [new tag] 1.1.4 -&gt; 1.1.4 * [new tag] 1.1.5 -&gt; 1.1.5HEAD is now at 7000f50 Merge pull request #2646 from sjackman/replace_text_in_files==&gt; Tapping homebrew/coreCloning into '/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core'...&lt;!-- 这个位置会等待非常长非常长的时间，请耐心等待，然后我们看到这个： --&gt;remote: Counting objects: 4419, done.remote: Compressing objects: 100% (4225/4225), done.remote: Total 4419 (delta 37), reused 473 (delta 14), pack-reused 0Receiving objects: 100% (4419/4419), 3.52 MiB | 5.00 KiB/s, done.Resolving deltas: 100% (37/37), done.Tapped 4224 formulae (4,462 files, 11.0MB)==&gt; Cleaning up /Library/Caches/Homebrew...==&gt; Migrating /Library/Caches/Homebrew to /Users/xiaobai/Library/Caches/Homebrew==&gt; Deleting /Library/Caches/Homebrew...Already up-to-date.Error: Could not link:/usr/local/etc/bash_completion.d/brewPlease delete these paths and run `brew update`.Error: Could not link:/usr/local/share/zsh/site-functions/_brewPlease delete these paths and run `brew update`.Error: Could not link:/usr/local/share/man/man1/brew.1Please delete these paths and run `brew update`.Error: Could not link:/usr/local/share/doc/homebrewPlease delete these paths and run `brew update`.==&gt; Installation successful!==&gt; Homebrew has enabled anonymous aggregate user behaviour analytics.Read the analytics documentation (and how to opt-out) here: http://docs.brew.sh/Analytics.html==&gt; Next steps:- Run `brew help` to get started- Further documentation: http://docs.brew.sh➜ /usr 4、升级brew，完成安装按照提示，我们删除对应的文件和文件夹，然后运行：123➜ /usr brew updateAlready up-to-date.➜ /usr 5、然后我们就可以安装bazel了。123456789101112131415161718192021➜ /usr brew install bazel==&gt; Downloading https://homebrew.bintray.com/bottles/bazel-0.4.5.sierra.bottle.t######################################################################## 100.0%==&gt; Pouring bazel-0.4.5.sierra.bottle.tar.gz==&gt; Using the sandbox==&gt; CaveatsBash completion has been installed to: /usr/local/etc/bash_completion.dzsh completions have been installed to: /usr/local/share/zsh/site-functions==&gt; Summary🍺 /usr/local/Cellar/bazel/0.4.5: 11 files, 137.4MB➜ /usr bazel versionExtracting Bazel installation...Build label: 0.4.5-homebrewBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jarBuild time: Thu Mar 16 13:39:33 2017 (1489671573)Build timestamp: 1489671573Build timestamp as int: 1489671573➜ /usr 6、安装python依赖包，通常已经安装了,如果没有请定位到anaconda下安装123➜ ~ cd anaconda➜ anaconda sudo pip install six numpy wheel➜ ~ cd anaconda 7、下面我们克隆tensorflow库到本地12345678➜ ~ cd anaconda➜ anaconda git clone https://github.com/tensorflow/tensorflowCloning into 'tensorflow'...remote: Counting objects: 187859, done.remote: Total 187859 (delta 0), reused 0 (delta 0), pack-reused 187859Receiving objects: 100% (187859/187859), 103.48 MiB | 347.00 KiB/s, done.Resolving deltas: 100% (143705/143705), done.➜ ~ cd anaconda 7、配置tensorflow注意我对每一个项目配置的选择。12345678910111213141516171819202122232425262728➜ ~ cd anaconda➜ anaconda cd tensorflow➜ tensorflow git:(master) ./configurePlease specify the location of python. [Default is /Users/xiaobai/anaconda/bin/python]:Found possible Python library paths: /Users/xiaobai/anaconda/lib/python3.6/site-packagesPlease input the desired Python library path to use. Default is [/Users/xiaobai/anaconda/lib/python3.6/site-packages]Using python library path: /Users/xiaobai/anaconda/lib/python3.6/site-packagesDo you wish to build TensorFlow with MKL support? [y/N] nNo MKL support will be enabled for TensorFlowPlease specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native]:Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] yGoogle Cloud Platform support will be enabled for TensorFlowDo you wish to build TensorFlow with Hadoop File System support? [y/N] yHadoop File System support will be enabled for TensorFlowDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] nNo XLA JIT support will be enabled for TensorFlowDo you wish to build TensorFlow with VERBS support? [y/N] nNo VERBS support will be enabled for TensorFlowDo you wish to build TensorFlow with OpenCL support? [y/N] nNo OpenCL support will be enabled for TensorFlowDo you wish to build TensorFlow with CUDA support? [y/N] nNo CUDA support will be enabled for TensorFlow...........................INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.Configuration finished➜ tensorflow git:(master) 8、生成一个pip的安装包如果你的网络环境不错，这会是个漫长的等待。 如果网络环境不好，请安装提示手动下载，移到指定目录，然后再次运行命令！因为中国网络环境很那个，你懂的！ 你可能需要反复操作几次，切忌烦躁，耐心操作。 最后编译，将会消耗大量计算资源，并会有警告信息。123456789101112131415161718192021222324➜ tensorflow git:(master) sudo bazel build --config=opt //tensorflow/tools/pip_package:build_pip_packagePassword:INFO: Downloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53\787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 764,355 bytes...tensorflow/core/kernels/hexagon/hexagon_control_wrapper.cc:35:12: warning: unused variable 'DBG_DUMP_INPUT_TENSOR_AS_FLOAT_DATA' [-Wunused-const-variable]const bool DBG_DUMP_INPUT_TENSOR_AS_FLOAT_DATA = false; ^In file included from tensorflow/core/kernels/hexagon/hexagon_control_wrapper.cc:16:./tensorflow/core/kernels/hexagon/hexagon_control_wrapper.h:56:38: warning: private field 'execute_info_' is not used [-Wunused-private-field] const RemoteFusedGraphExecuteInfo* execute_info_&#123;&#125;; ^9 warnings generated.INFO: From Linking tensorflow/cc/libcc_ops_internal.pic.lo:...l-py3-opt/bin/external/grpc/libgrpc_unsecure.pic.a(census.pb.pic.o))ld: warning: pointer not aligned at address 0x2C494A6 (_grpc_lb_v1_LoadBalanceRequest_fields + 6 from bazel-out/local-py3-opt/bin/external/grpc/libgrpc_unsecure.pic.a(load_balancer.pb.pic.o))ld: warning: pointer not aligned at address 0x2C49506 (_grpc_lb_v1_ServerList_fields + 6 from bazel-out/local-py3-opt/bin/external/grpc/libgrpc_unsecure.pic.a(load_balancer.pb.pic.o))ld: warning: pointer not aligned at address 0x2C49536 (_grpc_lb_v1_LoadBalanceResponse_fields + 6 from bazel-out/local-py3-opt/bin/external/grpc/libgrpc_unsecure.pic.a(load_balancer.pb.pic.o))Target //tensorflow/tools/pip_package:build_pip_package up-to-date: bazel-bin/tensorflow/tools/pip_package/build_pip_packageINFO: Elapsed time: 4804.411s, Critical Path: 1838.71s➜ tensorflow git:(master) 9、创建whl安装包上述命令会生成一个叫做build_pip_package的脚本，按照如下命令运行这个脚本，在/tmp/tensorflow_pkg文件夹中创建”.whl”的文件。也就说用上一步的脚本创建一个whl安装包 123456789101112➜ tensorflow git:(master) sudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkgPassword:2017年 5月21日 星期日 16时41分34秒 CST : === Using tmpdir: /var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/tmp.XXXXXXXXXX.gPUDRm6k~/anaconda/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/anaconda/tensorflow~/anaconda/tensorflow/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/tmp.XXXXXXXXXX.gPUDRm6k ~/anaconda/tensorflow2017年 5月21日 星期日 16时41分38秒 CST : === Building wheelwarning: no files found matching '*.dll' under directory '*'warning: no files found matching '*.lib' under directory '*'~/anaconda/tensorflow2017年 5月21日 星期日 16时41分59秒 CST : === Output wheel file is in: /tmp/tensorflow_pkg➜ tensorflow git:(master) 10、用pip安装whl安装包退出tensorflow环境，进入anaconda环境。注意你的whl文件名，我的文件名是：tensorflow-1.1.0-cp36-cp36m-macosx_10_7_x86_64.whl12345678910111213141516171819202122232425➜ anaconda sudo pip install /tmp/tensorflow_pkg/tensorflow-1.1.0-cp36-cp36m-macosx_10_7_x86_64.whlThe directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Processing /tmp/tensorflow_pkg/tensorflow-1.1.0-cp36-cp36m-macosx_10_7_x86_64.whlRequirement already satisfied: six&gt;=1.10.0 in ./lib/python3.6/site-packages (from tensorflow==1.1.0)Requirement already satisfied: wheel&gt;=0.26 in ./lib/python3.6/site-packages (from tensorflow==1.1.0)Requirement already satisfied: werkzeug&gt;=0.11.10 in ./lib/python3.6/site-packages (from tensorflow==1.1.0)Collecting html5lib==0.9999999 (from tensorflow==1.1.0) Downloading html5lib-0.9999999.tar.gz (889kB) 100% |████████████████████████████████| 890kB 786kB/sRequirement already satisfied: numpy&gt;=1.11.0 in ./lib/python3.6/site-packages (from tensorflow==1.1.0)Requirement already satisfied: protobuf&gt;=3.2.0 in ./lib/python3.6/site-packages (from tensorflow==1.1.0)Collecting markdown==2.2.0 (from tensorflow==1.1.0) Downloading Markdown-2.2.0.tar.gz (236kB) 100% |████████████████████████████████| 245kB 2.7MB/sCollecting bleach==1.5.0 (from tensorflow==1.1.0) Downloading bleach-1.5.0-py2.py3-none-any.whlRequirement already satisfied: setuptools in ./lib/python3.6/site-packages (from protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0)Requirement already satisfied: appdirs&gt;=1.4.0 in ./lib/python3.6/site-packages (from setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0)Requirement already satisfied: packaging&gt;=16.8 in ./lib/python3.6/site-packages (from setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0)Requirement already satisfied: pyparsing in ./lib/python3.6/site-packages (from packaging&gt;=16.8-&gt;setuptools-&gt;protobuf&gt;=3.2.0-&gt;tensorflow==1.1.0)Installing collected packages: html5lib, markdown, bleach, tensorflow Running setup.py install for html5lib ... done Running setup.py install for markdown ... doneSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 tensorflow-1.1.0 11、安装成功，指令集的错误没有了，噢耶！12345678910➜ anaconda pythonPython 3.6.0 |Anaconda custom (x86_64)| (default, Dec 23 2016, 13:19:00)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello).decode('utf-8'))Hello, TensorFlow!&gt;&gt;&gt; 三、评述这样我们就安装好了tensorflow。坑不少，但是安装官方help基本没有问题。我们来总结一下tensorflowr1.2.0rc2的编译安装： 1、安装Java SE Development Kit2、切换到根目录usr，安装Homebrew➜ /usr bin/ruby -e “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;➜ /usr brew update 3、然后安装bazel。➜ /usr brew install bazel 1、指定目录➜ ~ cd anaconda 2、克隆下来➜ anaconda git clone https://github.com/tensorflow/tensorflow 3、指定版本➜ ~ cd anaconda/tensorflow➜ tensorflow git checkout r1.2.0rc2 4、配置tensorflow➜ tensorflow git:(ce1d6ec49) ./configure 4、生成一个pip的安装包,mac和lunix系统，默认会编译指令集，win的好像要手动设置➜ tensorflow git:(ce1d6ec49) sudo bazel build –config=opt //tensorflow/tools/pip_package:build_pip_package 5、创建whl安装包➜ tensorflow git:(ce1d6ec49) bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 6、用pip安装whl安装包➜ anaconda sudo pip install /tmp/tensorflow_pkg/tensorflow-1.2.0rc2-cp36-cp36m-macosx_10_7_x86_64.whl 注意：win的好像要手动设置bazel build -c opt –copt=-mavx –copt=-msse4.1 –copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package 在GitHub上，有人编译好了，地址是 https://github.com/lakshayg/tensorflow-build 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2017-05-04-Mac下Tensorflow的安装/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>工程问题</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-04-05-%E6%84%9F%E7%9F%A5%E5%99%A8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A013%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习13/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 神经网络神经网络是由简单处理单元构成的大规模并行分布式处理器。 神经元 $$\begin{align}o_j(t)=f\{[\sum_{i=1}^{n}w_{ij}x_i(t-\tau_{ij})]-T_j\}\end{align}$$ 单神经元—感知机模型（Perceptron）模型的建立1943年生理学家W.S.McCulloch和W.A.Pitts提出了形式神经元数学模型，史称M-P模型。开创了神经科学理论研究的新时代！ 1949年心理学家Donald Olding Hebb在《行为构成》（Organization of Behavior）中提出了Hebb算法。而且首次提出了连接主义(connectionism)：大脑活动是靠脑细胞的组合连接实现的。 1958年美国Frank Rosenblatt提出了感知机（Perceptron）这应该是世界上第一个真正优秀的人工神经网络 概念与符号下面我们将说明这个模型，1、 $\displaystyle \boldsymbol{x}=[x_1,…,x_k]^\text{T}$为特征向量， 或者叫输入向量2、 $\displaystyle \boldsymbol{w}=[w_1,…,w_k]^\text{T}$为权值向量，也可以叫突触权值向量 ，模仿神经元的突触。3、 $\displaystyle w_0$为阈值。 $\displaystyle o=f$4、为简洁记令 $\displaystyle x_0=1$,于是有增广特征向量 $\displaystyle \boldsymbol{x}:=[x_0,x_1,…,x_k]^\text{T}$。5、增广权值向量$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$。6、定义输出 $\displaystyle o$，7、激活函数：硬限函数 $\displaystyle \mathrm{hardlims}(x)=\begin{cases} 1, &amp;x\geqslant0 \\\ -1, &amp;x&lt;0\end{cases}$。于是有：$$\begin{align}o=\mathrm{hardlims}\left(\boldsymbol{w}^\text{T}\boldsymbol{x}\right)\end{align}$$ 神经元 感知机学习与训练感知机学习算法：误差修正学习算法对于二分类增广数据集 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$，注意我们用的都是增广数据，$\displaystyle \boldsymbol{x}:=[1,x_1,…,x_k]^\text{T}$，$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$，同时 $\displaystyle y=+1\text{ or }-1$，表示 $\displaystyle c_1 \text{ or } c_2$类。 学习误差$\displaystyle y_t-o_t$于是有： 算法：误差修正学习算法1 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\eta_t(y_t-o_t)\boldsymbol{x}_t\end{array}\\$3 #end while 如果我们定义 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样取得新的数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$，一般称之为二分类规范增广数据集。在这个数据集下，线性可分意味着：存在一个 $\displaystyle \boldsymbol{w}$使得对于任意的 $\displaystyle \boldsymbol{z}_i$，$\displaystyle \boldsymbol{w}\boldsymbol{z}_i&gt;0$成立。 算法：误差修正学习算法2 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+2\eta_t\mathbb{I}(\boldsymbol{w}_t^\text{T} \boldsymbol{z}\leqslant 0)\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：梯度下降算法分析之前，令 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样我们把属于 $\displaystyle c_2$的特征变成了 $\displaystyle -\boldsymbol{x}$。定义目标函数： $\displaystyle J(\boldsymbol{w})=k \left(|\boldsymbol{w}^\text{T}\boldsymbol{z}|-\boldsymbol{w}^\text{T}\boldsymbol{z}\right),k&gt;0$ 通常令 $\displaystyle k=1$易知： $\displaystyle \min J(\boldsymbol{w})=0$时， $\displaystyle \boldsymbol{w}^\text{T}\boldsymbol{z}\geqslant 0$，于是分类正确。问题转化为：$$\begin{align}\min_{\boldsymbol{w}}J(\boldsymbol{w})\end{align}$$我们使用梯度下降算法：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_t\nabla_t\end{align}$$ 我们知道： $\displaystyle \nabla_t=\frac{\partial J}{\partial\boldsymbol{w}_t}=k[\boldsymbol{z}_t\times\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-\boldsymbol{z}_t]$ 代入3式得：$$\begin{align} \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t \end{align}$$当 $\displaystyle k=1$时，就是误差修正学习算法。 算法：梯度下降算法（Gradient Descent） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：最小均方误差算法取任意正数 $\displaystyle \nu_i$，有 $\displaystyle \boldsymbol{\nu}=[\nu_1,…,\nu_k]^\text{T}$。定义误差： $\displaystyle \boldsymbol{\epsilon}=\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}$ $$\begin{align} \mathrm{MES}(\boldsymbol{w}) =\boldsymbol{\epsilon}^\text{T}\boldsymbol{\epsilon} =\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right] \end{align}$$容易知道问题化为$$\begin{align}\min_{\boldsymbol{w}} \mathrm{MES}(\boldsymbol{w})\end{align}$$知道这就最小二乘解： $\displaystyle \hat{\boldsymbol{w}}=\left[\boldsymbol{X}^\text{T}\boldsymbol{X}\right]^{-1}\boldsymbol{X}^\text{T}\boldsymbol{\nu}=\boldsymbol{X}^\dagger \boldsymbol{\nu}$。求解伪逆的计算量大。我们使用梯度下降方法，知道$$\begin{align}\nabla\mathrm{MES}(\boldsymbol{w})=2\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]\end{align}$$ 算法：最小均方误差算法（Least Mean-Square Error） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{X}^\text{T}\boldsymbol{\nu},\boldsymbol{\nu}&gt;0$$2\displaystyle \alpha_0=\alpha$$3\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\alpha_t=\alpha\div t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-2\alpha_t\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}_t-\boldsymbol{\nu}\right]\\\end{array}\\$4 # end while$5\displaystyle \text{ if }\,\boldsymbol{\epsilon}_{end}=\boldsymbol{X}\boldsymbol{w}_{end}-\boldsymbol{\nu}\geqslant 0$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性可分’)}\\\end{array}$$\displaystyle \,\,\,\,\,\text{else }$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性不可分’)}\\\end{array}$6 # end if 感知器收敛定律如果样本集合线性可分，那么感知器存在收敛解。证明：1、我们使用规范增广数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n} $2、假定解向量 $\displaystyle \hat{\boldsymbol{w}}$，则对任意的 $\displaystyle \boldsymbol{z}$有 $\displaystyle \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}&gt;0$。我们注意到对于 $\displaystyle \delta&gt;0$ ，$\displaystyle \delta\cdot\hat{\boldsymbol{w}}$也是解向量。3、我们令 $\displaystyle \boldsymbol{z}_t$是所有错分样本， 有$$\begin{align}\boldsymbol{w}_t\boldsymbol{z}_t\leqslant 0\end{align}$$误差修正学习算法中令 $\displaystyle \eta_t=\frac{1}{2}$于是有：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\end{align}$$另外我们还令规范增广特征向量最大长度，与解向量最小内积$$\begin{align}\beta=\max_t ||\,\boldsymbol{z}_t||\end{align}$$$$\begin{align}\alpha=\min_t \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t&gt;0\end{align}$$4、现在我们把解向量 $\displaystyle \delta\hat{\boldsymbol{w}}$考虑进来于是：$$\begin{align}&amp;\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\\&amp;\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}=\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t\\&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t||^2\end{align}$$5、范数公式 $\displaystyle ||\boldsymbol{x}+\boldsymbol{y}||^2=||\boldsymbol{x}||^2+ 2&lt;\boldsymbol{x},\boldsymbol{y}&gt;+||\boldsymbol{y||^2},\,&lt;\boldsymbol{x},\boldsymbol{y}&gt;=\boldsymbol{x}^\text{T}\boldsymbol{y}$于是我们有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2+2 \boldsymbol{w}_t^\text{T}\boldsymbol{z}_t-2\delta\cdot\hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t+||\boldsymbol{z}_t||^2\end{align}$$6、根据上面的1、2、3的分析，有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-2\delta\cdot\alpha+\beta^2\end{align}$$7、为了简洁令： $\displaystyle \delta=\frac{\beta^2}{\alpha}$于是有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-\beta^2\end{align}$$8、考虑上式 $\displaystyle t=\{0,…,m\}$。并累加得：$$\begin{align}&amp;0\leqslant||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2-m\cdot\beta^2\end{align}$$9、可以看到随着 $\displaystyle m$的不断增加，范数 $\displaystyle ||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\to 0$，于是：$$\begin{align}m_{max}= \frac{||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2}{\beta^2}\end{align}$$10、如果我们令 $\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$,有：$$\begin{align}m_{max}=\frac{\delta^2}{\beta^2}||\,\hat{\boldsymbol{w}}||=\frac{\beta^2}{\alpha^2}||\,\hat{\boldsymbol{w}}||\end{align}$$这时感知机的解收敛于： $\displaystyle \frac{\beta^2}{\alpha}\cdot\hat{\boldsymbol{w}}$。史称感知机固定增量收敛定律。#### 评述1、 历史说明：分类是科学之始，为解决分类问题，人类殚精竭虑。其中模仿神经元的感知器就是其中之一。2、上帝给世界分了两类 $\displaystyle c_1$和 $\displaystyle c_2$。人类观测到了 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$。为了解决问题，人类思考了最简单的方法：劈下一刀，不就两半了。于是有了对称硬限函数。3、有个负号还是很麻烦，于是定义了 $\displaystyle \boldsymbol{z}=y\times \boldsymbol{x}$ 就有了新的数据集$\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$ 劈下一刀，变成了的折一下，然后裁一刀的问题：$$\begin{align} \forall \boldsymbol{z}_i,\boldsymbol{w}^\text{T}\boldsymbol{z}_i&gt;0\end{align}$$4、满足上式的问题，我们叫线性可分。并且根据学习规则和对问题认识，人类很块发现了感知机固定增量收敛定律。5、但是好景不长，人类的智者很快发现 线性不过是人类的YY，非线性才是上帝的YY。人类继续着征程： 我们的征途是星辰大海！ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习13/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>感知机</tag>
        <tag>人工神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[狄利克雷-多项式模型(Dirichlet-Multionmial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-08-%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 简介: 本文总结了狄利克雷-多项式模型的部分和我自己的一些体会，我们将再一次熟悉贝叶斯方法的基本概念、流程、特点。把我们思维进化到更高的维度。摘要：本文意在狄利克雷-多项式模型的问题。若有错误，请大家指正。关键词: 狄利克雷-多项式模型,分类分布,狄利克雷分布 一、狄利克雷-多项式模型(dirichlet-multionmial model)分类分布(Categorical distribution) 或者又叫1-C分布(multinoulli distribution) $$\begin{align}\bm{x}\sim \mathrm{Cat}(\bm{x}\mid\bm{\mu})\end{align}$$其中 $\displaystyle \bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\\\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1$ 在这里有必要解释一下符号问题。首先分类分布是对0-1分布的推广，这句话可能不好理解。我们可以这样思考，0-1分布是抛硬币。而分类分布类似于掷骰子。为了让符号统一我们使用小写 $\displaystyle c$表示有C个分类，例如 $\displaystyle c=6$可以类比于掷骰子。这样很多问题就好理解了。变量 $\displaystyle \bm{x}=\begin{bmatrix} x_1\\\ \vdots \\x_j\\\vdots\\x_c \end{bmatrix}$ 实例或者一个观测 $\displaystyle \bm{x}_i=\begin{bmatrix} x_{i1}\\\ \vdots \\x_{ij}\\\vdots\\x_{ic} \end{bmatrix}$ 例子： $\displaystyle \bm{x}_s=\begin{bmatrix} 0\ \\\vdots \\1\\\vdots\\0 \end{bmatrix} $ 1、分类分布(multinoulli distribution)概率质量函数：$$\begin{align}\mathrm{PMF：} \mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{i=1}^{c}\mu_i^{x_i}\end{align}$$ 其中： $\displaystyle \bm{x}=[x_1,x_2,…,x_c]^\text{T}\,,\bm{\mu}=[\mu_1,\mu_2,…,\mu_c]^\text{T}\,,\bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\,,\sum_{i=1}^{c}\mu_i=\bm{I}^\text{T}\bm{\mu}=1$这个表示方法也称为 $\displaystyle 1\,of\,c$编码方法。这个方法方便计算。 其他表示方法：$\displaystyle \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)},x\in\{1,…,c\}$,这个示性函数表示方法，有重要应用,降低表示维度，节约了有限的数学符号。 2、均值与方差我们知道向量函数微分结果： $\displaystyle \bm{f}(\bm{x})=\mathrm{e}^{\bm{A}\bm{x}} $ 有 $\displaystyle \frac{\partial\bm{f}}{\partial \bm{x}^\text{T}}=\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\bm{A}$。且有： $\displaystyle f(\bm{x})=\bm{\mu}^\text{T}\mathrm{e}^{\bm{A}\bm{x}}$，易得：$$\begin{align} &amp;\frac{\partial{f}}{\partial{\bm{x}}} =\mathrm{diag}\left[\mathrm{e}^{\bm{A}\bm{x}}\right]\bm{A}\bm{\mu}\\ &amp;\frac{\partial^2{f}}{\partial{\bm{x}}^2} =\bm{A}^\text{T}\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\mathrm{diag}[\bm{\mu}]\bm{A}\end{align}$$ 我们又知道特征函数： $\displaystyle \varphi_\bm{x}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{x}}]=\sum_{\bm{I}^\text{T}\bm{x}=1}p_j\mathrm{e}^{\mathrm{i}t x_j}=\sum_{j=1}^{c}\mu_i\mathrm{e}^{\mathrm{i}t x_j}=\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}$。于是可分析得到分类分布 $\displaystyle \bm{x}\sim \mathrm{Cat}(\bm{x}\mid \bm{\mu})$的期望与方差(协方差矩阵)： $\displaystyle \mathrm{E}[\bm{x}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}\right]\bm{X}\bm{\mu}=\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{x}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{x}]\mathrm{E}^\text{T}[\bm{x}]$$\displaystyle =\left.\bm{X}^\text{T}\mathrm{diag}[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}]\mathrm{diag}[\bm{\mu}]\bm{X}\right| _{\bm{t}=\bm{0}}-\bm{\mu}\bm{\mu}^\text{T}\\=\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}$ 其中： $\displaystyle \bm{X}=\bm{I}_{c\times c}$是 $\displaystyle c\times c$维的单位矩阵。也就说遍历了 $\displaystyle \bm{x}$所有可能取值组成的矩阵。为了表示简洁，我们在其中选用了$\displaystyle c\times c$维的单位矩阵。 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^{n}$于是有似然函数$$\begin{align}\mathrm{L}(\bm{\mu})=p(\mathcal{D}\mid\bm{\mu})&amp;=\prod_{i=1}^{n}\prod_{j=1}^{c}\mu_j^{x_j}=\prod_{j=1}^{c}\mu_j^{\sum_{i=1}^{n}x_{ij}}=\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中： $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$表示第 $\displaystyle j$个分类在 $\displaystyle n$次观测中发生了 $\displaystyle k_j$次。同时我们知道 $\displaystyle \sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1,\,\sum_{j=1}^{c}k_j=\bm{I}^\text{T}\bm{k}=n$ 4、对数似然函数$$\begin{align}\ell(\bm{\mu},\lambda)&amp;\propto\ln\prod_{j=1}^{c}\mu_j^{k_j}+\lambda(\sum_{j=1}^{c}\mu_j-1)\\&amp;=\bm{k}^\text{T}\ln\bm{\mu}+\lambda[\bm{I}^\text{T}\bm{\mu}-1]\end{align}$$ 5、求极大似然估计$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}&amp;=\frac{\bm{k}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}&amp;=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$分析可得 $\displaystyle \bm{\mu}_{MLE}=-\frac{\bm{k}}{\lambda} $ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}-1=0$得： $\displaystyle \frac{\bm{I}^\text{T}\bm{k}}{\lambda}=-1$由于 $\displaystyle \bm{I}^\text{T}\bm{k}=n$可得：$$\begin{align}\lambda=-n\end{align}$$ $$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 其中 $\displaystyle \mu_j^{MLE}=\frac{k_j}{n},\,k_j=\sum_{i=1}^{n}x_{ij}$。 6、多项式分布我们定义 $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$是分类 $\displaystyle j$在 $\displaystyle n$次观测中发生的次数。同时令 $\displaystyle \bm{k}=[k_1,…,k_j,…,k_c]^\text{T}$ 则有：$$\begin{align}\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)=\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中 $\displaystyle \sum_{j=1}^{c}k_j=n\,,\sum_{j=1}^{c}\mu_j=1$ 我们有特征函数：$\displaystyle \varphi_\bm{k}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{k}}]=\sum_{\bm{I}^\text{T}\bm{k}=n}p_i\mathrm{e}^{\mathrm{i}t k_j}=\sum_{\bm{I}^\text{T}\bm{k}=n}\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\left(\mu_j\mathrm{e}^{\mathrm{i}t}\right)^{k_j}=\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^n$。于是可得多项式分布的均值与协方差矩阵 $\displaystyle \mathrm{E}[\bm{k}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=-(\mathrm{i})^2n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}=n\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{k}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{k}]\mathrm{E}^\text{T}[\bm{k}]$$=\left.n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\mathrm{diag}[\bm{\mu}]\right| _{\bm{t}=\bm{0}}+\left.n(n-1)\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-2}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\left[\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\right]^\text{T}\right| _{\bm{t}=\bm{0}}-n^2\bm{\mu}\bm{\mu}^\text{T}$$=n\mathrm{diag}[\bm{\mu}]+n(n-1)\bm{\mu}\bm{\mu}^\text{T}-n^2\bm{\mu}\bm{\mu}^\text{T}\\=n\left[\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}\right]$ 7、共轭先验分布把 $\displaystyle \bm{\mu}$作为变量。观察似然函数，我们知道$$\begin{align}p(\bm{\mu})\propto p(\mathcal{D}\mid\bm{\mu})= \prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 我们知道狄利克雷分布 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\Gamma(a_0)}{\Gamma(a_1)\Gamma(a_2)…\Gamma(a_c)}\prod_{j=1}^{c}\mu_j^{a_j-1}$。而这正是我们需要的共轭先验，即后验与先验有相同的函数形式：$$\begin{align}\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\end{align}$$ 其中： $\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1\,,a_0=\bm{I}^\text{T}\bm{a}=a_1+…+a_c,\,a_j&gt;0$ 利用 $\displaystyle \Gamma,\,\mathrm{B}$函数性质容易知道：$\displaystyle \mathrm{E}(\bm{\mu})=\frac{\bm{a}}{\bm{I}^\text{T}\bm{a}}=\frac{\bm{a}}{a_0}$ $\displaystyle \mathrm{cov}(\bm{\mu})=\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\bm{a}-\bm{1}}{a_0-c}$ 为了方便使用，我们把它写成离散形式：$\displaystyle \mathrm{E}(\mu_j)=\frac{a_j}{a_0}$ $\displaystyle \mathrm{var}(\mu_j)=\frac{(a_0-a_j)a_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{cov}(\mu_i,\mu_j)=\frac{-a_ia_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\mu_j]=\mathop{\mathrm{argmax}}_{\mu_j}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{a_i-1}{a_0-c}$ 8、后验分布我们用似然函数乘以狄利克雷先验得到后验分布：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})\propto\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\end{align}$$ 归一化得：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})\end{align}$$ 1、在线学习我们发现狄利克雷分布也具有再生性质，和在线性学习性质，我们假设，陆续观测到两个的数据集 $\displaystyle \mathcal{D}_1\,\mathrm{D}_2$。1、当我们观察到 $\displaystyle \mathcal{D}_1$时，有$$ p(\bm{\mu}\mid\mathcal{D}_1)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_1+\bm{a}) $$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\bm{\mu}) p(\bm{\mu}\mid\mathcal{D}_1) $$易得： $$\begin{align}p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_2+\bm{k}_1+\bm{a})\end{align}$$ 2、后验分析最大后验估计$$\begin{align}\bm{\mu}_{MAP}=\mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}\end{align}$$ 极大似然估计：$$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 后验均值:$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}\end{align}$$ 后验均值是先验均值和最大似然估计的凸组合。知道 $\displaystyle \bm{\mu}=\frac{\bm{a}}{a_0}$$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}=\frac{a_0}{n+a_0}\bm{\mu}_0+\frac{n}{n+a_0}\bm{\mu}_{MLE}\end{align}$$我们发现 $\displaystyle a_0$可以理解为先验对于后验的等价样本大小。 $\displaystyle \frac{a_0}{n+a_0}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计，知道 $\displaystyle \mathrm{mode}[\bm{\mu}_0]=\frac{\bm{a}-\bm{1}}{a_0-c}$$$\begin{align}\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}=\frac{a_0-c}{n+a_0-c}\mathrm{mode}[\bm{\mu}_0]+\frac{n}{n+a_0-c}\bm{\mu}_{MLE}\end{align}$$ 3、 拉格朗日乘数法首先为了让符号有意义我们定义向量点除运算 $\displaystyle \frac{1}{\bm{a}}=1./\bm{a}=[1/a_1,…,1/a_n]^\text{T}$下面我们用拉格朗日乘数法在推理一遍，我们知道 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1 $，于是有：$$\begin{align}\ell(\bm{\mu},\lambda)=\ln \prod_{j}^{c}\mu_j^{k_j+a_j-1}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)=[\bm{k}+\bm{a}-\bm{1}]^\text{T}\ln\bm{\mu}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)\end{align}$$求解得：$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}=\frac{\bm{k}+\bm{a}-\bm{1}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$ 知道： $\displaystyle \bm{\mu}_{MAP}=-\frac{\bm{k}+\bm{a}-\bm{1}}{\lambda}$ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1$得： $\displaystyle \lambda=n+a_0-c$$$\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c} $$ 写成离散形式有$$\begin{align}\mu_j^{MAP}=\frac{k_j+a_j-1}{n+a_0-c}\end{align}$$ 4、后验协方差矩阵$$\begin{align}\mathrm{cov}(\bm{\mu})=\frac{(n+a_0)\mathrm{diag}[\bm{k}+\bm{a}]-[\bm{k}+\bm{a}][\bm{k}+\bm{a}]^\text{T}}{(n+a_0)^2(n+a_0+1)}\end{align}$$当 N足够大时： $$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降 5、后验预测分布开始分析之前，我们回忆一下B函数：$\displaystyle \mathrm{B}(\bm{a})=\int_{\bm{x}\in [0,1]^c}x_i^{a_i-1}\mathrm{d}\bm{x}\,,a_i&gt;0\,and\,\sum_{i-1}^{n}x_i=1$且有： $\displaystyle \mathrm{B}(\bm{a})=\frac{\prod_{i=1}^{n}\Gamma(a_1)\cdots \Gamma(a_n)}{\Gamma(a_0)}$。同时为了简化符号，我们令后验分布 $\displaystyle p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{a})$现在我们令下一次观测 $\displaystyle \bm{x}_{n+1}=\tilde{\bm{x}}$。我们现在想知道 $\displaystyle \tilde{\bm{x}}$各种情况下概率，以辅助决策。考虑到 $\displaystyle \tilde{\bm{x}}\in\{0,1\}^c \,,\bm{\mu}\in[0,1]^c=\mathcal{U}$。我们有：$$\begin{align}p(\tilde{\bm{x}}\mid \mathcal{D})&amp;=\int_{\mathcal{U}}p(\tilde{\bm{x}}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\mathrm{Cat}(\bm{x}\mid\bm{\mu})\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{\tilde{x}_j}\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{a_j+\tilde{x}_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma\left(\bm{I}^\text{T}[\bm{a}+\bm{x}]\right)}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma(a_0+1)}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}a_j^{\tilde{x}_j}\prod_{j=1}^{c}\Gamma(a_j)}{a_0^{\tilde{x}_j}\Gamma(a_0)}=\prod_{j=1}^{c}\frac{a_j}{a_0}^{\tilde{x}_j}=\prod_{j=1}^{c}\mathrm{E}^{\tilde{x}_j}(\bm{\mu}\mid\mathcal{D})\\&amp;=\mathrm{Cat}\left(\tilde{\bm{x}}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于 $\displaystyle plug-in\,\mathrm{E}[\mu\mid\mathcal{D}]$。我们也可以分析出类似贝塔-伯努利模型的结论。 6、多试验后验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么分类 $\displaystyle j$发生 $\displaystyle s_j$次的概率。写成向量形式 $\displaystyle \bm{s}$。于是有：$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\frac{m!}{s_1!…s_c!}\prod_{j=1}^{c}\mu_j^{s_j}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 我们称 $\displaystyle \mathrm{Dm}(\bm{s}\mid \mathcal{D},m)=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}$为狄利克雷-多项式分布(Dirichlet-multionmial distribution)。后验预测分布的均值与协方差矩阵问题 后验预测分布均值：$$\begin{align} \mathrm{E}[s_j\mid \mathcal{D},m] &amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\mathrm{E}(s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}=\int_{\mathcal{U}}m\mu_j\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=m\frac{\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+1)}=m\frac{a_j}{a_0}\end{align}$$ $\displaystyle \mathrm{E}[\bm{s}\mid \mathcal{D},m]=m\frac{\bm{a}}{a_0}$ 后验预测分布方差：$$\begin{align} \mathrm{var}[s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{E}(s_j^2)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}[m\mu_j+m(m-1)\mu_j^2]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{\Gamma(a_j+2)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{(a_j+1)a_j}{a_0(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=\frac{ma_j(a_0-a_j)}{a_0^2}\frac{a_0+m}{a_0+1}\end{align}$$ 后验预测分布协方差：$$\begin{align} \mathrm{cov}[s_i,s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{cov}(s_i,s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}[-m\mu_i\mu_j]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{\Gamma(a_i+1)\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_i)\Gamma(a_j)\Gamma(a_0+2)}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{a_ia_j}{(a_0+1)a_0}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=\frac{-ma_ia_j}{a_0^2}\frac{a_0+m}{a_0+1}\\\end{align}$$ 后验预测分布协方差矩阵：$\displaystyle \mathrm{cov}[\bm{s}\mid \mathcal{D},m]=m(m+a_0)\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ 对于 $plug−in\,\bm{\mu}_{MAP}$插值 $\displaystyle \mathrm{Mu}(\bm{s}\mid \bm{\mu}_{MAP},m)$，我们知道 $\displaystyle \bm{\mu}_{MAP}=\frac{\bm{a}-\bm{1}}{a_0-c}$其协方差矩阵为：$\displaystyle \mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]=m\left[\mathrm{diag}[\bm{\mu}_{MAP}]-\bm{\mu}_{MAP}\bm{\mu}_{MAP}^\text{T}\right]$ $\displaystyle \mathrm{var}[s_j\mid \bm{\mu}_{MAP},m]=\frac{m(a_j-1)(a_0-a_j+1-c)}{(a_0-c)^2}$ $\displaystyle \mathrm{cov}[s_i,s_j\mid \bm{\mu}_{MAP},m]=\frac{-m(a_i-1)(a_j-1)}{(a_0-1)^2}$比较大小容易证明：$$\begin{align}\mathrm{cov}[\bm{s}\mid \mathcal{D},m]\geqslant\mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]\end{align}$$所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论。 7、数据集后验预测与边缘似然函数有数据集 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，这有$$\begin{align} p(\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_t\mid\bm{\mu})p(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^t}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ $$\begin{align} p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid\mathcal{D}_t,\bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^{t+1}}\frac{1}{\mathrm{B}(\bm{k}_t+\bm{a})}\prod_{j=1}^{c}\mu_j^{k_j+a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{k}_t+\bm{a})}\end{align}$$ 所以有：$$\begin{align} p(\mathcal{D}_{t+1}\mid\mathcal{D}_t,\bm{a})=\frac{p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid\bm{a})}{ p(\mathcal{D}_t\mid\bm{a})}=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 9、评述通过狄利克雷-多项式模型，我们从离散二维拓展到了离散多维。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)}, x\in\{1,..,c\},\bm{\mu}\in[0,1]^c $ 这个分布由 $\displaystyle \bm{\mu}$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \bm{\mu}$。于是人类搞了个假设空间 $\displaystyle \mathcal{H}=\{\bm{\mu}_i\}$，为了找到上帝的那个 $\displaystyle \hat{\bm{\mu}}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。并且动用了自己的经验和感觉，人类假设 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}$。另外我们还可以结合，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}…\mathcal{D}_{\infty}$，来给假设空间的每个 $\displaystyle \bm{\mu}$更新概率。于是这个表达式横空出世 $\displaystyle p(\bm{\mu}\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid \mathcal{D}_{t})$。于是我们得到了： $$p(\bm{\mu}\mid\mathcal{D}_t)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_t+\bm{a})$$这样我们就把假设空间的 $\displaystyle \bm{\mu}$都给了个概率。这样我们就有关于 $\displaystyle \bm{\mu}$决策的信息。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$p(\tilde{x}\mid \mathcal{D})=\int_{\mathcal{U}}p(\tilde{x}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}=\mathrm{Cat}\left(\tilde{x}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)$$于是基于对假设空间再次赋概，我们对上帝有了新的认识 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{\bm{x}}$的次数是$\bm{s}$次的概率$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\frac{n!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$这样我们对 $\displaystyle \bm{s}$的可能值也赋概了。这个式子称之为狄利克雷-多项式分布。我们对上帝又有了新的认识 6、至此，由于 $\displaystyle p(\bm{\mu}\mid\mathcal{D}_t,\mathcal{D}_{t+1})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_{t+1}+\bm{k}_t+\bm{a})$。当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。因为$$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降，我们发现人类的认识 $\displaystyle \hat{\bm{\mu}}$会越来越逼近上帝的那个 $\displaystyle \bm{\mu}$概率。也就是说$$\begin{align}p(\hat{\bm{\mu}}\to \bm{\mu})\to1\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>分类分布</tag>
        <tag>狄利克雷-多项式模型</tag>
        <tag>狄利克雷分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝塔-伯努利模型(Beta-Binomial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-07-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝塔-伯努利模型(beta-binomial model)伯努利分布： $$\begin{align}x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1]\end{align}$$ 1、0-1分布(bernoulli distribution)概率质量函数可以表示为：$$\begin{align}\mathrm{PMF}：\mathrm{Ber}(x\mid \mu)=\mu^x(1-\mu)^{1-x}\end{align}$$其他表示方法：$\displaystyle \mathrm{Ber}(x\mid \mu)=\mu^{\mathbb{I}(x=1)}(1-\mu)^{\mathbb{I}(x=0)} $ $\displaystyle \mathrm{Ber}(x\mid \mu)=\begin{cases}\mu&amp;\text{if }x=1\\\nu=1-\mu&amp;\text{if }x=0\end{cases} $ 2、均值与方差：知道 $\displaystyle \varphi_x(t)=\mu\mathrm{e}^{\mathrm{i}t}+\nu$ $\displaystyle \mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\mu$ $\displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\mu(1-\mu)=\mu\nu$ 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$于是有似然函数 $$\begin{align}\mathrm{L}(\mu)=p(\mathcal{D}\mid\mu)=p(\boldsymbol{X}\mid\mu)=\prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 其中 $\displaystyle N_1=\sum_{i=1}^{n}\mathbb{I}(x_i=1)$， $\displaystyle N_0=\sum_{i=1}^{n}\mathbb{I}(x_i=0)$ 4、对数似然函数$$\begin{align}\mathcal{L}(\mu)&amp;=\ln p(\mathcal{D}\mid\mu)=\ln \prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\sum_{i=1}^{n}\left(x_i\ln\mu+(1-x_i)\ln(1-\mu)\right)\\&amp;=\boldsymbol{I}^\text{T}\boldsymbol{x}\ln\mu+\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})\ln(1-\mu)\end{align}$$ 5、求极大似然估计：$$ \frac{\partial{\mathcal{L}}}{\partial{\mu}}=\frac{1}{\mu}\boldsymbol{I}^\text{T}\boldsymbol{x}-\frac{1}{1-\mu}\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})=0\\\mu\boldsymbol{I}^\text{T}\boldsymbol{I}=\boldsymbol{I}^\text{T}\boldsymbol{x}$$$\begin{align}\mu_{MLE}=\frac{\boldsymbol{I}^\text{T}\boldsymbol{x}}{\boldsymbol{I}^\text{T}\boldsymbol{I}}=\frac{1}{n}\sum_{i=1}^{n}x_i=\bar{x}\end{align}$极大似然估计分析：我们知道 $\displaystyle x\in\{0,1\}$，在0-1分布的n次试验中, $\displaystyle x=1$的次数为 $\displaystyle k$。于是$$\begin{align}\mu_{MLE}=\frac{1}{n}\sum_{i=1}^{n}=\frac{k}{n}\end{align}$$现在我们假设抛硬币3次，3次都是正面朝上，那么 $\displaystyle n=k=3$，那么 $\displaystyle \mu_{MLE}=1$。这种情况下，极大似然的结果预测所有未来的观察值都是正面向上。常识告诉我们这是不合理的。事实上，这就是极大似然中过拟合现象的一个极端例子。下面我们引入 $\displaystyle \mu$的先验分布，我们会得到一个更合理的结论。 6、二项式分布(binomial distribution)我们现在把 $\displaystyle k$作为随机变量 $\displaystyle k=x_1+x_2+…+x_n$，于是我们有 $\displaystyle k\in\{N_1\}$，易知二项式分布和0-1分布的似然函数有相同的形式，是正比关系。最大化它们的 $\displaystyle \mu$都是 $\displaystyle \frac{k}{n}$。 $$\begin{align}\mathrm{L}(\mu)\propto\mathrm{Bin}(k\mid \mu,n)=\mathrm{C}_{n}^k\,\mu^k(1-\mu)^{n-k}\end{align}$$ 为了与后面符号衔接：我们令 $\displaystyle k=N_1,n=N,N_0=n-k$于是又有：$$\begin{align}p(\mathcal{D}\mid\mu)=\mathrm{L}(\mu)\propto\mathrm{Bin}(N_1\mid \mu,N_1+N_0)=\mathrm{C}_{N}^{N_1}\,\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 我们知道 $\displaystyle \mathrm{C}_{n}^{k}=\frac{n!}{(n-k)!k!}$。同时我们有离散随机变量特征函数$\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x_i}]=\sum_{i=1}^{\infty}p_i\mathrm{e}^{\mathrm{i}t x_i}$，注意虚数 $\displaystyle \mathrm{i}$和变量$\displaystyle i$的区别。我们令 $\displaystyle \nu=1-\mu$于是有：$$\begin{align}\varphi_k(t)=(\mu\mathrm{e}^{\mathrm{i}t}+\nu)^n\end{align} $$$\displaystyle \mathrm{E}[k]=\sum_{k=0}^{n}k\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=n\mu$$\displaystyle \mathrm{var}[k]=\sum_{k=0}^{n}(k-\mathrm{E}[k])^2\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[k]=n\mu(1-\mu)=n\mu\nu$ 7、共轭先验分布如果先验和似然函数有相同形式，那就非常方便：这样后验也有相同形式。这个时候我们称之为共轭先验(conjugate prior)。现在，我们把 $\displaystyle \mu$做为一个变量观察似然函数得到$$\begin{align}p(\mu)\propto\mu^{\rho_1}(1-\mu)^{\rho_2}\end{align}$$这样计算后验就很容易了： $\displaystyle p(\mu\mid\mathcal{D})\propto p(\mathcal{D}\mid \mu)p(\mu)=\mu^{N_1}(1-\mu)^{N_0}\mu^{\rho_1}(1-\mu)^{\rho_2}=\mu^{N_1+\rho_1}(1-\mu)^{N_0+\rho_2}$ 在给出先验分布之前，我们看看这个两个函数。之后我们会补充 $\displaystyle \Gamma(x),\mathrm{B}(a,b)$函数的性质的证明。$\displaystyle \Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u, x&gt;0$$\displaystyle \mathrm{B}(a,b)=\int_{0}^{1}x^{a-1}(1-x)^{b-1}\mathrm{d}x,a&gt;0,b&gt;0$$\displaystyle \Gamma(x+1)=x\Gamma(x)$$\displaystyle \Gamma(n+1)=n!$$\displaystyle \mathrm{B}(a,b)=\mathrm{B}(b,a)$$\displaystyle \mathrm{B}(a,b)=\frac{b-1}{a+b-1}\mathrm{B}(a,b-1),a&gt;0,b&gt;1$ $\displaystyle \mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ 然后我们知道贝塔分布： $\displaystyle \mathrm{Beta}(\mu\mid a,b)=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $。这正是我们要的共轭先验。 $$\begin{align}\mathrm{Beta}(\mu\mid a,b)=\frac{1}{\mathrm{B}(a,b)}\mu^{a-1}(1-\mu)^{b-1}\end{align}$$利用 $\displaystyle \Gamma$函数性质易知：$\displaystyle \mathrm{E}[\mu]=\frac{a}{a+b} $$\displaystyle \mathrm{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}$$\displaystyle \mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,\mathrm{Beta}(\mu\mid a,b)=\frac{a-1}{a+b-2},a&gt;0,b&gt;0$ 8、后验分布我们用似然函数乘以贝塔先验得到后验分布：$$\begin{align}p(\mu\mid\mathcal{D})\propto\mathrm{Bin}(N_1\mid\mu, N_1+N_0)\mathrm{Beta}(\mu \mid a,b)\end{align}$$ 归一化得：$$\begin{align}p(\mu\mid\mathcal{D})=\mathrm{Beta}(\mu\mid N_1+a,N_0+b)\end{align}$$ 1、在线学习容易证明 $\displaystyle \mathrm{Beta}$分布具有再生性质。于是我们可以假设我们有两个数据集 $\displaystyle \mathcal{D}_1,\mathcal{D}_2$。于是有：1、当我们观察到 $\displaystyle \mathrm{D}_1$时，有$$p(\mu\mid\mathcal{D}_1)=\mathrm{Beta}(\mu\mid N_1^1+a,N_0^1+b)$$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\mu) p(\mu\mid\mathcal{D}_1)$$易得：$$\begin{align}p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Beta}(\mu\mid N_1^1+N_1^2+a,N_0^1+N_0^2+b)\end{align}$$这表明 该贝叶斯推断具有在线学习的良好性质。 2、后验均值与众数最大后验估计：$$\begin{align}\mu_{MAP}=\mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,p(\mu\mid\mathcal{D})=\frac{N_1+a-1}{N+a+b-2}=\frac{k+a-1}{n+a+b-2}\end{align}$$极大似然估计： $$\begin{align} \mu_{MLE}=\frac{N_1}{N}=\frac{k}{n} \end{align}$$ 后验均值: $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{N_1+a}{N+a+b}=\frac{k+a}{n+a+b}\end{align}$$我们发现众数和均值不同。我们还发现 后验均值是先验均值和最大似然估计的凸组合。下面我们下证明这一点。 令先验均值$\displaystyle \frac{a}{a+b}=\mu_0,a+b=c$。 $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+a}{n+a+b}=\frac{c}{n+c}\mu_0+\frac{n}{n+c}\mu_{MLE}=\lambda\mu_0+(1-\lambda)\mu_{MLE}\end{align}$$ 我们发现 $\displaystyle c$可以理解为先验对于后验的等价样本大小。 $\displaystyle \lambda=\frac{c}{n+c}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计：$$\begin{align}\mu_{MAP}=\frac{c-2}{n+c-2}\mathrm{mode}[\mu_0]+\frac{n}{n+c-2}\mu_{MLE}=\eta\mathrm{mode}[\mu_0]+(1-\eta)\mu_{MLE}\end{align}$$ 3、后验方差$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]=\frac{(N_1+a)(N_0+b)}{(N_1+a+N_0+b)^2(N_1+a+N_0+b+1)}\end{align}$$当 $\displaystyle N$足够大时：$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]\approx\frac{N_1N_0}{NNN}=\frac{\mu_{MLE}(1-\mu_{MLE})}{n}\end{align}$$后验标准差 $$\begin{align}\sigma=\sqrt{\frac{\mu_{MLE}(1-\mu_{MLE})}{n}}\end{align}$$1、它随着我们数据的增加以 $\displaystyle \sqrt{\frac{1}{n}}$的速度下降。2、 $\displaystyle \mu=\mathop{\mathrm{argmax}}_{\mu}\sigma(\mu)=0.5$3、 $\displaystyle \mu=\mathop{\mathrm{argmin}}_{\mu}\sigma(\mu)=0\,or\,1$ 4、后验预测分布(posterior predictive distribution)上述，我们讨论了未知参数的推断，现在我们来讨论预测问题。在这之前：我们令后验分布 $\displaystyle p(\mu\mid\mathcal{D})=\mathrm{Beta}(a,b)$以简化符号 。考虑 $\displaystyle x_{n+1}$发生了，那么我们想预测 $\displaystyle x_{n+1}=0\,or\,1$的概率。为了简记，令 $\displaystyle \tilde{x}$为一随机变量，代表 $\displaystyle n$次观测后的值。同时考虑到 $\displaystyle \tilde{x}\in\{0,1\}$于是有：$$\begin{align}p(\tilde{x}\mid \mathcal{D})&amp;=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D})\mathrm{d}\mu\\&amp;=\int_0^1\mu^\tilde{x}(1-\mu)^{1-\tilde{x}}\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\int_0^1\mu^{a+\tilde{x}-1}(1-\mu)^{b+1-\tilde{x}-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\frac{\Gamma(a+\tilde{x})\Gamma(b+1-\tilde{x})}{\Gamma(a+b+1)}\\&amp;=\frac{a^{\tilde{x}}b^{1-\tilde{x}}}{(a+b)^{\tilde{x}}(a+b)^{1-\tilde{x}}}\\&amp;=\left(\frac{a}{a+b}\right)^{\tilde{x}}\left(\frac{b}{a+b}\right)^{1-\tilde{x}}\\&amp;=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于plug-in$\displaystyle \mathrm{E}[\mu\mid\mathcal{D}]$。这里的plug-in x是插值近似 plug-in approximation的意思。 5、过拟合与黑天鹅悖论如果我们使用 $\displaystyle plug-in\,\mu_{MLE}$：$$\begin{align}p(\tilde{x}\mid \mathcal{D})\approx\mathrm{Ber}\left(\tilde{x}\mid\mu_{MLE}\right)\end{align}$$1、当数据集较小，例如 $\displaystyle n=3,k=0,then,\mu_{MLE}=\frac{0}{3}=0$。这样我们预测 $\displaystyle \tilde{x}=0$的概率为 $\displaystyle 0$。这叫零数问题(zero count problem)，或者叫数据匮乏问题(sparse data problem)。这种问题在小样本情况，经常发生。当然有人就要说了，现在是大数据时代，干嘛关注这个问题？如果我们基于特定的目的，划分数据：例如特定的人做特别的事——个人购物推荐。这种情况下即使是在大数据时代，数据也不会很大。所以 $$即使在大数据当道的时代，贝叶斯方法依然是有用的！[Jordan2011]$$2、零数问题类似于黑天鹅事情。3、我们来用贝叶斯方法来解决这个问题： 使用均匀分布先验 $\displaystyle \mathrm{Beat}(1,1)=\mathrm{U}[0,1]$，plug-$\displaystyle in\,\mathrm{E}[\mu\mid\mathcal{D}]$给出了拉普拉斯继承规则(laplace’s rule of succession):$$\begin{align}p(\tilde{x}=1\mid\mathcal{D})=\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+1}{n+2}\end{align}$$可以理解为：在实践中，通常在数据集中加一应该是正确的做法。称之为加一平滑(add-one smoothing)4、注意到：plug-in$\displaystyle \mu_{MAP}$没有这个性质。 6、多试验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么发生 $\displaystyle \tilde{x}=1$的次数是 $\displaystyle s$次的概率。$$\begin{align}p(s\mid\mathcal{D},m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid a.b)\mathrm{d}\mu\\&amp;=\frac{\mathrm{C}_m^s}{\mathrm{B}(a.b)}\int_0^1\mu^{s+a-1}(1-\mu)^{m-s+b-1}\mathrm{d}\mu\\&amp;=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}\end{align}$$我们称 $\displaystyle \mathrm{Bb}(s\mid a,b,m)=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}$为贝塔-二项式分布(beta-binomial distribution)。易知(使用积分表达式,同时利用 $\displaystyle \mathrm{Bin}(s\mid \mu,m)$容易求得)：$\displaystyle \mathrm{E}[s\mid\mathcal{D},m]=m\frac{a}{a+b}$ $\displaystyle \mathrm{var}[s\mid\mathcal{D},m]=\frac{mab}{(a+b)^2}\frac{a+b+m}{a+b+1}$ $\displaystyle \mathrm{var}[s\mid\mu_{MAP}]=\frac{m(a-1)(b-1)}{(a+b-2)^2}$(插值求得的)于是有：$$\begin{align} \mathrm{var}[s\mid\mathcal{D},m]\geqslant\mathrm{var}[s\mid\mu_{MAP}]\end{align}$$至于证明，可以分析这个式子： $\displaystyle f(a,b)=ab \left( a+b-2 \right) ^{2} \left( a+b+m \right) - \left( a-1 \right) \left( b-1 \right) \left( a+b \right) ^{2} \left( a+b+1 \right) \geqslant 0,(a&gt;0,b&gt;0)$可以使用微积分一阶导数，二阶导数证明。所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论$$ 9、评述通过贝塔-伯努利模型，我们熟悉了贝叶斯方法的基本概念、流程、特点。一旦我们熟悉了伯努利、二项、贝塔、均匀、贝塔-伯努利分布后，很多关键的东西就可以用文字表述清楚了。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1] =\mu^x(1-\mu)^{1-x} $ 这个分布由 $\displaystyle \mu$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \mu$。于是人类搞了假设空间 $\displaystyle \mathcal{H}=\{\mu_i\}$，为了找到上帝的那个 $\displaystyle \hat{\mu}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。因为我们可以根据我们在某个时刻，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，来给假设空间的每个 $\displaystyle \mu$更新概率。于是这个表达式横空出世 $\displaystyle p(\mu\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\mu)p(\mu\mid \mathcal{D}_{t})$。于是我们得到了：$$p(\mu\mid\mathcal{D}_t)=\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b) $$这样我们就把假设空间的 $\displaystyle \mu$的都给了个概率。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$ p(\tilde{x}\mid \mathcal{D}_t)=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}_t]\right)$$于是基于对假设空间赋概，我们对未来的可能值也赋概。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{x}=1$的次数是$s$次的概率$$\begin{align}p(s\mid\mathcal{D}_t,m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu\\&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b)\mathrm{d}\mu\\&amp;=\mathrm{Bb}(s\mid N_1^t+a,N_0^t+b,m)\end{align}$$这样我们对 $\displaystyle s$的可能值也赋概了。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 6、至此，当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。 7、靠那个贝叶斯是谁，怎么总要出来嘟囔一下。决策论是啥？被冷落的贝叶斯微微一笑。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>大数据</tag>
        <tag>贝塔-伯努利模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯统计学概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝叶斯统计学框架经典统计学利用总计和样本信息来做统计分析，而贝叶斯统计学还加入了先验信息。下面我们用单参数一维随机变量加以说明： 1、记号以一维随机变量为例：频率学派中，依赖参数的概率密度(质量)函数表示为 $\displaystyle p_\beta(x) $或者 $\displaystyle p(x\,;\beta) $。表示在参数空间 $\displaystyle \mathcal{B}=\{\beta_i\} $中，不同 $\displaystyle \beta $对应不同密度概率(质量)函数。而在贝叶斯学派中，表示为 $\displaystyle p(x\mid \beta) $，代表了随机变量 $\displaystyle \beta $给定某个值时，总体 $x$的条件分布。而频率学派中不认为$\displaystyle \beta $是随机变量。它们认为上帝不玩骰子。 2、先验概率根据参数 $\displaystyle \beta $的先验信息确定先验分布$$\displaystyle p(\beta) $$ 3、样本的产生与似然函数贝叶斯观点认为：一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $产生要分两步：1、上帝从先验分布 $\displaystyle p(\beta) $中选了一个 $\displaystyle \beta_k $我们人类不知道，但是可以$设想$。2、从总体分布 $\displaystyle p(x\mid\beta_k) $产生一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $，这是具体的，我们人类能看到的。(按照频率学派观点，这里我们蕴含了 $\displaystyle x_i $是随机变量，且独立同分布的，贝叶斯认为这不需要，不过通常情况下，我们是使用IID，因为这样方便。)：$$\mathrm{L}(\beta_k)=p(\boldsymbol{x}\mid \beta_k)=\prod_{i=1}^{n}p(x_i\mid\beta_k) $$我们称 $\displaystyle \mathrm{L}(\beta_i) $为似然函数，它综合了总体信息和样本信息。 4、样本与参数的联合分布由于 $\displaystyle \beta_k $是上帝选的，我们人类$设想$的，它仍然是未知的，所以要把这个未知考虑进来，也是就 $\displaystyle \beta $的先验信息，对 $\displaystyle \beta $的一切可能加以考虑，而不仅仅是 $\displaystyle \beta_k $。这样我们人类就有了 $\displaystyle \boldsymbol{x} $和 $\displaystyle \beta $的联合分布：$$p(\boldsymbol{x},\beta)=p(\boldsymbol{x}\mid \beta)p(\beta) $$ 5、贝叶斯推断在没有样本信息时，人类只能根据先验分布 $\displaystyle p(\beta) $对 $\beta$做出推断。现在我们人类有了 $\displaystyle p(\boldsymbol{x},\beta) $，这样我们就可以做出新的推断了。1、先分解： $\displaystyle p(\boldsymbol{x},\beta)=p(\beta\mid\boldsymbol{x})p(\boldsymbol{x}) $。2、其中 $\displaystyle p(\boldsymbol{x})=\int_{\mathcal{B}}p(\boldsymbol{x},\beta)\mathrm{d}\beta=\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta$ ，它与$\beta$无关，或者说 $\displaystyle p(\boldsymbol{x}) $不含 $\displaystyle \beta $的任何信息。因此能用来推断的仅仅是条件分布：$$ p(\beta\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta)p(\beta)}{\displaystyle\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta} $$这就是贝叶斯公式的概率密度函数形式。 $\displaystyle p(\beta\mid\boldsymbol{x}) $史称后验分布，它集中了总体、样本、先验的一切信息，又排除了一切与 $\displaystyle \beta $无关的信息之后得到的结果。所以基于后验分布 $\displaystyle p(\beta\mid\boldsymbol{x}) $对 $\displaystyle \beta $进行统计推断是更为有效，也是最合理的。考虑离散情形：$$p(\beta_k\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta_k)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta_k)p(\beta_k)}{\displaystyle \sum_{\beta\in \mathcal{B}}p(\boldsymbol{x}\mid \beta)p(\beta)}$$ 自然语言表述的贝叶斯定理：$$\text{posterior}\propto\text{likeihood}\times\text{prior} $$或者说：$$p(\beta\mid\boldsymbol{x})\propto p(\boldsymbol{x}\mid \beta)p(\beta) $$ 6、贝叶斯统计分析的关键问题：1、确定先验分布： $\displaystyle p(\beta) $2、求联合分布： $\displaystyle p(\boldsymbol{x},\beta) $3、求后验分布： $\displaystyle p(\beta\mid \boldsymbol{x})$ 二、一个精彩的入门例子下面我们来通过一个入门的例子说明，贝叶斯定理是如何工作的。 [例子1.0] 为了提高相亲的成功率，小美考虑打扮一下自己，于是决定买一件羊绒大衣。预计要花费2000块。但是对相亲效果的影响，闺蜜们有2种意见： $\displaystyle \beta_1 $：相亲成功率提高到90%$\displaystyle \beta_2 $：相亲成功率提高到70% 小美当然希望 $\displaystyle \beta_1 $发生，有一个喜欢自己的男朋友，这笔花费还是值得的。根据一个好朋友的情况，先验概率：小美认为 $\displaystyle \beta_1 $的可信度只有40%， $\displaystyle \beta_2 $的可信度是60%。即： $$p(\beta_1)=0.4,\quad p(\beta_2)=0.6 $$小美不想花冤枉钱，于是她做了一个测试：把自己看中大衣，ps一下照片，给5个男性朋友看，结果：$$A: 5个男性朋友都认为小美更漂亮了 $$小美对测试很满意，于是她改变了看法，由二项分布知：$$ p(A\mid \beta_1)=0.9^5=0.590,\quad p(A\mid\beta_2)=0.7^5=0.168 $$由全概率公式 $\displaystyle p(A)=p(A\mid \beta_1)p(\beta_1)+p(A\mid \beta_2)p(\beta_2)=0.337 $。于是有后验概率 $$p(\beta_1\mid A)=\frac{p(A\mid \beta_1)p(\beta_1)}{p(A)}=0.7,\quad p(\beta_2)=\frac{p(A\mid \beta_2)p(\beta_2)}{p(A)}=0.3 $$这个概率综合了小美主观和实验的结果获得，要比小美之前认识的更有吸引力，更贴近实际。经过测试后，小美对买大衣有了兴趣，但是毕竟2000块还是很多的，于是小美再ps了一张图片，给她的男性朋友，结果如下：$$B: 10个男性朋友中，有9个都认为小美更漂亮了 $$$$ p(B\mid \beta_1)=C_{10}^{9}0.9^90.1=0.387,\quad p(B\mid\beta_2)=C_{10}^{9}0.7^90.3=0.121 $$由全概率公式 $\displaystyle p(B)=p(B\mid \beta_1)p(\beta_1)+p(B\mid \beta_2)p(\beta_2)=0.307 $。于是小美再次更新了自己的看法$$p(\beta_1\mid B)=\frac{p(B\mid \beta_1)p(\beta_1)}{p(B)}=0.883,\quad p(\beta_2)=\frac{p(B\mid \beta_2)p(\beta_2)}{p(B)}=0.117 $$小美经过两次测试，$\displaystyle \beta_1(相亲成功率提高到90\%) $的概率上升到了0.883，可以下决心买了。 三、共轭先验分布在叙述前，我们声明一下符号： $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X})=\frac{h(\boldsymbol{X},\boldsymbol{\beta})}{m(\boldsymbol{X})}=\frac{p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{\displaystyle\int_{\mathcal{B}}p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})\mathrm{d}\boldsymbol{\beta}} $ 1、共轭族定义设 $\displaystyle \boldsymbol{\beta} $是总体分布 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})$的参数向量， $\displaystyle \mathcal{F},\mathcal{P} $表示函数族。如果对任意的 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})\in\mathcal{F} $，存在先验分布函数$\displaystyle \pi(\boldsymbol{\beta})\in\mathcal{P} $，且 $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X}) \in \mathcal{P}$。就是说 $\displaystyle \mathcal{P} $是 $\displaystyle \mathcal{F} $的共轭族、称 $\displaystyle \pi(\boldsymbol{\beta}) $是共轭先验分布。 2、一维随机变量共轭先验分布例子1、方差已知下，一维高斯分布均值的先验分布是高斯分布。为了理解，我们先举一个简单的例子： $\displaystyle x\mid \mu\sim\mathcal{N}(x\mid \mu,\sigma) $，设 $\displaystyle \sigma $已知。有一组样本观测值 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $或者说有数据集 $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$。 我们现在开始分析：1、样本似然函数 $\displaystyle p(\mathcal{D}\mid\mu)=p(\boldsymbol{x}\mid\mu)=\prod_{i=1}^{n}p(x_i\mid\mu)=(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})\right]$2、取 $\displaystyle \mu\sim\mathcal{N}(\mu\mid \bar{\mu},\delta) $为先验分布，其中 $\displaystyle \bar{\mu},\delta $是已知的。接下来我们将看到它是共轭的。3、于是有联合分布：$\displaystyle p(\mathcal{D},\mu)=p(\mathcal{D}\mid \mu)p(\mu)=\frac{1}{(2\pi)^{\frac{n+1}{2}}\sigma^{-\frac{n}{2}}\delta^{-1}}\exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] $4、应用自然语言的贝叶斯定理，我们有后验分布：$$\begin{align}p(\mu\mid\mathcal{D})=\propto&amp; p(\boldsymbol{x}\mid \mu)p(\mu)\propto \exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] \\\propto&amp;\exp\left[-\frac{1}{2}\left((\frac{n}{\sigma^2}+\frac{1}{\delta^2})\mu^2-2(\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2})\mu+\frac{\boldsymbol{x}^\text{T}\boldsymbol{x}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2}\right)\right]\\\propto &amp;\exp\left[-\frac{1}{2}(A\mu^2-2B\mu+C)\right]\\\propto &amp;\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right]\end{align}$$ 其中 $\displaystyle A=\frac{n}{\sigma^2}+\frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2},B=\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2} =\frac{\bar{x}}{\bar{\sigma}^2}+\frac{\bar{\mu}}{\delta^2}$ 在这里我们一般把 $\displaystyle \exp\left[\boldsymbol{w}^\text{T}\boldsymbol{g}(\boldsymbol{x})\right] $称为正态分布的核。于是$$\displaystyle p(\mu\mid\mathcal{D})=p(\mu\mid\boldsymbol{x})=\frac{1}{(2\pi A^{-1})^{\frac{1}{2}}}\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right] $$也就是说后验分布是：$$\mu\mid\mathcal{D}\sim\mathcal{N}(\mu\mid \frac{B}{A},A^{-1}) $$ $\displaystyle \frac{B}{A}=\frac{\delta^2}{\bar{\sigma}^2+\delta^2}\bar{x}+\frac{\bar{\sigma}^2}{\bar{\sigma}^2+\delta^2}\bar{\mu}=\lambda\bar{x}+(1-\lambda)\bar{\mu} $$\displaystyle \frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2} $这就证明了：正态方差已知，它的均值的共轭先验分布是正态分布 3、若干技巧总结1、贝叶斯分析非常依赖于去求后验分布,如果按照定理，分母有一个积分，事实上它是一个数。于是我们经常应用$$\text{posterior}\propto\text{likelihood}\times\text{prior} $$这个式子分析，求得解后，在做归一化处理。就能得到posterior的表达式。2、在高斯分布下，我们经常需要配平方，以及观察随机变量的二次项(二次项的逆就是方差），一次项( 二次项的逆乘以一次项就是均值）。这是一个很重要的技巧。 四、充分统计量1、直观理解：就是不损失信息的统计量就是充分统计量。也就是说 $\displaystyle p_\beta(\boldsymbol{x}\mid T(\boldsymbol{x}))=p(\boldsymbol{x}\mid T(\boldsymbol{x})) $。 2、在这里我们只给出定理：设有样本 $\displaystyle \boldsymbol{x}=[x_1.x_2,…,x_n] $。样本密度 $\displaystyle p(x\mid \beta) $。有一个函数 $\displaystyle T: \boldsymbol{x}\mapsto \mathbb{R}$。 $\displaystyle t=T(\boldsymbol{x}) $它的密度为 $\displaystyle p(t\mid\beta) $。 $\displaystyle \mathcal{P}=\{\pi(\beta)\} $是$\beta$的某个先验分布族。如果对任意的 $\displaystyle \pi(\beta)\in\mathcal{P} $有 $$\displaystyle \pi(\beta\mid T(\boldsymbol{x}))=\pi(\beta\mid\boldsymbol{x}) $$这是 $\displaystyle T(\boldsymbol{x}) $是$\beta$的充分统计量的充要条件。 3、似然函数理解：$$\displaystyle \mathrm{L}(\beta)=p(\boldsymbol{x}\mid \beta)=h(\boldsymbol{x})g(T(\boldsymbol{x})\mid\beta)\propto g(T(\boldsymbol{x})\mid\beta) $$其中$h$与$\beta$无关，因此似然函数与$g(T(\boldsymbol{x})\mid\beta)$成比例，那么按照似然原理，有关$\beta$的推断可以有$T$给出。史称因子分解定理。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵高斯分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-11-%E7%9F%A9%E9%98%B5%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Each day has enough trouble of its own.摘要：本文主要总结了矩阵高斯分布的若干基本问题，和我自己的一些体会。若有错误，请大家指正。关键词: 矩阵高斯分布,矩阵分布,统计学,概率论 一、标准矩阵高斯分布1、问题表述为了就研究数据集分布，我们将涉及：【矩阵分布问题】，当然矩阵分布是指的它所有元素的联合分布。 研究独立同分布的数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$的分布，我们将其写成数据矩阵： $\displaystyle \bm{X}=\big[\bm{x}_1,\bm{x}_2 \cdots \bm{x}_n\big]^\text{T}$。其中 $\displaystyle \bm{x}\in\mathbb{R}^k$且它的元素是相互独立的一元标准高斯分布： $\displaystyle x_i\sim\mathcal{N}(0,1)$。于是有： $$\begin{align}\mathrm{vec}\big(\bm{X}^\text{T}\big)\sim \mathcal{N}\big(\mathrm{vec}\big(\bm{0}_{n\times k}^\text{T}\big),\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 特别的我们用矩阵简洁的表示为： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$其中1、$\displaystyle \mathrm{vec}\big(\bm{X}_{n\times k}^\text{T}\big)=\big[\bm{x}_1^\text{T},\bm{x}_2^\text{T} \cdots \bm{x}_n^\text{T}\big]^\text{T}$，即 $\displaystyle \bm{X}$转置以后，按列拉成向量。 2、张量积 $\displaystyle \bm{A}\otimes\bm{B}=\big[a_{ij}\bm{B}\big]$。于是 $\displaystyle \bm{\varSigma}_{nk\times nk}=\bm{E}_n\otimes \bm{E}_k$ 3、 $\displaystyle \mathrm{tr}\big(\bm{A}^\text{T}\bm{B}\big)=\mathrm{vec}\big(\bm{A}\big)^\text{T}\mathrm{vec}\big(\bm{B}\big)$。于是 $\displaystyle \sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i=\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)=\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)$4、 $\displaystyle \mathrm{cov}\big[\mathrm{vec}\big(\bm{X}^\text{T}\big)\big]=\bm{E}_n\otimes \bm{E}_k=\bm{E}_{nk}$ 我们来简要说明一下：$$\begin{align}p\bigg(\mathrm{vec}\big(\bm{X}^\text{T}\big)\bigg)&amp;=p(\mathcal{D})=\prod_{i=1}^np(\bm{x}_i)=\prod_{i=1}^n\prod_{j=1}^kp(x_{ij})\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\bm{x}_1 ^\text{T}\bm{x}_1^\text{T}+\cdots+\bm{x}_n ^\text{T}\bm{x}_n^\text{T}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 到目前为止，遗留的问题是 $\displaystyle \bm{E}_n\otimes \bm{E}_k$这个参数做何理解。为何要写成克罗内克积的形式。 2、特征函数下面我们求上述矩阵分布的特征函数：我们定义：$\displaystyle \bm{T}=[\bm{t}_1,\bm{t}_2\cdots \bm{t}_n]^\text{T}$, 且知道 $\displaystyle \varphi_{\bm{x}_i}(\bm{t}_i)=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{E}_k\bm{t}_i\big]=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{t}_i\big]$。由独立随机变量联合分布特征函数等于这些随机变量的特征函数之积，知道$$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\varphi_{\mathrm{vec}\big(\bm{X}^\text{T}\big)}\big(\bm{T}\big)&amp;=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t})=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t}_i)\\&amp;=\exp\big[-\frac{1}{2}\big(\bm{t}_1 ^\text{T}\bm{t}_1+\bm{t}_2 ^\text{T}\bm{t}_2+\cdots+\bm{t}_n ^\text{T}\bm{t}_n\big)\big]\\&amp;=\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{T}\big)\big]\end{align}$$ 二、一般矩阵高斯分布1、分布形式现在我们开始考虑更一般的问题： $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}$，且 $\displaystyle \bm{W}=\bm{A}\bm{A}^\text{T}\,,\bm{V}=\bm{B}\bm{B}^\text{T}$，有： $$\begin{align}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\sim\mathcal{N}\big(\mathrm{vec}\big(\bm{M}^\text{T}\big),\bm{W}\otimes \bm{V}\big)\end{align}$$ $$\begin{align}\bm{Y}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\end{align}$$ 2、矩阵分布的特征函数下面，我们用特征函数来证明这一点：$$\begin{align}\varphi_{\bm{Y}}\big(\bm{T}\big)&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{vec}\big(\bm{T}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\big]\bigg]\\&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}\bm{Y}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{Y}\bm{T}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{Y}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{A}\bm{X}\bm{B}^\text{T}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{B}\bm{B}^\text{T}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big)\big]\\&amp;=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 也就是说矩阵分布： $\displaystyle \bm{X}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)$的特征函数是 $$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 3、一般矩阵高斯分布密度我们知道： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 由 $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}\to \bm{X}=\bm{A}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{B}^{-\text{T}}$、微分形式、变量代换定理有： $\displaystyle \frac{\partial \bm{X}}{\partial \bm{Y}}=\big|\,\bm{A}\,\big|^{-k}\big|\,\bm{B}^\text{T}\big|^{-n}=\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}$代入即可得到 $$\begin{align}p\big(\bm{Y}\big)=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\big)\right]\end{align}$$这样我们就得到了密度：$$\begin{align}\bm{X}&amp;\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\\&amp;=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\big)\right]\end{align}$$ 4、一般矩阵高斯分布性质1、$\displaystyle \bm{x}_{i,:}\sim \mathcal{N}(\bm{\mu}_i,w_{ii}\bm{V})$2、 $\displaystyle \bm{x}_{:,j}\sim\mathcal{N}(\bm{\mu}_j,v_{jj}\bm{W})$3、 $\displaystyle\mathrm{cov}[\bm{x}_{i,:},\bm{x}_{j,:}]=w_{ij}\bm{V}$4、 $\displaystyle\mathrm{cov}[\bm{x}_{:,i},\bm{x}_{:,j}]=v_{ij}\bm{W}$ 这个性质是显而易见的，然后如果你没发现“显然”，请仔细阅读上面的内容。要理解上述内容我们需要补充向量矩阵微分、微分形式、和变量代换定理。 三、评述充分熟悉矩阵微分、微分形式(外微分)、和变量代换定理是我们把握高维世界的基本工具。多加练习，容易掌握。矩阵微分大师：许宝騄。外微分大师：陈省生。可以读读他们的书。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学</tag>
        <tag>矩阵高斯分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布的熵]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-10-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E7%86%B5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、若干引理1、引理1.01、连续随机向量函数考虑一般情况，我们有随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$。现在有函数 $\displaystyle \bm{y}=\bm{g}(\bm{x}):\mathbb{R}^k\mapsto\mathbb{R}^d$。即有：$$\begin{align}\bm{y}=\bm{g}(\bm{x})\end{align}$$若上述方程有唯一解：$$\begin{align}\bm{x}=\bm{h}(\bm{y})\end{align}$$则称函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$是 $\displaystyle \bm{y}=\bm{g}(\bm{x})$的反函数。同时我们有雅可比行列式：$$\begin{align}\bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]\end{align}$$ 2、变量代换引理【定理1.0】对于连续随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$，函数 $\displaystyle \bm{y}=\bm{g}(\bm{x})$满足下列条件：1、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$有唯一反函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$2、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$和 $\displaystyle \bm{x}=\bm{h}(\bm{y})$连续3、 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]$存在而且连续那么$$\begin{align}f(\bm{y})=\left\{\begin{array}{l}f_\bm{x}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|&amp;\text{ 若 }\bm{y}\in G \\\ 0 &amp;\text{ 若 }\bm{y}\notin G \end{array}\right.\end{align}$$其中 $\displaystyle G=\{\bm{y}\mid \bm{y}=\bm{g(\bm{x})},\,\bm{x}\in \mathbb{R}^k\}$。 $\displaystyle \left|\,\bm{J}\right|$是雅可比行列式的绝对值，请勿与行列式符号混淆 证明： 为了区分随机变量与随机变量实例，我们定义 $\displaystyle \bm{\xi}$是随机向量，而 $\displaystyle \bm{x}$是随机向量$\displaystyle \bm{\xi}$的实例；$\displaystyle \bm{\eta}$是随机向量，而 $\displaystyle \bm{y}$是随机向量$\displaystyle \bm{\eta}$的实例当 $\displaystyle \bm{y}\notin G$时， 显然有 $\displaystyle f_\bm{\eta}(\bm{y})=0$。当 $\displaystyle \bm{y}\in G$时，有：$$\begin{align}F_ \bm{\eta}(\bm{y})=P(\bm{\eta}\leqslant \bm{y})=\int_A f_ \bm{\xi}(\bm{x})\mathrm{d}\bm{x}\end{align}$$其中： $\displaystyle A=\bigcap_{j=1}^{d}\{\bm{x}\mid g_j(\bm{x})\leqslant y_j\}$，在上式中换元： $\displaystyle \bm{x}=\bm{h}(\bm{y})$得：$$\begin{align}F_ \bm{\eta}(\bm{y})=\int_C\mathbb{I}_G(\bm{y})\times f_ \bm{\xi}\left[\bm{h}(\bm{y})\right]\cdot\left|\bm{J}\right|\mathrm{d}\bm{y}\end{align}$$其中 $\displaystyle C=\prod_{j=1}^{d}(-\infty,y_j]$， $\displaystyle \mathbb{I}_G(\bm{y})$是 $\displaystyle G$的示性函数。由此当 $\displaystyle \bm{y}\in G$时：$$\begin{align}f_ \bm{\eta}(\bm{y})=f_\bm{\xi}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|\end{align}$$证毕。其中证明中最关键的地方在于： $\displaystyle A\to C$的转变中，函数增减涉及积分方向的问题。这一问题的清晰说明较为繁琐，可以参考《数学分析原理》229页定理10.9以及微分形式的积分。 2、引理2.0定义方阵的幂(可以是分数) $\displaystyle \bm{A}^n=\bm{U}\bm{\Lambda}^n\bm{U}^\text{T}$。其中 $\displaystyle \bm{A}=\bm{U}\bm{\Lambda}\bm{U}^\text{T}$是约当分解或者叫谱分解，简单说就是对角化。 1、马哈拉诺比斯变换引理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{I}_k\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。当然我们也可以通过特征函数的方法对马哈拉诺比斯变换引理加以证明。 二、熵对于连续随机变量有： $\displaystyle \mathrm{H}[\bm{x}]=\mathrm{E}[\mathrm{I}(\bm{x})]=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}$下面我们推导多元高斯分布的熵：$$\begin{align}\mathrm{H}[\bm{x}]&amp;=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x})\ln \left[(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\right]\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x}) \left[\ln \left((2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\right)-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{x}) \left[(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{y})\times\bm{y}^\text{T}\bm{y}\mathrm{d}\bm{y}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\sum_{i=1}^k\mathrm{E}[y_i^2]\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{k}{2}\\&amp;=\ln \left[(2\pi\mathrm{e})^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right]\\&amp;=\frac{k}{2}\left(\ln2\pi+1\right)+\frac{1}{2}\ln\left|\bm{\varSigma}\right|\end{align}$$ 注意：推导中我们使用了马哈拉诺比斯变换引理。 三、评述1、在求解多元高斯分布的熵中，我们使用了变量代换，同时引用了马哈拉诺比斯变换引理。2、深层次的原理涉及到微分形式的积分。同时我们也可以浅层次的理解：使用特征函数导出马哈拉诺比斯变换引理3、好了我们不应止步，我们征途是星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-09-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 本文主要总结了多元高斯分布的若干基本问题，和我自己的一些体会。欢迎大家留言讨论，如有错误，请批评指正。 一、一元高斯分布$$\begin{align}x\sim\mathcal{N}(x\mid \mu,\sigma^2)=\left(2\pi\right)^{-1/2}(\sigma^2)^{-1/2}\exp\left[-\frac{1}{2}(x-\mu)^2\sigma^{-2}\right]\end{align}$$ 1、一元高斯分布特征函数我们有特征函数： $\displaystyle \varphi(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tx}p(x)\mathrm{d}x=\mathrm{e}^{i t\mu-\frac{1}{2}t^2\sigma^2}$。$$\begin{align}\varphi(t)=\exp \left[\mathrm{i} t\mu-\frac{1}{2}t^2\sigma^2\right]\end{align}$$下面我们来证明这一点： 令 $\displaystyle z=\frac {x-\mu}{\sigma}$,于是有 $\displaystyle z\sim\mathcal{N}\left(z\mid 0,1\right)=\left(2\pi\right)^{-1/2}\exp \left[-\frac{1}{2}z^2\right]$：$$\begin{align}\varphi_z(t)&amp;=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\end{align}$$ 1、我们知道虚数 $\displaystyle \mathbb{z}=\mathrm{e}^{\mathrm{i}\theta}=\cos(\theta)+\mathrm{i}\sin(\theta)$, 虚数的模 $\displaystyle \left| \mathbb{z}\right|=1$。令 $\displaystyle A(t)=\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}$于是有：$$\left|\frac{\partial{A}}{\partial{t}}\right|=\left|\mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right|=\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2} $$而且有：$$\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \left|\frac{\partial{A}}{\partial{t}}\right|\mathrm{d}z=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z&lt;\infty$$由于 $\displaystyle \int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z$收敛：故由含参反常积分一致收敛的可微性质知函数 $\displaystyle \left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \frac{\partial{A}}{\partial{t}}\mathrm{d}z$关于 $\displaystyle t\in (-\infty,+\infty)$上一致收敛。所以我们可以在 $\displaystyle \varphi_z(t)$ 的积分号下求导(交换积分与求导顺序) ：$$\varphi’_z(t)=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\mathrm{d}z $$2、现在对上式进行分布积分： $\displaystyle\begin{cases} u=\mathrm{e}^{\mathrm{i}tz}\\v=-\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $，同时 $\displaystyle \begin{cases} u’=\mathrm{i}t\mathrm{e}^{\mathrm{i}tz}\\v’=z\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $ 于是有：$$\begin{align}\varphi’_z(t)&amp;=i\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} u\mathrm{d}v\\&amp;= \left.-i\left(2\pi\right)^{-1/2}\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right| _{x=0}-t\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\\&amp;=-t\varphi_z(t)\end{align}$$ 得到微分方程：$$\begin{cases}\varphi’_z(t)+t\varphi_z(t)=0\\\varphi_z(0)=1\end{cases}$$解得：$$\begin{align}\varphi_z(t)=\mathrm{e}^{-\frac{1}{2}t^2}\end{align}$$又因为： $\displaystyle x=\mu+\sigma z$$$\begin{align}\varphi_x(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\mu+\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\varphi_z(\sigma t)=\mathrm{e}^{\mathrm{i}t\mu-\frac{1}{2}t^2\sigma^2}\end{align}$$ 二、多元高斯分布1、多元高斯分布我们知道一维的特征函数为：$$\begin{align}\varphi(t)=\exp \left[\mathrm{i}t\mu -\frac{1}{2}t^2\sigma^2\right]\end{align}$$ 多元情况： $$\begin{align}\bm{x}\sim\mathcal{N}(\bm{x}\mid\bm{\mu},\bm{\varSigma})=(2\pi)^{-k/2}\left|\bm{\varSigma}\right|^{-1/2}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$ 2、马哈拉诺比斯变换定理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{E}_p\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。 独立随机变量联合分布特征函数等于这些随机变量的特征函数之积。于是我们有$$\begin{align}\varphi_y(t)=\exp \left[-\frac{1}{2}t^2\right]\to\varphi_{\bm{y}}(\bm{t})=\prod_{i=1}^p\exp \left[-\frac{1}{2}t_i^2\right]=\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{t}\right]\end{align}$$ 3、多元高斯分布特征函数接着我们使用特征函数的线性变换性质有：$$\begin{align}\varphi_{\bm{x}}(\bm{t})&amp;=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{x}\right]\bigg]=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\left(\bm{\varSigma}^{1/2}\bm{y}+\bm{\mu}\right)\right]\bigg]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\varphi_{\bm{y}}\bigg(\big[\bm{\varSigma}^{1/2}\big]^\text{T}\bm{t}\bigg)=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}^{1/2}\bm{\varSigma}^{1/2}\bm{t}\right]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]\end{align}$$ 三、多元高斯分布的性质1、高斯随机向量的任意边缘依然是高斯分布有随机向量 $\displaystyle \bm{x}$是 $\displaystyle k$维的，且 $\displaystyle \bm{x}\sim \mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$ 现在我们从 $\displaystyle \{x_i\}_{i=1}^k$中任意选取 $\displaystyle p$个元素，令 $\displaystyle s:p\mapsto k$，则向量 $\displaystyle \tilde{\bm{x}}=[x_{s_1},x_{s_2}\cdots x_{s_p}]^\text{T}$仍然是高斯分布：$$\begin{align}\tilde{\bm{x}}\sim\mathcal{N}\big(\tilde{\bm{\mu}},\tilde{\bm{\varSigma}}\big)\end{align}$$其中： $\displaystyle \tilde{\bm{\mu}}=[\mu_{s_1},\mu_{s_2}\cdots \mu_{s_p}]^\text{T}$， $\displaystyle \tilde{\bm{\varSigma}}=[c_{ij}],i,j\in\{s\mid s{p}\}$即保留 $\displaystyle \bm{\varSigma}$的第 $\displaystyle s_1,s_2\cdots s_p$行和列的 $\displaystyle p$阶矩阵。 证明：我们有 $\displaystyle \bm{x}$的特征函数 $\displaystyle \varphi_{\bm{x}}(\bm{t})=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]$,我们令 $\displaystyle t_i=0,i\in \{s\mid k \lnot s(p)\}$有：$$\begin{align}\varphi_{\tilde{\bm{x}}}(\tilde{\bm{t}})=\exp \left[\mathrm{i}\tilde{\bm{t}}^\text{T}\tilde{\bm{\mu}}-\frac{1}{2}\tilde{\bm{t}}^\text{T}\tilde{\bm{\varSigma}}\tilde{\bm{t}}\right]\end{align}$$故而得证。 2、独立性与相关性等价有$\displaystyle \bm{x}_1\sim \mathcal{N}\big(\bm{\mu}_1,\bm{\varSigma}_1\big)$和 $\displaystyle \bm{x}_2\sim \mathcal{N}\big(\bm{\mu}_2,\bm{\varSigma}_2\big)$，我们有$\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0$，且有$\displaystyle \bm{x}=[\bm{x}_1,\bm{x}_2]^\text{T}$服从： $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu}, \bm{\varSigma}\big)$。其中： $\displaystyle \bm{\mu}=[\bm{\mu}_1,\bm{\mu}_2]^\text{T},\bm{\varSigma}=\begin{bmatrix} \bm{\varSigma}_1 &amp; \bm{0}\\\bm{0}&amp;\bm{\varSigma}_2 \end{bmatrix}$。 证明利用1、特征函数的唯一性定理2、独立随机变量联合分布的特征函数是它们特征函数之积。证明是显然的。$$\begin{align}\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0\end{align}$$ 3、仿射(线性)变换不变性有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$， 仿射变换 $\displaystyle \bm{y}=\bm{A}\bm{x}+\bm{b}$，则 $\displaystyle \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big) $ 证明：由特征函数的仿射变换性质有：$$\begin{align}\varphi_{\bm{y}}\big(\bm{t}\big)&amp;=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}\big]\varphi_{\bm{x}}\big(\bm{A}’\bm{t}\big)=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}+\mathrm{i}\big(\bm{A}^\text{T}\bm{t}\big)^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\big]\\&amp;=\exp\bigg[\mathrm{i}\bm{t}^\text{T}\big[\bm{A}\bm{\mu}+\bm{b}\big]–\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\bigg]\end{align}$$又由特征函数唯一性定理知： $$\begin{align} \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big)\end{align}$$【推论1】：分解：多元高斯分布随机向量都可以经过仿射变换为独立随机变量，且它们是标准高斯随机变量。【推论2】：降维：随机向量的线性组合是高斯分布 $\displaystyle \iff$随机向量服从高斯分布 推论2的性质颇为惊奇，我们来推导一下，以窥细节：证明：【充分性】若有一维高斯分布 ：$$\begin{align}y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)\end{align}$$知其特征函数为：$$\begin{align}\varphi_y(t)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$现在对其观察角度加以变换：令 $\displaystyle t=1$同时把 $\displaystyle \bm{a}$看成任意有：$$\begin{align}\varphi_{\bm{x}}\big(\bm{a}\big)=\mathrm{E}\big[\mathrm{i}\bm{a}^\text{T}\bm{x}\big]=\mathrm{E}\big[\mathrm{i}y\big]=\varphi_y\big(1\big)=\exp\big[\mathrm{i}\bm{a}^\text{T}\bm{x}-\frac{1}{2}\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$【必要性】若有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$，则 $\displaystyle y=\bm{a}^\text{T}\bm{x}$的特征函数为：$$\begin{align}\varphi_y(t)=\varphi_{\bm{x}}\big(t \bm{a}\big)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)$证毕 4、条件分布我们考虑如下分布 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_2\mid \bm{x}_1$依然是高斯分布： $$\begin{align} \bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 证明：1、为了利用独立性与相关性等价的结论，我们使用线性变换来构造两个独立的新随机变量： $$\begin{align}\bm{y}_1&amp;=\bm{x}_1\\\bm{y}_2&amp;=\bm{T}\bm{x}_1+\bm{x}_2\end{align}$$ 欲使 $\displaystyle \bm{y}_1\bot \bm{y}_2$，则：$$\begin{align}\mathrm{cov}\big[\bm{y}_1,\bm{y_2}\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_1- \mathrm{E}\big[\bm{y}_1\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\bm{x}_1+\bm{x}_2- \bm{T}\bm{\mu}_1- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)+\bm{x}_2- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_1- \bm{\mu}_1\big)^\text{T}\Big]\bm{T}^\text{T}+\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_2- \bm{\mu}_2\big)^\text{T}\Big]\\&amp;=\bm{\varSigma}_{11}\bm{T}^\text{T}+\bm{\varSigma}_{12}=\bm{0}\end{align}$$于是有：$$\begin{align}\bm{T}=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\end{align}$$也就是说：$$\begin{align}\bm{y}_2=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1+\bm{x}_2\end{align}$$这个线性变换是：$$\begin{align}\begin{bmatrix} \bm{y}_1 \\\ \bm{y}_2 \end{bmatrix}=\begin{bmatrix} \bm{I} &amp; \bm{0}\\ -\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}&amp;\bm{I}\end{bmatrix}\begin{bmatrix} \bm{x}_1 \\\\\bm{x}_2\end{bmatrix}\end{align}$$2、根据数字特征的性质，我们求出 $\displaystyle \bm{y}_1,\bm{y}_2$的期望和方差，由高斯分布的性质知道经过线性变换后的它们也是服从高斯分布的，从而我们可以确定其分布。容易知道：$\displaystyle \mathrm{E}\big[\bm{y}_1\big]=\bm{\mu}_2$，$\displaystyle \mathrm{cov}\big[\bm{y}_1\big]=\bm{\varSigma}_{11}$，$\displaystyle \mathrm{E}\big[\bm{y}_2\big]=\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1$。下面关键是求：$$\begin{align} \mathrm{cov}\big[\bm{y}_2\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)^\text{T}\Bigg]\\&amp;=\bm{\varSigma}_{22}+\bm{\varSigma}_{21}\bm{T}^\text{T}+\bm{T}\bm{\varSigma}_{12}+\bm{T}\bm{\varSigma}_{11}\bm{T}^\text{T}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{11}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$ 于是有：$$\begin{align}\bm{y}_2\sim\mathcal{N}\bigg(\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1,\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 3、并且我们有： $\displaystyle \bm{J}\big(\bm{y}\to \bm{x}\big)=\bigg|\frac{\partial \bm{y}}{\partial \bm{x}^\text{T}}\bigg|=1$，再有 $\displaystyle \bm{y}_1,\bm{y}_2$独立，于是：$$\begin{align}p\big(\bm{x}_1,\bm{x}_2\big)=p\big(\bm{y}_1,\bm{y}_2\big)\bm{J}\big(\bm{y}\to \bm{x}\big)=p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)\end{align}$$ 现在我们可以求得 $\displaystyle \bm{x}_2\mid \bm{x}_1$的分布：$$\begin{align}p\big(\bm{x}_2\mid \bm{x}_1\big)=\frac{p\big(\bm{x}_1,\bm{x}_2\big)}{p\big(\bm{x}_1\big)}=\frac{p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)}{p\big(\bm{y}_1\big)}=p\big(\bm{y}_2\big)=p_{\bm{y}_2}\big(\bm{x}_2- \bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1\big)\end{align}$$代入到密度函数，经过简单变换有：$$\begin{align}\bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$条件数学期望和协方差矩阵是：$$\begin{align}\bm{\mu}_{2\mid1}&amp;=\mathrm{E}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big]\\\bm{\varSigma}_{2\mid 1}&amp;=\mathrm{cov}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$证毕 当然我们也可以通过概率归一约束，和配平方加以推导。 5、高斯线性模型若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，则有：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}^{-1}=\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}$ $\displaystyle \bm{\mu}_{\bm{x}\mid \bm{y}}=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 证明：要想彻底说明这个问题，我们需要分块矩阵的若干引理：首先我们来证明一个引理 1、【引理1】单位矩阵引理：$$\begin{align} [\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$证明：$$\begin{align}&amp;[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E} \\&amp;\iff\bm{S}[\bm{E}-\bm{S}]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\big[\bm{S}^{-1}-\bm{S}\bm{S}^{-1}\big]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\bm{S}^{-1}[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}^{-1}\bm{S}\\&amp;\iff[\bm{S}^{-1}-\bm{E}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\\&amp;\iff\bm{E}=\bm{E}\end{align}$$证毕 2、【引理2】分块矩阵的逆证明：若分块矩阵的逆矩阵存在，：$$\begin{align}\bm{H}^{-1}=\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}\end{align}$$ 1、 【$\displaystyle \bm{A}$若可以逆】首先我们左乘一个矩阵，消除 $\displaystyle \bm{B} $ $$\begin{align} \left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{C} &amp; \bm{D}\end{array}\right]\end{align}$$ 第二步我们右乘一个矩阵，消除 $\displaystyle \bm{C} $ $$\begin{align}\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E}\end{bmatrix}= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{0} &amp; \bm{D}\end{array}\right]\end{align}$$ 简写$\displaystyle \bm{U}\bm{H}\bm{V}=\bm{W} $，于是 $\displaystyle\bm{H}^{-1}=\bm{V}\bm{W}^{-1}\bm{U} $（这是显然的） $$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}=\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E} \end{bmatrix}\left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} &amp; \bm{0} \\\bm{0} &amp; \bm{D}^{-1}\end{array}\right]\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\end{align}$$简化得：$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp;-[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}\bm{B}\bm{D}^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{M}&amp;- \bm{M} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}\bm{M}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}\bm{M}\bm{B}\bm{D}^{-1}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}$。 2、【同理若 $\displaystyle \bm{B}$可逆】$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1}&amp;-\bm{B}\bm{D}^{-1}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\\-[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}^{-1}\bm{C}&amp; [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{A}^{-1}+\bm{A}^{-1}\bm{B}\bm{M}\bm{C}\bm{A}^{-1}&amp;- \bm{B}\bm{D}^{-1}\bm{M}\\-\bm{M} \bm{D}^{-1}\bm{C}&amp;\bm{M}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}= [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}$ 3、【引理3】分块矩阵行列式：由引理2，消除 $\displaystyle \bm{B}$后的结论知道：$$\begin{align}\det \begin{bmatrix} \bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{bmatrix}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det \begin{bmatrix} \bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]\det[\bm{D}]\end{align}$$ 我们也可以这样（也就是说右乘矩阵消除 $\displaystyle \bm{B}$）： $$\begin{align}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}\det \begin{bmatrix} \bm{E} &amp; -\bm{A}^{-1}\bm{B} \\\bm{0} &amp; \bm{E}\end{bmatrix}=\det \begin{bmatrix} \bm{A} &amp; \bm{0}\\\bm{C}&amp;\bm{D}-\bm{C}\bm{A}^{-1}\bm{B} \end{bmatrix}=\det[\bm{A}]\det[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]\end{align}$$ 也就是说：$$\begin{align}\begin{vmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D}\end{vmatrix}=\big|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}\big|\big|\bm{D}\big|=\big|\bm{A}\big|\big|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\big|\end{align}$$ 我们也有$$\begin{align} \left|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} \right|=\left|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\right|\left|\bm{D}^{-1}\right|\big|\bm{A}\big|\end{align}$$ 4、【引理4】维度变换$\displaystyle [\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}$ $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} =\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1} $ $\displaystyle [\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}=\bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1} $证明：我们挑一个做说明,为了应用【引理1】我们右乘 $\displaystyle \bm{A}$于是有：$$\begin{align} &amp;[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\\ &amp;\iff[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}\bm{A}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\bm{A}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-\bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-[\bm{D}^{-1}\bm{C}^{-1}\bm{B}^{-1}\bm{A}+\bm{E}]^{-1}\\ &amp;\iff\big[\bm{E}-[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]\big]^{-1}-\big[[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}-\bm{E}\big]=\bm{E}\,,\bm{S}=-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}\\ &amp;\iff[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$当然我们也可以直接对比引理2的两个结论得出。证毕 5、【引理5】高斯线性回归模型的联合分布我们知道 $\displaystyle p(\bm{x},\bm{y})=p(\bm{x})p(\bm{y}\mid \bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，现在令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 我们取对数：$$\begin{align}&amp;\ln p(\bm{z})=\ln p(\bm{x})+\ln p(\bm{y}\mid \bm{x})\\&amp;=-\frac{1}{2}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)^\mathrm{T}\bm{\varSigma}_{\bm{x}}^{-1}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)-\frac{1}{2}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)^\mathrm{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)+\\&amp;\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1} \end{bmatrix}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}^{-1}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=\ln\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中我们使用了一些二次型的技巧：简要说明一下，以免显得突兀,其中 $\displaystyle \bm{S}$是对称矩阵,有二次型：$$\begin{align}Q&amp;=\frac{1}{2}(\bm{x}-\bm{\mu})^\text{T}\bm{S}(\bm{x}-\bm{\mu})=\frac{1}{2}\bm{x}^\text{T}\bm{S}\bm{x}-\bm{x}^\text{T}\bm{S}\bm{\mu}+\frac{1}{2}\bm{\mu}^\text{T}\bm{S}\bm{\mu}\\&amp;=\frac{1}{2}\bm{x}^\text{T}\bm{A}\bm{x}-\bm{x}^\text{T}\bm{B}+\frac{1}{2}\bm{\mu}^\text{T}\bm{A}\bm{\mu}\end{align}$$其中：$$\begin{align}\bm{S}&amp;=\bm{A}\\\bm{\mu}&amp;=\bm{A}^{-1}\bm{B}\end{align}$$也就是说我们可以通过观测一次项、二次项的系数来求得参数。 考察二次项：$$\begin{align}&amp;-\frac{1}{2}\bm{x}^\text{T}[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}]\bm{x}-\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}+\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}\bm{x}+\frac{1}{2}\bm{x}^\text{T}\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}\\&amp;=-\frac{1}{2}\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{cc}\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\end{array}\right]\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]\end{align}$$考虑一次项$$\begin{align}\bm{x}^\text{T}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}=\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{c}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}-\bm{A}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\\\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\end{array}\right]\end{align}$$这样再通过分块矩阵求逆，和二次型的特点可以求得：$$\begin{align}\bm{\mu}_{\bm{x}}&amp;=\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\\\bm{\varSigma}_{\bm{z}}&amp;=\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\end{align}$$同时我们通过分块矩阵行列式注意到：$$\begin{align}\big|\bm{\varSigma}_{\bm{z}}\big|=\big|\bm{\varSigma}_{\bm{x}}\big|\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|\end{align}$$以上就是联合分布是高斯分布的推导细节。 【高斯线性回归模型的联合分布】若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 则：$$\begin{align}\bm{z}\sim\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中：$\displaystyle \bm{\varLambda}_{\bm{z}}=\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1} \end{bmatrix}$###### 6、【引理6】精度矩阵与协方差矩阵再看出显然之前我们有必要叙述一精度矩阵的概念: 精度矩阵是协方差矩阵的逆$$\begin{align}\bm{\varLambda}=\bm{\varSigma}^{-1}=\begin{bmatrix} \bm{\varLambda}_{11} &amp; \bm{\varLambda}_{12} \\\bm{\varLambda}_{21}&amp;\bm{\varLambda}_{22}\end{bmatrix}\end{align}$$若 $\displaystyle \bm{\varSigma}_{11},\bm{\varSigma}_{22}$可以逆，根据上述引理我们容易知道：$\displaystyle \bm{\varLambda}_{11}=\big[\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\big]^{-1}$$\displaystyle \bm{\varLambda}_{22}=\big[\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\big]^{-1}$$\displaystyle \bm{\varLambda}_{12}=-\bm{\varLambda}_{11}\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}$$\displaystyle \bm{\varLambda}_{21}=-\bm{\varLambda}_{22}\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}$有了精度矩阵，我们可以重写条件分布定理： 若 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_1\mid \bm{x}_2$依然是高斯分布： $$\begin{align} \bm{x}_1\mid \bm{x}_2\sim\mathcal{N}\bigg(\bm{\mu}_1+\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big],\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\bigg)\end{align}$$且有$$\begin{align}\bm{\mu}_{1\mid2}&amp;=\bm{\mu}_1+\bm{\varSigma}_{11}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\mu}_1- \bm{\varLambda}_{11}^{-1}\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\varLambda}_{11}^{-1}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\\&amp;=\bm{\varSigma}_{1\mid 2}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\end{align}$$ $$\begin{align}\bm{\varSigma}_{1\mid 2}=\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}=\bm{\varLambda}_{11}^{-1}\end{align}$$ 7、最后的战斗有了联合分布我们就可以用应用条件分布定理了：结论是显然的：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}=\big[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}\big]^{-1}=\bm{\varLambda}_{xx}^{-1}$ $$\begin{align}\bm{\mu}_{\bm{x}\mid \bm{y}}&amp;=\bm{\varSigma}_{\bm{x}\mid\bm{y}}\bigg[\bm{\varLambda}_{\bm{x}\bm{x}}\bm{\mu}_{\bm{x}}-\bm{\varLambda}_{\bm{x}\bm{y}}\big[\bm{y}-\bm{A}\bm{\mu}_{x}-\bm{b}\big]\bigg]\\&amp;=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]\end{align}$$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 四、评述1、我们来总结一下：$$\begin{align}\begin{cases}\text{马哈拉诺比斯变换定理}\\\text{一元高斯分布特征函数}\\\text{特征函数性质}\end{cases}\Rightarrow\text{多元高斯分布特征函数}\end{align}$$2、当然也可以使用定义与变量代换定理一步到位。3、有了特征函数，我们就可以证明一系列关键定理4、我们讨论一下逆矩阵计算的问题，定义精度矩阵，重写了条件分布定理5、然后我们显然得出了高斯线性模型的结果。6、当然高斯模型还有非常多性质，我们将继续踏上征途。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00001%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、机器学习若干符号解释我们在表达概念时，通常用集合论，空间之类的术。这个时候，我们注意元素和集合的区别。而在我们表达运算时，我们通常用矩阵的概念，这个时候你要注意维度、列、行的概念。多加练习，你很快就会掌握这个表达。 1、输入空间用矩阵表示数据集$\mathcal{D} : \boldsymbol{X}=\left[\begin{array}{c}\boldsymbol{x}_{1}^\text{T}\\\boldsymbol{x}_{2}^\text{T}\\\vdots\\\boldsymbol{x}_{n}^\text{T}\end{array}\right]=\left[\begin{array}{c}\boldsymbol{x}_{1,:}^\text{T}\\\boldsymbol{x}_{2,:}^\text{T}\\\vdots\\\boldsymbol{x}_{n,:}^\text{T}\end{array}\right]=\left[\begin{array}{ccc}x_{1,1} &amp; \dots &amp; x_{1,k} \\\vdots &amp; \dots &amp; \vdots \\x_{n,1} &amp; \dots &amp; x_{n,k}\end{array}\right]$。$\displaystyle \boldsymbol{x}_i=\boldsymbol{x}_{i,:} $表示用 $\displaystyle \boldsymbol{X} $的第i行转置构造向量 $\displaystyle \underbrace {\boldsymbol{x}_i}_{k\times1} $用matlab举个例子： 1234567891011121314%矩阵与机器学习&gt;&gt; X=[12, 14, 15; 1.5, 5.4 ,6.7;20,3.4,5]X = 12.0000 14.0000 15.0000 1.5000 5.4000 6.7000 20.0000 3.4000 5.0000&gt;&gt; x1=X(1,:)'x1 = 12 14 15&gt;&gt; x1(2,1)ans = 14 所以 $\displaystyle \boldsymbol{x_1} $表示包含k个维度的一次观测(示例)。我们用集合论的方式再叙述一遍有n个样本的数据集或者样本空间 $\displaystyle X=\{\boldsymbol{x}_1,\,\boldsymbol{x}_2,\, …,\,\boldsymbol{x}_i,\,…,\,\boldsymbol{x}_n\} \subseteq \mathcal{X}^n$ ,其中 $\displaystyle \boldsymbol{x}_i $是样本点(样本)，我们把输入的所有可能取值集合 $\displaystyle \mathcal{X} $叫做输入空间,无监督学习下也可以称为样本空间。显然 $\displaystyle X\subseteq\mathcal{X}^n $。 输入空间的矩阵表示和集合表示我们需要多加熟悉、灵活运用。这是我们思考多维问题的基础。 2、输出空间集合$\displaystyle Y=\{y_1\,,y_2\,,..\,,y_i\,,…\,,y_n\} \subseteq \mathcal{Y} $，其中输出空间 $\displaystyle \mathcal{Y} $、 输入样本$\displaystyle Y $、输入样本点 $\displaystyle y_i $。 矩阵表示： $\displaystyle \boldsymbol{y}=[y_1\,,y_2\,,..\,,y_i\,,…\,,y_n]^{\text{T}} $ 在有监督学习中，我们把集合 $\displaystyle \{(\boldsymbol{x}_i,y_i)\mid 1 \leqslant i\leqslant n\}$也叫做训练集$\mathcal{D}$ ，$\displaystyle (\boldsymbol{x}_i,y_i) $表示第i个样本。 符号$\displaystyle P(y\mid \boldsymbol{x})=\mathcal{N}(y\mid \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$与$\displaystyle y\mid \boldsymbol{x}\sim\mathcal{N}( \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$是同一个意思。注意 $\displaystyle \mid $的不同意思。[^1] 统计学中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{\beta}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{\beta}) $ 机器学习中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{w}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{w}) $ 3、假设空间：1、如果真实的世界的关系是 $\displaystyle y=h(\boldsymbol{x})$， 世界充满噪声。所以 $\displaystyle y=h(\boldsymbol(x)+e$。现在我们有一个样本或者数据集 $\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$。我们想通过这个数据集 $\displaystyle \mathcal{D}$估计出 $\displaystyle f(\boldsymbol{x})$来找到 $\displaystyle h(\boldsymbol{x})$。其实我们能找到的 $\displaystyle f$有很多，现在我们把它汇集起来： $\displaystyle \mathcal{H}=\{f_i\}$。我们的模型是 $\displaystyle y=f(\boldsymbol{x})+\varepsilon$。 现在我们换一个说法:1、世界是这样的： $\displaystyle p(y=h(\boldsymbol{x})\mid\boldsymbol{x})$2、我们观察到的世界是这样的：$\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$3、我们假设世界是这样的： $\displaystyle p(y=f(\boldsymbol{x)\mid }\boldsymbol{x},\mathcal{D},M)$[^1]，其中 $\displaystyle M$是模型(算法)。于是$\displaystyle \varepsilon=y-f=y-h+h-f=y-h+h-\mathrm{E}[f]+\mathrm{E}[f]-f$$\displaystyle \mathrm{E}[\varepsilon^2]=\mathrm{Var}[e]+\left(h-\mathrm{E}[f]\right)^2+\mathrm{E}\left[\left(f-\mathrm{E}[f]\right)^2\right]$$$ 平方损失期望=噪声方差+偏误^2+模型方差$$ 4、我们还可以写成：$\displaystyle \mathcal{H} =\{f\mid p(y=f(\boldsymbol{x})\mid\boldsymbol{x}, \mathcal{D})\}=\{f(\boldsymbol{\beta})\mid p(y=f(\boldsymbol{x};\boldsymbol{\beta})\mid \boldsymbol{x},\mathcal{D};\boldsymbol{\beta}),\boldsymbol{\beta}\in \mathbb{R}^k\}$。这里的$\displaystyle \mathcal{H} $是模型 $f$的集合。 2、这里的符号有一个重要的解释：$\displaystyle y $是一个随机变量，它的取值是 $\displaystyle y=y_i $。 $\displaystyle \boldsymbol{x} $表示的是 $\displaystyle n $个 $\displaystyle k $维输入数据。也就是说 $\displaystyle \boldsymbol{x} $也是一个变量，不过是向量的形式。它的取值是 $\displaystyle \boldsymbol{x}=\boldsymbol{x}_i $。 3、换一个程序员比较好理解的说法：$\displaystyle y,\boldsymbol{x} $是一个类。而 $\displaystyle y_i,\boldsymbol{x}_i $是一个实例。所以一个实例 $\displaystyle P(y_i\mid \boldsymbol{x}_i,\mathcal{D})$，又有 $\displaystyle P(y_{n+1}\mid \boldsymbol{x}_{n+1},\mathcal{D})$是一个数或者一个概率。$\displaystyle \hat{y},\hat{y}_i $也是类和实例的区别。 输出的最佳估计： $\displaystyle \hat{y}=\hat{f}(x)=\mathop {\text{argmax}}\limits_{\hat{y}}P(y=\hat{y}\mid \boldsymbol{x},\mathcal{D})$ 4、算法空间$\displaystyle \zeta\in\mathcal{L} $，它是算法的集合。 5、参数空间$\displaystyle \boldsymbol{\beta} \in\mathbb{R}^k $。这里的元素我们将 $\displaystyle \mathbb{R}^k$的k维有序组与向量矩阵$\mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}$等同，以方便表达。 6、概念总结有了这些基本概念，我们就可以建立起机器学习的基本框架。一张图搞定: 机器学习框架 7、指示函数，或者叫示性函数$\displaystyle \mathrm{I}_x(A)=\begin{cases}1&amp;\text{if }x\in A\\0&amp;\text{if }x\notin A\end{cases}$ 8、评论这段概论，大部分是站在频率学派的角度解释的，以后我们还会用贝叶斯学派的观点。 我们注意到符号与文字的转换要非常熟练，这就像英语，如果做到同声翻译的水平，这将有利于快速理解。所以一套好的数学符号是非常关键，好数学符号令人赏心悦目。但是每个人都有不同的风格，这就有点无语，以至于不同的书，符号不一样。TMD这是英语有了方言啊。有些书上的符号真是丑的不堪入目啊。严重影响阅读学习体验。 二、回归模型1、线性回归模型：模型的一些表示方法$ y_i=\boldsymbol{x}_{i}^T\boldsymbol{\beta}+\epsilon_i=\boldsymbol{x}_{i,:\,}^T\boldsymbol{\beta}+\epsilon_i$$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $模型矩阵解释：$$\displaystyle \mathop{\boldsymbol{y}}\limits_{(n\times 1)}=\underbrace{\mathop{\boldsymbol{X}}\limits_{(n\times k)} \mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}}_{n\times k} +\mathop{\boldsymbol{\epsilon}}\limits_{(n\times 1)} $$ 2、梯度下降算法：$\boldsymbol{\beta}: =\boldsymbol{\beta}-\alpha\nabla S$梯度下降算法的本质：可以使用相图的思想加以理解。例如有关系$\displaystyle F(x,y,t)=0$。如果我们画出$$\begin{cases}\dot{x}=-3x+5y\\\dot{y}=-5x-7\sin(y)\end{cases}$$动力系统的相图。那么如果是凸函数。相图上的曲线集就会流向平衡点。如图 相图所谓梯度就是图中的箭头乘以梯度的大小。代表了该点速度最快的方向。而 $\displaystyle\alpha_k$就是给梯度加了一个控制器。所以应该能够理解梯度下降算法了：$$\boldsymbol{\beta}_{k+1}=\boldsymbol{\beta}_{k}-\alpha_k\nabla S(\boldsymbol{\beta}_k)$$所以当系统比较复杂的时候，必然就面临问题。例如这种：$$\begin{cases}\dot{x}=-x+y\\\dot{y}=xy-1\end{cases}$$这个系统就非常复杂了。初始位置不同，我们将走向完全不同的结局。相图2 3、规范方程规范方程的本质可以如下理解： 线性回归几何解释 解决学习平方误差 $\displaystyle S$的最小化问题：$$\mathop {\min }\limits_\boldsymbol{\beta}S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 现在我们用线性空间的概念来加以理解：$\displaystyle \boldsymbol{X}$张成的空间,或者说超平面 $\displaystyle span(\boldsymbol{X})=span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,j},…,\boldsymbol{x}_{:,k})$ 这里的 $\displaystyle\boldsymbol{x}_{:,j}=\left[\begin{array}{c}x_{1,j} \\x_{2,j}\\\vdots\\x_{n,j}\end{array}\right]$。如图我们很容发现要使得 $\displaystyle\boldsymbol{\epsilon} $的欧式距离最短。那么$\displaystyle\boldsymbol{\epsilon} $必然与 $\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$垂直。即有如下方程。$$\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})=\boldsymbol{0}$$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$。所以 $\displaystyle \boldsymbol{y} $的最佳估计量 $\displaystyle \hat{\boldsymbol{y} }$是 $\displaystyle \boldsymbol{y} $在$\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$空间上的投影。 注释：[^1]: 如果 $\displaystyle \zeta$表示算法,可写为$\displaystyle P(y\mid \boldsymbol{x},\mathcal{D},\zeta) $ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-08-%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-08-三大分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$分布1、定义$$\begin{align}x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0\end{align}$$ 我们有时候也这样书写： $\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\exp(-vx)$。我们称之为等待时机分布，其中 $\displaystyle v$是速度参数。 $\displaystyle n$是次数参数。随机变量的含义是：事件 $\displaystyle A$出现后，再次出现 $\displaystyle n$次需要的时间。 2、$\displaystyle \Gamma$分布的特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 我们来求 $\displaystyle \Gamma$分布的特征函数：$$\begin{align}\displaystyle \varphi(t)&amp;=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\times\mathrm{Ga}(x\mid n,v)\mathrm{d}x\\&amp;=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\mathrm{d}x\\&amp;=\int_0^{\infty}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-(vx-\mathrm{i}t x)}\mathrm{d}x\,,y=vx-\mathrm{i}t x\\&amp;=\frac{v^n}{\Gamma(n)}\int_A \left(v- \mathrm{i}t\right)^{-n}y^{n-1}\mathrm{e}^{-y}\mathrm{d}y\\&amp;=\frac{v^n}{\Gamma(n)}\frac{\Gamma(n)}{\left(v- \mathrm{i}t\right)^n}\\&amp;=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 3、$\displaystyle \Gamma$分布的数字特征期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\left(-\mathrm{i}\right)^2n \frac{v^{n+1}}{\left(v- \mathrm{i}t\right)^{n+2}}\bigg|_{t=0}=\frac{n}{v}$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\frac{n}{v^2}$ 众数$\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{Ga}(x\mid n,v)=\frac{n-1}{v}$其中：$\displaystyle \frac{\partial }{\partial x}\mathrm{Ga}(x\mid n,v)=-C\mathrm{e}^{-vx} \left( x^{n-1}v-x^{n-2}n+x^{n-2} \right)=0\to\mathrm{mode}[x] $ 4、$\displaystyle \Gamma$分布的可加性【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从$\displaystyle \Gamma$分布 $\displaystyle x_i\sim \mathrm{Ga}(x_i\mid m_i,v)$。那么$$\begin{align} \zeta=x_1+x_2+\cdots+x_n\sim\mathrm{Ga}(\zeta\mid m_1+m_2+\cdots+m_n,v)\end{align}$$ 证明：我们有 $\displaystyle \Gamma$分布的特征函数： $\displaystyle \varphi(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^n$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特征函数之积。知道： $$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^{m_1+m_2+\cdots+m_n}\end{align}$$ 于是证明了结论。 二、$\displaystyle \chi^2(x\mid n)$分布1、定义$\displaystyle \chi^2(x\mid n)$分布$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 可以看出： $\displaystyle x\sim\chi^2(x\mid n)=\mathrm{Ga}(x\mid \frac{n}{2},\frac{1}{2}) $。 【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$是相互独立的标准高斯分布，那么随机变量：$$\begin{align}\xi=\chi^2=x_1^2+x_2^2+\cdots+x_n^2\sim\chi^2(\xi\mid n)\end{align}$$ 证明： 【引理】: 若 $\displaystyle x\sim \mathcal{N}(0,1)$,则 $\displaystyle x^2\sim\chi^2(x\mid 1)$ 有$\displaystyle y=x^2$的反函数：$$x=h(y)=\begin{cases} -\sqrt{y} \,,y&lt;0 \\\ \sqrt{y}\,,0\leqslant y&lt;+\infty \end{cases}$$容易知道导数的绝对值 $\displaystyle \left|h’(y)\right|=\frac{1}{2\sqrt{y}}$。由变量代换定理有：$$\begin{align}p(y)=\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}\mathrm{e}^{-\frac{y}{2}}=\frac{2^{-\frac{1}{2}}}{\Gamma \left(\frac{1}{2}\right)}y^{\frac{1}{2}-1}\mathrm{e}^{-y/2}\end{align}$$也就是说：$$\begin{align}p(y)=\chi^2(x\mid 1)=\mathrm{Ga}(x\mid \frac{1}{2},\frac{1}{2})\end{align}$$ 然后应用 $\displaystyle \Gamma$分布的可加性容易证明定理。 2、$\displaystyle \chi^2(x\mid n)$分布的特征函数和数字特征特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(1-2 \mathrm{i}t\right)^{-n/2}\end{align}$$ 期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=-\left(\mathrm{i}\right)^2\left( 1-2\,it \right) ^{-n/2-1}n\bigg|_{t=0}=n$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=-(\mathrm{i})^2 \left( 1-2\,it \right) ^{-n/2-2} \left( n+2 \right) n-n^2=2n$ 众数 $\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{\chi}^2(x\mid n)=n-2$其中：$\displaystyle \frac{\partial }{\partial x}\chi^2(x\mid n)=1/2C\,\mathrm{e}^{-x/2}\left( x^{n/2-2}n-2\,x^{n/2-2}-x^{n/2-1} \right) =0\to\mathrm{mode}[x] $ 3、$\displaystyle \chi^2(x\mid n)$分布的可加性质【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从 $\displaystyle \chi^2$分布 $\displaystyle x_i\sim \chi^2(x_i\mid m_i)$。那么$$\begin{align} x_1+x_2+\cdots+x_n\sim\chi^2(x\mid m_1+m_2+\cdots+m_n)\end{align}$$证明：我们有 $\displaystyle \chi^2$分布的特征函数： $\displaystyle \varphi(t)=\left(1-2 \mathrm{i}t\right)^{-n/2}$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特殊函数之积。知道：$$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)= \left(1-2 \mathrm{i}t\right)^{-\left(m_1+m_2+\cdots+m_n\right)/2}\end{align}$$于是证明了结论。 三、 $\displaystyle t(x\mid n)$分布1、定义$$\begin{align}x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}\end{align}$$ 【定理】若 $\displaystyle x\sim\mathcal{N}(0,1),y\sim\chi^2(n)$,且 $\displaystyle x$和 $\displaystyle y$相互独立,那么：$$\begin{align}\tau=\dfrac{x}{\sqrt{y/n}}\sim t(\tau\mid n)\end{align}$$ 证明：知：$$\begin{align}p(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\,,-\infty&lt;x&lt;+\infty\end{align}$$ 又知：$$\begin{align}p(y)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$于是由变量代换定理有 $\displaystyle z=\sqrt{y/n}$的密度：$$\begin{align}p(z)&amp;=p(nz^2)\times\left|2nz\right|=\frac{2nz}{2^{n/2}\Gamma \left(\frac{n}{2}\right)}\left(nz^2\right)^{n/2-1}\mathrm{e}^{-(nz^2)/2}\\&amp;=\frac{\sqrt{2n}}{\Gamma \left(\frac{n}{2}\right)}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2}\end{align}$$由随机变量商的分布知：$$\begin{align}p(\tau)&amp;=\int_{-\infty}^{+\infty}p_x(\tau z)\times p_z(z)\times \left|z\right|\mathrm{d}z\\&amp;=(n\pi)^{-1/2}\frac{1}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2 \left(1+\tau^2/n\right)}\,,s=(nz^2)/2 \left(1+\tau^2/n\right)\\&amp;=(n\pi)^{-1/2}\frac{\left(1+\frac{\tau^2}{n}\right)^{-(n+1)/2}}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}s^{(n+1)/2-1}\mathrm{e}^{-s}\mathrm{d}s\\&amp;=(n\pi)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+\tau^2/n\right)^{-(n+1)/2}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-\frac{n+1}{2}}\\\end{align}$$ 即有：$$\begin{align}\tau\sim t(\tau\mid n)=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-(n+1)/2}\end{align}$$ 证毕 2、$\displaystyle t(x\mid n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]\end{align}$$ 知：$\displaystyle t(-x\mid n)=t(x\mid n)$。所以：$$\begin{align}\mathrm{E}[x^k]=0,k\in\{2a\mid a\in \mathbb{R}\}\end{align}$$ 下面我们来考虑： $\displaystyle k\in\{2a+1\mid a\in \mathbb{R}\} $$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{-\infty}^{+\infty} x^kt(x\mid n)\mathrm{d}x=\int_{-\infty}^{+\infty} x^k\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{-\infty}^{+\infty} x^k\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\,,y=1+\frac{x^2}{n}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}2\int_{1}^{+\infty}n^{\frac{k}{2}}(y-1)^{\frac{k}{2}}y^{-\frac{n+1}{2}}\frac{1}{2}n^{\frac{1}{2}}(y-1)^{\frac{1}{2}}\mathrm{d}y\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{+\infty}(y-1)^{\frac{k-1}{2}}y^{-\frac{n+1}{2}}\mathrm{d}y\,,z=\frac{1}{y}\\&amp;=-\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{0}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{-\frac{k-1}{2}}z^{\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{\frac{n-k}{2}-1}\mathrm{d}z\\&amp;=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\\\end{align}$$ 即有：$$\begin{align}\mathrm{E}[x^k]=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)},k\in\{2a+1\mid a\in \mathbb{R}\}\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=0\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n)}{n-2}\,,n&gt;2$ 四、 $\displaystyle F(x\mid m,n)$分布1、定义$$\begin{align}x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0\end{align}$$ 【定理】若有随机变量 $\displaystyle x_1\sim \chi^2(m),x_2\sim\chi^2(n)$，且 $\displaystyle x_1$和 $\displaystyle x_2$相互独立，那么：$$\begin{align}\varphi= \frac{x_1/m}{x_2/n}= \frac{\chi^2(m)/m}{\chi^2(n)/n}\sim F(\varphi\mid m,n)\end{align}$$证明：首先我们来分析一下： $\displaystyle x/n$的分布, 我们知道：$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 令 $\displaystyle x=nz$，由变量代换定理知道： $$\begin{align} z\sim \frac{2^{-\frac{n}{2}}n}{\Gamma \left(\frac{n}{2}\right)}(nz)^{n/2-1}\mathrm{e}^{-nz/2}\,,z&gt;0 \end{align}$$于是有 $\displaystyle z_1=\varphi z_2$由随机变量商的分布知：$$\begin{align}p(\varphi)&amp;=\int_{0}^{+\infty}p_{z_1}(\varphi z_2)\times p_{z_2}(z_2)\times \left|z_2\right|\mathrm{d}z_2\\&amp;=C\int_{0}^{+\infty}(\varphi z_2)^{m/2-1}\mathrm{e}^{-m\varphi z_2/2}(z_2)^{n/2-1}\mathrm{e}^{-n z_2/2}z_2\mathrm{d}z_2\\&amp;=C\varphi^{m/2-1}\int_{0}^{+\infty}z_2^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}z_2(n+m\varphi)}\mathrm{d}z_2\,v=z_2(n+m\varphi)\\&amp;=C\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\int_{0}^{+\infty}v^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}v}\mathrm{d}v\\&amp;=C\Gamma\big(\frac{m+n}{2}\big)2^{(m+n)/2}\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\,,C=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}2^{-(m+n)/2}}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\varphi^{m/2-1}\left(n+m\varphi\right)^{-(m+n)/2}\,,\varphi&gt;0\end{align}$$ 2、$\displaystyle F(x\mid m,n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{0}^{+\infty}x^kF(x\mid m,n)\mathrm{d}x\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\int_{0}^{+\infty}x^kx^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=C\int_{0}^{+\infty}x^{k+m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=Cn^{-(m+n)/2}\int_{0}^{+\infty}x^{k+m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2}\mathrm{d}x\,,y=1+\frac{m}{n}x\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{+\infty}(y-1)^{k+m/2-1}y^{-(m+n)/2}\mathrm{d}y\,z=\frac{1}{y}\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{0}(\frac{1}{z}-1)^{k+m/2-1}z^{(m+n)/2}(-z^{-2})\mathrm{d}z\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{0}^{1}(1-z)^{m/2+k-1}z^{n/2-k-1}\mathrm{d}z\\&amp;=\big(\frac{n}{m}\big)^k\frac{\mathrm{B}\left(\frac{m}{2}+k,\frac{n}{2}-k\right)}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\,,2k&lt;n\\&amp;=\big(\frac{n}{m}\big)^k\frac{\Gamma\big(\frac{m}{2}+k\big)\Gamma\big(\frac{n}{2}-k\big)}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\,,2k&lt;n\\\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=\frac{n}{n-2}\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)^2}\,,n&gt;4$ 五、评述我们通过伽马函数引出伽马分布，当然这有点突兀，怎么就突然冒出来了伽马函数？后面还有一个贝塔函数，似乎有一种神秘力量在背后。这两个函数反复出现是有原因的。有机会我们专门会抽空补充一下，让它更加自然。事实上我们可以通过指数分布引入伽马分布。 伽马分布：$\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0$ 卡方分布：$\displaystyle x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0$ t分布：$\displaystyle x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}$ F分布：$\displaystyle x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-08-三大分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>三大分布</tag>
        <tag>伽马分布</tag>
        <tag>卡方分布</tag>
        <tag>t分布</tag>
        <tag>F分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伽马函数]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-07-%E4%BC%BD%E9%A9%AC%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-07-伽马函数/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$函数$$\begin{align}\Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\,,x&gt;0\end{align}$$ 它有如下性质：1、对于 $\displaystyle x\in(0,+\infty)$，有 $\displaystyle \Gamma(x+1)=x \Gamma(x)$成立2、 $\displaystyle \Gamma(n+1)=n!\,,n=1,2,3,…$3、 $\displaystyle \log \Gamma$在 $\displaystyle (0,+\infty)$上是凸的 1、若干引理在证明上诉性质前：我们要证明 $\displaystyle \Gamma(x)$在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数。我们先说明反常积分柯西判别法的两个引理 【引理1】设 $\displaystyle f$定义在 $\displaystyle [a,+\infty)$，在任何有限区间 $\displaystyle [a,v]$上可积，且$$\begin{align}\lim_{x\to +\infty}x^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle p&gt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^{+\infty}f(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\leqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 与无界反常积分类似，也存在类似的瑕积分的判别方法【引理2】设 $\displaystyle f$定义在 $\displaystyle(a,b]$，其中 $\displaystyle a$位瑕点，在任何 $\displaystyle [v,b]\subset(a,b]$上可积，且$$\begin{align}\lim_{x\to a^+}(x-a)^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle 0&lt;p&lt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^b\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^bf(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\geqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 于是做如下分析：$$\begin{align}\Gamma(x)=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\end{align}$$ 根据引理1、2分析 $\displaystyle \Phi(x)，\Psi(x)$。我们容易得出 $\displaystyle \Gamma(x)$函数的定义域是： $\displaystyle (0,+\infty) $ 回忆一下：含参反常积分在 $\displaystyle E$上一致收敛概念： 然后我们归纳如下引理： 魏尔斯特拉斯 $\displaystyle M$判别法，以及含参反常积分一致收敛的可微性质。 【魏尔斯特拉斯 $\displaystyle M$判别法】 设有函数 $\displaystyle g(y)$，使得：$$\begin{align}\left|f(x,y)\right|\leqslant g(y)\,,(x,y)\in E\times[c,+\infty)\end{align}$$若 $\displaystyle \int_c^{+\infty}g(y)\mathrm{d}y$收敛。则 $\displaystyle \int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛 。【含参反常积分一致收敛的连续性质】设 $\displaystyle f(x.y)$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle [a,b]\in[c,+\infty)$上连续。 【含参反常积分一致收敛的可微性质】设 $\displaystyle f(x.y)$与 $\displaystyle \frac{\partial f}{\partial x}$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上收敛， $\displaystyle \int_c^{+\infty}\frac{\partial f}{\partial x}\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle E$上可微，且：$$\begin{align}\frac{\partial F}{\partial x}=\frac{\partial}{\partial x}\int_c^{+\infty}f(x,y)\mathrm{d}y=\int_c^{+\infty}\frac{\partial }{\partial x}f(x,y)\mathrm{d}y\end{align}$$ 2、$\displaystyle \Gamma(x)$是在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数下面我们来说明 $\displaystyle \Gamma(x)$的内闭一致收敛性质，对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有 $$\begin{align}\Gamma(x)&amp;=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\\&amp;\leqslant\int_0^1u^{a-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{b-1}\mathrm{e}^{-u} \mathrm{d}u&lt;\infty\end{align}$$同时考虑到 $\displaystyle u^{x-1}\mathrm{e}^{-u}&gt;0$且是连续函数。由魏尔斯特拉斯 $\displaystyle M$判别法和含参反常积分一致收敛的连续性质知道 $\displaystyle \Gamma(x)$在 $\displaystyle [a,b]\subset (0,+\infty)$一致收敛且在第一域上连续。 同时我们考虑到：$$\begin{align}\int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln u\mathrm{d}u\end{align}$$ 对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有$$\begin{align}&amp;\int_0^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u\right|\mathrm{d}y\\&amp;\leqslant\int_0^1\left|u^{x-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u\\&amp;\leqslant\int_0^1\left|u^{a-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{b-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u&lt;\infty\\\end{align}$$故 $\displaystyle \int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u$在 $\displaystyle [a,b]\subset (0,+\infty)$上一致收敛。于是由含参反常积分一致收敛的可微性质知道 $\displaystyle \Gamma(x)$在任意$\displaystyle [a,b]\subset (0,+\infty)$上可导，也是说在在定义域 $\displaystyle (0,+\infty)$上可导。且有：$$\begin{align}\Gamma^{(n)}(x)=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln^n u\mathrm{d}u\,,x&gt;0\end{align}$$容易知道： $\displaystyle \Gamma’’(x)&gt;0$。于是就证明了： $$\begin{align}\Gamma(x)是在定义域(0,+\infty)内连续且可导的凸函数。\end{align}$$证明中，一些细节并未详细说明，但是这是简单的。所以请注意。 3、其他性质的证明$$\begin{align}\int_0^a u^{x}\mathrm{e}^{-u}\mathrm{d}u&amp;=-u^x\mathrm{e}^{-u}\big|_0^a+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\&amp;=-a^x\mathrm{e}^{-a}+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\\end{align}$$令 $\displaystyle a\to+\infty$有：$$\begin{align}\Gamma(x+1)=x\Gamma(x)\end{align}$$ 若 $\displaystyle x\in \mathbb{Z}^+$有：$$\begin{align}\Gamma(n+1)=n(n-1)\cdots 2\cdot \Gamma(1)=n!\int_0^{+\infty}\mathrm{e}^{-u} \mathrm{d}u=n!\end{align}$$ 4、 $\displaystyle \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}$证明：令 $\displaystyle A=\Gamma\big(\frac{1}{2}\big)=\int_0^\infty u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u$，于是有：$$\begin{align}A&amp;=\int_0^{+\infty} u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u\,,u=t^2\\&amp;=2\int_0^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\\&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\end{align}$$ 同时有：$$\begin{align}A^2&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-x^2}\mathrm{d}x\int_{-\infty}^{+\infty}\mathrm{e}^{-y^2}\mathrm{d}y\\&amp;=\iint_{R^2}\mathrm{e}^{-(x^2+y^2)}\mathrm{d}x \mathrm{d}y\,,x=r\cos(\theta),y=r\sin(\theta)\\&amp;=\int_{0}^{2\pi}\int_{0}^{+\infty}r \mathrm{e}^{-r^2}\mathrm{d}r \mathrm{d}\theta=\frac{1}{2}\int_{0}^{2\pi}\mathrm{d}\theta=\pi\end{align}$$ 于是有：$$\begin{align} \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-07-伽马函数/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习书单]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2016-12-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在列出一些书单。若有错误，请大家指正。关键词: 机器学习,,书单,学习资源 机器学习资源因为大数据的兴起，机器学习大热。这一行业吸引了大量人才涌入。然后现状是学习机器学习需要较高的数学知识，导致人才供应不足。跟不上行业发展需求。较高的工资和技术时尚吸引大量的人们争相学习机器学习、人工智能。故收集网络整理出这些书籍： 网络上建议的入门顺序：1、斯坦福大学公开课 ：机器学习课程2、李航.统计学习方法3、Machine Learning in Action(机器学习实战） 解释一下这个入门：在这之前你可以简单了解一下，周志华：数据挖掘与机器学习，，斯坦福的公开课可以带你入门。《统计学习方法》可以带你了解基本理论和推导。《机器学习实战》可以让你了解一下实操。这个之后有两个路径，看大神Bengio的《deep learning》，或者继续深化机器学习的知识。当然你也可以参加这个：数据分析竞赛kaggle 附照片&lt;img src=http://oiol5pi05.bkt.clouddn.com/Yoshua_Bengio.jpg width=20%&gt;Yoshua BengioFull ProfessorDepartment of Computer Science and Operations ResearchCanada Research Chair in Statistical Learning Algorithms 解决了机器学习入门问题，我们接下来要进阶：1、PRML（Pattern Recognition and Machine Learning）2、ESL（The Elements of Statistical Learning ）3、MLAPP(Machine Learning: a Probabilistic Perspective) 4、Deep learning-author Yoshua Bengio 上面的书单应该是经典的四本书了。个人比较偏爱《MLAPP》，符号比较优美，叙述比较全面。当然你也可以看模式识别的书，例如《统计模式识别（第3版）Statistical Pattern Reco》，那个封面是豹子的书。 &lt;img src=http://oiol5pi05.bkt.clouddn.com/%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB.jpg width=”50%”&gt; 一些理论当然看这些书的时候，你也许会发现，凸优化，图模型，EM，MCMC之类的，你可以通过下面的书深入一下：1、Convex Optimization2、Probabilistic Graphical Models3、The EM Algorithm and Extensions4、Simulation Fifth Edition Sheldon M. Ross 实战编程1、Python for Data Analysis2、SciPy and NumPy3、Machine Learning for Hackers（这本是用R的）4、集体智慧编程（这个书名有点误导人） 网上搜索很容易找到。现在（2017）又出来很多新书，也发生了很多事情。就不一一列举。 好玩的一本书1、Bad Data Handbook 最后这些书我也还没看。。。。。我会把它们看完的！！！！^_^ 1我们还是来点别的吧： 神经网络学习机器学习之前，你应该先了解、学习神经网络。我知道现在媒体关注点在深度学习，机器学习上。网络上的学习路线也多少是从机器学习开始：从线性模型到深度学习。 可以是，你要知道Hinton的深度学习是挖掘了神经网络的潜能，Hinton是要把被抛弃、被侮辱、几起几落的神经网络再度复兴。在被人摒弃的10年中，加拿大多伦多大学的Geoffery Hinton教授依然坚守。2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。 Geoffery Hinton教授&lt;img src=http://oiol5pi05.bkt.clouddn.com/Geoffrey%20Hinton.jpg width=100%&gt; 与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。 他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。所以我觉得从神经网络到深度学习或者说从神经元到深度学习是会对人工智能有更好的认识。而不是机器学习。神经网络有着传奇的历史，如果你了解它，你将为之着迷。 Andrew Ng对神经网络的看法：&lt;img src=http://oiol5pi05.bkt.clouddn.com/Andrew%20Ng%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%81%E5%BA%A6.jpg width=100%&gt; 神经网络的书籍在这里：1、人工神经网络教程 第一版、第二版 韩立群2、神经网络原理3、神经网络设计3、神经网络与机器学习4、Matlab与神经网络，这种书籍就非常多了。 下面是我口号：$$\displaystyle Life=\int_{Birth}^{Death} (Learning+Working) \mathrm{d}t$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行列式]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-24-%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[摘要：本文意在理清行列式的基础问题。若有错误，请大家指正。关键词: 行列式,机器学习 一、定义函数集合$\displaystyle V=\{f\mid f$是矩阵$M_n(K)$上的数量函数$\}$ $\displaystyle V_1=\{f\mid f\in V,$且满足列线性性$\} $ $\displaystyle V_2=\{f\mid f\in V_1,$且满足列反对称性$\}$ $\displaystyle V_3=\{f:f\in V_2,$且满足规范性$\} $ 容易验证， $\displaystyle V,V_1,V_2$关于加法和数乘运算构成函数空间。 二、性质性质1$\displaystyle V$是 $\displaystyle K$上的 $\displaystyle n^2$元函数空间，所以有 $\displaystyle \dim(V)=\infty$ 性质2$\displaystyle \dim(V_1)=n^n$，其一组基底是 $\displaystyle f_{i_1i_2\cdots i_n}(\varepsilon_{j_1}，\varepsilon_{j_2},\cdots,\varepsilon_{j_n})=\delta_{i_1j_1}\delta_{i_2j_2}\cdots\delta_{i_nj_n}$ 性质3$\displaystyle \dim(V_2)=1，V_2=span(\det(\cdot))$，其中 $\displaystyle \det(\cdot)$为行列式函数。 性质4$\displaystyle V_3=\{\det(\cdot)\}$ 三、证明性质2的证明定义 $\displaystyle n^n$维空间 $\displaystyle L=\{(c_{i_1i_2\cdots i_n}):1\leq i_1,i_2,\cdots,i_n \leq n \}$ 定义映射空间 $\displaystyle V_1$到 $\displaystyle L$的线性映射，从而有 $$\begin{align}\dim(V_1)=\dim(L)=n^n\end{align}$$ 1.【H是单射】 若 $\displaystyle f，g \in V_1且H(f)=H(g)=(c_{i_1i_2\cdots i_n})$则有 $$\begin{align}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=c_{i_1i_2\cdots i_n}\end{align}$$又$$\begin{align}f\big(\alpha_1,\alpha_2,\cdots,\alpha_n)= f(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$ $$\begin{align}g\big(\alpha_1,\alpha_2,\cdots,\alpha_n\big)= g\big(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$所以有 $$\begin{align}f=g\end{align}$$ 2.【H是满射】 同时任意给定 $\displaystyle (c_{i_1i_2\cdots i_n})\in L$，定义函数 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}c_{i_1i_2\cdots i_n}\end{align}$$ 可以验证 $\displaystyle f \in V_1$ ，此时有$$\begin{align}H(f)=(c_{i_1i_2\cdots i_n})\end{align}$$所以 $\displaystyle H$是满射。根据同态映射 $\displaystyle H$我们不难找到 $\displaystyle V_2$的一组基底 性质3的证明若 $\displaystyle f\in V_2$，由列反对称性，有 $$\begin{align}f(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n})=\begin{cases}0 &amp;\exists s,t,\to i_s=i_t\\(-1)^{\tau(i_1i_2\cdots i_n)}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)&amp; i_1,i_2,\cdots ,i_n \textit{ pairwise unequal }\end{cases}\end{align}$$ 其中 $\displaystyle \tau(i_1i_2\cdots i_n)$为排列 $\displaystyle i_1.i_2,\cdots,i_n$的逆序数。所以我们有 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{i_1,i_2,\cdots,i_n\textit{ pairwise unequal } }(-1)^{\tau(i_1i_2\cdots i_n)}a_{i_1}a_{i_2}\cdots a_{i_n}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)\end{align}$$从而有 $$\begin{align}f(A)=f(E)\cdot \det(A)\end{align}$$ 容易验证具有该表达式的函数 $\displaystyle f$属于 $\displaystyle V_2$ 性质4的证明若 $\displaystyle f\in V_3$,则有 $$\begin{align}f(A)=f(E)\cdot \det(A)=\det(A)\end{align}$$]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学分析笔记-数的建立]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-23-%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0-%E6%95%B0%E7%9A%84%E5%BB%BA%E7%AB%8B01%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清数的问题。若有错误，请大家指正。关键词: 有理数,实数 这是测试文章： 一、数的建立1、有理数的缝隙数的建立是数学分析的基础。实数的最小上界性质是我们开启现代数学的钥匙。 关于$p=\sqrt{2}$的重要定律： 构造一个数 $\displaystyle z=\frac{2x+2}{x+2}=x-\frac{x^2-2}{x+2}$ 同时令 $\displaystyle A=\{a\mid a^2&lt;2,a\in \mathbb{Q}^+\}$ 又有 $\displaystyle B=\{b\mid b^2&gt;2, b \in \mathbb{Q}^+\}$ 我们有 $\displaystyle z^2-2=\frac{2(x^2-2)}{(p+2)^2}$ 即可证明$\displaystyle A$ 里面没有最大的数$$\begin{align}x \in A,\to z \in A\,, z&gt;x\end{align}$$ $\displaystyle B$ 里面没有最小的数$$\begin{align}\,x \in B\to z \in B\,,0&lt;z&lt;x\end{align}$$ 这说明尽管有理数之间还有有理数，但是有理数系还是有空隙。而实数系填满了这些空隙，这就是实数系能在分析学中能起基础作用的主要原因。 $\displaystyle \ell \mathbb{ABC}$ 二、行列式行列式定义： $\displaystyle det: M_n(F) \to \Bbb{R} $ 行列式表达式$$\begin{align}det(A)=\sum _{\sigma \in S_n}sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }\end{align}$$ 其中$\sigma$是集合$X=\{1,2,\,…\,n\}$上的置换：$\sigma: X \to X$。$S_n$是置换$\sigma$的集合,易知$S_n$是一个对称群。$\tau(\sigma)$是$\sigma$的逆序数。$\displaystyle sgn(\sigma)=\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)=(-1)^{\tau(\sigma)}$是置换的符号函数。于是： $$\displaystyle det(A)=\sum _{\sigma \in S_n}\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)\prod _{i=1}^n A_{\sigma(i)i }=\sum _{\sigma \in S_n}(-1)^{\tau(\sigma)}\prod _{i=1}^n A_{\sigma(i)i }$$ 简记为：$$det(A)=\sum _{\sigma \in S_n} sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }$$ 三、【函数的极限】令$X$和$Y$是度量空间，假设$E \subseteq X$、$\bm{f}$将$E$映入$Y$内、且$\bm{p}$是$E$的极限点。如果 $\forall \epsilon\,,\exists \delta&gt;0$，对于 $\{\bm{x} \in E\mid 0&lt;d_{X}(\bm{x},\bm{p})&lt;\delta\}$中一切点 $\bm{x}$，使得 $\displaystyle d_{Y}(\bm{f}(\bm{x}),\bm{q})&lt;\epsilon,\,\bm{q}\in Y$成立。就说: $$\lim_{\bf{x} \to\bm{p}} \bm{f}(\bm{x})=\bm{q}$$ 卷积$\displaystyle \mathbf{C}=\mathbf{X}*\mathbf{W}$ $\displaystyle \bm{C}=\bm{X}*\bm{W}$ 分段函数$$\displaystyle f(n)= \left\{\begin{matrix} n/2, &amp; \text {if $n$ is even} \\\ 3n+1, &amp; \text{if $n$ is odd} \end{matrix}\right. \\$$ $\displaystyle \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4 \end{bmatrix}$ 我们可以看到 session.run专业我们就可以Variable 1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello).decode('utf-8')) 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
        <tag>数学</tag>
        <tag>现代数学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyhton-notebook主题颜色配置]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2016-05-01-pyhton-notebook%E4%B8%BB%E9%A2%98%E9%A2%9C%E8%89%B2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、安装jupyter-themer插件12345678910111213➜ ~ cd anaconda➜ anaconda sudo pip install jupyter-themerPassword:The directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting jupyter-themer Downloading jupyter-themer-0.3.0.tar.gz (40kB) 100% |████████████████████████████████| 40kB 35kB/sRequirement already satisfied: jupyter in ./lib/python3.6/site-packages (from jupyter-themer)Requirement already satisfied: notebook in ./lib/python3.6/site-packages (from jupyter-themer)Installing collected packages: jupyter-themer Running setup.py install for jupyter-themer ... doneSuccessfully installed jupyter-themer-0.3.0 二、设定主题1、语法介绍：1234567%语法格式如下usage: jupyter-themer [-c COLOR, --color COLOR] [-l LAYOUT, --layout LAYOUT] [-t TYPOGRAPHY, --typography TYPOGRAPHY] [-f CODE_FONT, --font CODE_FONT] [-b BACKGROUND, --background BACKGROUND] [-s OPTION, --show OPTION] 2、显示所有可选颜色主题：12345678910111213141516171819202122232425262728293031323334353637383940414243➜ anaconda jupyter-themer --show color3024-day3024-nightabcdefambiancebase16-darkbase16-lightblackboardcobaltcolorforthdraculaeclipseeleganterlang-darkicecoderlesser-darkliquibytematerialmbomdn-likemidnightmonokaineatneonightparaiso-darkparaiso-lightpastel-on-darkrubybluesetisolarized-darksolarized-lightthe-matrixtomorrow-night-brighttomorrow-night-eightiesttcntwilightvibrant-inkxq-darkxq-lightyetizenburn➜ anaconda 3、设定monokai主题12➜ anaconda jupyter-themer -c monokaiCustom jupyter notebook theme created - refresh any open jupyter notebooks to apply theme. 效果如下 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>notebook</tag>
        <tag>计算环境</tag>
        <tag>工程问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夜晓风]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2015-09-19-%E5%A4%9C%E6%99%93%E9%A3%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2015-09-19-夜晓风/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夜晓风皎月榕树头，夜晓风。 一曲江畔，感似岁月悠悠， 山河在，壮志未酬。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2015-09-19-夜晓风/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[穗夜]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2014-06-19-%E7%A9%97%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2014-06-19-穗夜/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 穗夜夜声碎月光如练，榕树头江畔愁眠。 桥头浪卷已三更，沉沉幕夏雨如注。 引线小白写于2014-6-19 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2014-06-19-穗夜/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>穗夜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再回大学城]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-05-20-%E5%86%8D%E5%9B%9E%E5%A4%A7%E5%AD%A6%E5%9F%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-05-20-再回大学城/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 再回大学城沉浸校园曲风气爽清秋节，叶至桂香来，小湖椰影廊桥，曾记否，谷围晓月，灯影朦胧，勘回首，沉浸校园曲风，且看从容，壮志未酬笑谈中。 引线小白写于2011-5-20 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-05-20-再回大学城/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>大学城</tag>
        <tag>诗歌</tag>
        <tag>小谷围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夏雨急]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-04-30-%E5%A4%8F%E9%9B%A8%E6%80%A5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-04-30-夏雨急/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夏雨急夏雨急，落如珠， 今朝且看风疾。 迎地扫，路人急， 此地空余。。。 小白写于2011-4-30日 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-04-30-夏雨急/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏雨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初夏里]]></title>
    <url>%2F%E4%BC%A4%E6%84%9F%2F2011-04-27-%E5%88%9D%E5%A4%8F%E9%87%8C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/伤感/2011-04-27-初夏里/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 初夏里清风夜，晓月枝头，照斑驳绿影初夏里，世事心愁，思长夜忧忧叹悲歌，一曲无穷，怎泪眼朦胧自惆怅，知与谁同，独望江水流 引线小白写于2011-4-27广州 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/伤感/2011-04-27-初夏里/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>伤感</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>初夏里</tag>
        <tag>引线小白</tag>
        <tag>诗词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[烈日炎炎]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2010-08-04-%E7%83%88%E6%97%A5%E7%82%8E%E7%82%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 烈日炎炎日炎炎，清风绝。惜雨荫少，浪浪绵绵。汗易求冰难接。是个酷热天！ 引线小白写于2010-8-4 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏天</tag>
        <tag>日日炎炎</tag>
      </tags>
  </entry>
</search>
