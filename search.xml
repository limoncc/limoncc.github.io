<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[macOS_mojave_10.14.6下Tensorflow2.6的安装与SSE, AVX, FMA]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2021-09-18-mac%E5%AE%89%E8%A3%85tensorflow2.6%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2021-09-18-mac安装tensorflow2.6/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、下载源码本人电脑macOS mojave 10.14.6, 编译tensorflow解决SSE4.1 SSE4.2 AVX指令集问题。 先下载，进入目录定位到v2.6.0123git clone https://github.com/tensorflow/tensorflow.gitcd tensorflowgit checkout v2.6.0 二、准备必要环境准备必要环境，请安装好java,和minconda 12345678brew install bazelisk conda create -n pencil python=3.8conda activate pencilpip install -U pip pip install -U sixpip install numpy==1.19.5pip install -U wheelpip install -U keras_preprocessing --no-deps 三、开始编译关键语句，-march=native会做cpu指令集优化 bazelisk build -c opt –copt=-march=native //tensorflow/tools/pip_package:build_pip_package 12345678910111213141516171819202122232425262728293031323334353637383940./configureYou have bazel 3.7.2 installed.Please specify the location of python. [Default is /Users/xiaobai/miniconda/envs/dawn/bin/python3]:Found possible Python library paths: /Users/xiaobai/miniconda/envs/dawn/lib/python3.8/site-packagesPlease input the desired Python library path to use. Default is [/Users/xiaobai/miniconda/envs/dawn/lib/python3.8/site-packages]Do you wish to build TensorFlow with ROCm support? [y/N]: nNo ROCm support will be enabled for TensorFlow.Do you wish to build TensorFlow with CUDA support? [y/N]: nNo CUDA support will be enabled for TensorFlow.Do you wish to download a fresh release of clang? (Experimental) [y/N]: nClang will not be downloaded.Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -Wno-sign-compare]:Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: nNot configuring the WORKSPACE for Android builds.Do you wish to build TensorFlow with iOS support? [y/N]: nNo iOS support will be enabled for TensorFlow.Preconfigured Bazel build configs. You can use any of the below by adding "--config=&lt;&gt;" to your build command. See .bazelrc for more details. --config=mkl # Build with MKL support. --config=mkl_aarch64 # Build with oneDNN and Compute Library for the Arm Architecture (ACL). --config=monolithic # Config for mostly static monolithic build. --config=numa # Build with NUMA support. --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects. --config=v1 # Build with TensorFlow 1 API instead of TF 2 API.Preconfigured Bazel build configs to DISABLE default on features: --config=nogcp # Disable GCP support. --config=nonccl # Disable NVIDIA NCCL support.Configuration finishedbazelisk build -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 在/tmp/tensorflow_pkg找到wheel文件，pip安装即可。目前编译了这些，看官按需下载 下载链接 tensorflow-2.6.0-cp38-cp38-numpy_1.21.2-macosx_10_14_x86_64.whl tensorflow-2.6.0-cp37-cp37m-macosx_10_14_x86_64.whl tensorflow-2.6.0-cp38-cp38-macosx_10_14_x86_64.whl 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2021-09-18-mac安装tensorflow2.6/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大数据</tag>
        <tag>人工智能</tag>
        <tag>深度学习</tag>
        <tag>工程问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测度论与革命一]]></title>
    <url>%2F%E7%A7%91%E6%99%AE%2F2019-01-01-%E6%B5%8B%E5%BA%A6%E8%AE%BA%E4%B8%8E%E9%9D%A9%E5%91%BD%E4%B8%80%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/科普/2019-01-01-测度论与革命一/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文主要用革命的比喻，通俗的总结了测度论的基本问题，并且添加了我自己的一些体会。若有错误，请大家指正。关键词: 测度论,科普,革命 一、开篇大家好，这里用黑帮或者革命的故事，通俗讲解集合论、测度论的基本知识。帮助大家理解这门颇为难懂的数学知识。本系列的初衷，是以前在学习时的一些困惑，教科书过于冰冷与严肃，很多概念不知所云，理解学习数学最好的方法之一就是举例子，看数学书本身就是解读故事的过程，故事的精彩取决于你的理解。希望这种看数学书就像看故事书的体验，大家也能有所感受。这就是我的初衷。 初次开篇，写的还不够科普，在之后的岁月我会尽量通俗化，清晰化，故事化，大家敬请期待。也是我博客很久没有更新了，未来推进自身构建可计算知识，我准备开始写些文章，更新不定。本文主要内容，在知乎问答回答过，属于原创，请遵守版权声明。 集合是数学的基石，初等集合论是非常直观的。问题是一旦引入无限的概念，理论就开始复杂起来。在无限的影响下诞生了很多概念。下面我们就来说明一下常用的点集，我们先回顾一下下面点集的定义： 内点：$\displaystyle \exists r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：某种关系下的小弟（包括他自己） 外点：$\displaystyle \forall r &gt;0\,,U(x_0,r)=\{x \in S \mid d(x,x_0)&lt;r\}$：所有各种关系下的小弟（包括他自己） 边界点：$\displaystyle \forall r &gt;0\,,\mathring{U}(x_0,r)=\{x \in S-\{x_0\}\mid d(x,x_0)&lt;r\}$：所有各种关系（不包括他自己）的小弟 此处应该有配图，稍后补上。 其中$\displaystyle S$是某一集合、 $\displaystyle x_0$是我们的点集里面点、 $\displaystyle d(x,x_0)$表示距离，这个距离是抽象的，并不一定是欧几里得距离。我们将点集的定义翻译为人话，或者一个故事情节。基于此，我们来开始我们的旅程。 二、点集の想像一如果某国 $\displaystyle S$有一个无限人数组成的黑帮 $\displaystyle A=\{x \in R \mid x $是黑帮一成员$\} \subset S$ 说 $ x$是内点：意思是说 $ x$在某种关系下所有小弟（包括他自己）是黑帮的成员。显然 $ x$自己是黑帮成员 说 $ x$是外点：意思是说 $ x$在某种关系下的所有小弟（包括他自己）都不是的黑帮成员，显然x 自己不是黑帮成员 说 $ x$是边界点：意思是说 $ x$所有各种关系下的有一部分小弟（包括他自己）都是的黑帮成员，显然 $ x$自己可能是，也可能不是黑帮成员 说 $ x$是孤立点：意思是说 $ x$在所有关系下的小弟（不包括他自己）都不是的黑帮成员，但是他自己是黑帮成员，所以孤立点这个名词：还是非常形象的。显然孤立点是边界点的成员并且是黑帮成员。 三、点集の想像二1、可见孤立点 $\displaystyle x_0$ 是忠诚的，那么他应该是黑帮最可以信赖的人，因为他只和组织 $A$ 有关系 $\displaystyle x_0\in A$。但是关系似乎不是很好，是个独行侠。是一把利刃。但是不忠诚的话，也有可能是警方卧底。因为他和组织外有关系。 2、内点，很显然他们组织的基础。孤立点是边界点中，是黑帮成员那一部分。 3、边界点是组织要提防的争取的人。 4、外点，显然不是非常重要的。 同理：某国 $\displaystyle S$有一个有限人数组成的黑帮，上述定义也是有意义的。 四、点集の想像三我们来继续考虑 $\displaystyle S$国的一个无限人数组成的黑帮 $\displaystyle A$$\displaystyle x$是极限点： 意思是说 $\displaystyle x$在所有各种关系（不包括他自己）总有一个小弟是黑帮成员。 显然 $ x$ 自己可能是，也可能不是黑帮成员。 $\displaystyle x$是聚点：意思是说 $ x$在所有各种关系下有无数个小弟是黑帮成员。 五、点集の想像四1、我们发现极限点和聚点其实是一个意思。证明请看 bady rudin 2.20定理。还有一些书上有附着点 $ x$的定义，是一个意思。2、显然聚点：是带特殊关系的黑帮成员或者是非黑帮成员。通过组织其他的人，总可以环环相扣找到他聚点。3、如果聚点不是黑帮成员，可以设想他是某个高官。是这个黑帮的后台，但是不是黑帮成员，但是通过组织其他的人，总可以环环相扣找到他。4、如果聚点是黑帮成员，可以想想，他混的不错，是个能人 六、点集の想像五如果某国 $\displaystyle S$有一个有限人数组成的黑帮 B我们可以发现，黑帮 $\displaystyle B$中没有极限点。证明请看baby rudin 2.20定理。 七、点集の想像六显然我们思考有限和无限时，内、边、外和孤是可以的。对于无限，还有聚点的概念。而且有 Weierstrass Theorem魏尔斯特拉斯聚点定理： $\displaystyle \Bbb{R}^n$中每个有界无限子集在 $\displaystyle \Bbb{R}^n$中有聚点。 换句话说， 每个黑帮都有一个强大的高官后台或者内部的能人。这就是神秘无限世界的一瞥。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/科普/2019-01-01-测度论与革命一/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>科普</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>科普</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类分布大意]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-05-06-%E5%88%86%E7%B1%BB%E5%88%86%E5%B8%83%E5%A4%A7%E6%84%8F%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理清分类分布的基础问题。若有错误，请大家指正。关键词: multinoulli,Categorical,softmax 一、分类分布的若干形式1.1、分类分布指示形式第一，回顾一下猫猫分布 (multinoulli distribution)或者叫 (Categorical distribution)。因为有英文 cat，我又叫它猫猫分布🐈：$$\begin{align}\mathrm{Cat}(x\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{\mathbb{I}(x=c)},x\in\{1,…,C\}\end{align}$$其中$\displaystyle \bm{\mu}=[\mu_1,\mu_2,…,\mu_C]^\text{T}\,,\sum_{c=1}^{C}\mu_c=\bm{1}^\text{T}\bm{\mu}=1$ 1.2、分类分布0-1编码形式第二、特别的，我们使用0-1编码来表示分类时，我们有：$$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{c=1}^{C}\mu_c^{x_c}\,,\bm{x}=[x_1,x_2,…,x_C]^\text{T}\,,\bm{x}\in\{0,1\}^C\end{align}$$其中 $\displaystyle \sum_{c=1}^{C}x_c=\bm{1}^\text{T}\bm{x}=1$ 1.3、分类分布指数族形式第三、继续使用0-1编码，现在我们使用指数族的思想，对分类分布，也就是这个猫猫加以变形，这形式是我们要经常用到的形式： $$\begin{align}\mathrm{Cat}(\bm{x}\mid\bm{\mu})&amp;=\prod_{c=1}^{C}\mu_c^{x_c}=\exp\bigg[\sum_{c=1}^C x_c\ln\mu_c\bigg]=\exp\bigg[\bm{x}^\text{T}\ln \bm{\mu}\bigg]\\&amp;=\exp\bigg[\bm{x}^\text{T}\ln\frac{\bm{\mu}}{\mu_C}+\ln \mu_C\bigg]\end{align}$$我们定义一个新的向量 $\displaystyle \bm{\eta}=\ln\frac{\bm{\mu}}{\mu_C}$，于是有：$$\begin{align}\bm{\mu}=\mathcal{S}\big(\bm{\eta}\big)=\mathrm{softmax}\big(\bm{\eta}\big)=\Bigg[\frac{\mathrm{e}^{\eta_c}}{\sum_{c=1}^{C}\mathrm{e}^{\eta_c}}\Bigg]_{1\times C}=\frac{\mathrm{e}^\bm{\eta}}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\end{align}$$亦有$$\begin{align}\bm{x}\sim \mathrm{Cat}\big(\bm{x}\mid \mathcal{S}\big(\bm{\eta}\big)\big)=\exp \big[\bm{x}^\text{T}\bm{\eta}-\ln [\bm{I}^\text{T}\mathrm{e}^\bm{\eta}]\big]\end{align}$$ 二、分类分布、sofxmax和多元logistic回归。2.1、多元logistic回归模型考虑如下广义线性模型连接函数 $\displaystyle g \big(\bm{\mu}\big)=\bm{\eta}=\bm{W}\bm{x}\to \bm{\mu}=\mathcal{S}\big(\bm{W}\bm{x}\big)$。这一模型的形式具有十分重要的意义，首先让我们把诸多特征与与分类建立的了数学模型，其次分类分布的期望是一组归一化的概率，直接代表了我们对一次特征观测应该对应于哪个分类的信心。辅助以决策论，很容易做出推断。$$\begin{align}\bm{y}\sim \mathrm{Cat}\big(\bm{y}\mid \mathcal{S}\big(\bm{W}\bm{x}\big)\big)\end{align}$$我们有对数似然：$$\begin{align}\ell \big(\mathcal{D}\mid \bm{W}\big)&amp;=\ln \prod_{i=1}^N\prod_{c=1}^{C}\mathcal{S}_{ic}^{y_{ic}}=\sum_{i=1}^N \bm{y}_i ^\text{T}\ln \mathcal{S}\big(\bm{W}\bm{x}_i\big)=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{W}\bm{x}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{W}\bm{x}_i\big]\bigg)\bigg]\\&amp;=\sum_{i=1}^N\bigg[\bm{y}_i ^\text{T}\bm{\eta}_i-\ln \bigg(\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]\bigg)\bigg]\end{align}$$ 2.2、多元logistic回归的梯度我们有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\sum_{i=1}^N \Bigg[\bm{y}_i- \frac{\mathrm{diag}\big[\mathrm{e}^{\bm{\eta}_i}\big]\bm{I}}{\bm{I}^\text{T} \exp \big[\bm{\eta}_i\big]}\Bigg]=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\end{align}$$有如下微分$$\begin{align}\mathrm{d}\ell=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i=\mathrm{tr}\bigg(\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{\eta}_i\bigg)\end{align}$$注意到 $\displaystyle \bm{\eta}_i=\bm{W}\bm{x}_i\to \mathrm{d}\bm{\eta}_i=\mathrm{d}\bm{W}\bm{x}_i$于是根据数量函数与矩阵微分的地定义有 ：$$\begin{align}&amp;\mathrm{d}\ell=\mathrm{tr}\bigg(\frac{\partial \ell}{\partial \bm{\eta}_i ^\text{T}}\cdot\mathrm{d}\bm{\eta}_i\bigg)=\sum_{i=1}^N\mathrm{tr}\Big( \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\bm{x}_i\Big)=\sum_{i=1}^N\mathrm{tr}\Big( \bm{x}_i\big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]^\text{T}\mathrm{d}\bm{W}\Big)\\&amp;\Longrightarrow \frac{\partial \ell}{\partial \bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}\end{align}$$也就是说我们有梯度$$\begin{align}\nabla_{\bm{W}}=\sum_{i=1}^N \big[\bm{y}_i- \mathcal{S}\big(\bm{\eta}_i\big)\big]\bm{x}_i^\text{T}=\sum_{i=1}^N \big[\bm{y}_i- \bm{\mu}_i\big]\bm{x}_i^\text{T}\end{align}$$ 2.3、多元logistic回归的海赛矩阵注意到梯度的维数是 $\displaystyle D\times D$，应用 $\displaystyle \frac{\partial \big[a(\bm{x})\bm{f}(\bm{x})\big]}{\partial \bm{x}^\text{T}}=a\frac{\partial \bm{f}}{\partial \bm{x}^\text{T}}+\bm{f}\frac{\partial a}{\partial \bm{x}^\text{T}}$，于是我们注意到有如下结论$$\begin{align}\dot{\mathcal{S}}&amp;=\frac{\partial \mathcal{S}\big(\bm{\eta}\big)}{\partial \bm{\eta}^\text{T}}=\frac{1}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]-\mathrm{e}^\bm{\eta}\left[\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]\bm{I}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}\right]^\text{T}\\&amp;=\frac{\mathrm{diag}\big[\mathrm{e}^\bm{\eta}\big]}{\bm{I}^\text{T}\mathrm{e}^\bm{\eta}}-\frac{ \mathrm{e}^\bm{\eta}\big[\mathrm{e}^\bm{\eta}\big]^\text{T}}{\big[\bm{I}^\text{T}\mathrm{e}^\bm{\eta}\big]^2}=\mathrm{diag}\big[\bm{\mu}\big]-\bm{\mu}\bm{\mu}^\text{T}\end{align}$$现在我们更进一步求这个矩阵梯度的微分，同时注意到有( $\displaystyle \mathrm{vec}\big(\bm{A}\bm{X}\bm{B}\big)=\big[\bm{B}^\text{T}\otimes \bm{A}\big]\mathrm{vec}\big(\bm{X}\big)$)，于是我们可以导出矩阵导数：$$\begin{align}&amp;\mathrm{d}\nabla=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{\eta}_i \cdot\bm{x}_i ^\text{T}=\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\\\Longrightarrow&amp;\mathrm{vec}\big(\mathrm{d}\nabla\big)=\mathrm{vec}\bigg(\sum_{i=1}^N\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\bigg)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N \mathrm{vec}\big(\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\cdot\mathrm{d}\bm{W}\cdot\bm{x}_i\cdot \bm{x}_i ^\text{T}\big)\\\Longrightarrow&amp;\mathrm{d}\mathrm{vec}\big(\nabla\big)=\sum_{i=1}^N\big[\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)\big]\mathrm{d}\mathrm{vec}\big(\bm{W}\big)\\\Longrightarrow&amp;\bm{H}=\frac{\partial \mathrm{vec}\big(\nabla\big)}{\partial \mathrm{vec}^\text{T}\big(\bm{W}\big)}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes\dot{\mathcal{S}}\big(\bm{\eta}_i\big)=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{align}$$为了方便计算，我们把梯度也向量化，于是我们有$$\begin{align}\begin{cases}\displaystyle\nabla=\mathrm{vec}\big(\nabla_{\bm{W}}\big)=\sum_{i=1}^N\bm{x}_i \otimes\big[\bm{y}_i- \bm{\mu}_i\big]\\\displaystyle\bm{H}=\sum_{i=1}^N\bm{x}_i \bm{x}_i ^\text{T}\otimes \big[\mathrm{diag}\big[\bm{\mu}_i\big]-\bm{\mu}_i\bm{\mu}_i^\text{T}\big]\end{cases}\end{align}$$牛顿-拉弗迭代法有：$$\begin{align}\mathrm{vec}\big(\bm{W}\big):=\mathrm{vec}\big(\bm{W}\big)-\bm{H}^{-1}\nabla\end{align}$$当然各种优化方法都可以加以应用。 三、评述1、分类分布，有人又叫多项式分布，这里n=1，不过我不太赞成这个说法，使用分类分布更加恰当。当然更萌(๑•ᴗ•๑)一点可以叫猫猫分布。2、分类分布是一种很特殊的分布：对任意一个连续分布，如果我们将其离散化，我们都可以归结为一个分类分布。3、分类问题可以归结为：对特征向量分类，或者说对特征空间进行划分。由于与分类分布的关系使得 softmax函数经常出现，一般把它放在神经网络的最后一层来输出概率。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-05-06-分类分布大意/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类分布</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-04-05-%E6%84%9F%E7%9F%A5%E5%99%A8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 神经网络神经网络是由简单处理单元构成的大规模并行分布式处理器。 神经元 $$\begin{align}o_j(t)=f\{[\sum_{i=1}^{n}w_{ij}x_i(t-\tau_{ij})]-T_j\}\end{align}$$ 单神经元—感知机模型（Perceptron）模型的建立1943年生理学家W.S.McCulloch和W.A.Pitts提出了形式神经元数学模型，史称M-P模型。开创了神经科学理论研究的新时代！ 1949年心理学家Donald Olding Hebb在《行为构成》（Organization of Behavior）中提出了Hebb算法。而且首次提出了连接主义(connectionism)：大脑活动是靠脑细胞的组合连接实现的。 1958年美国Frank Rosenblatt提出了感知机（Perceptron）这应该是世界上第一个真正优秀的人工神经网络 概念与符号下面我们将说明这个模型，1、 $\displaystyle \boldsymbol{x}=[x_1,…,x_k]^\text{T}$为特征向量， 或者叫输入向量2、 $\displaystyle \boldsymbol{w}=[w_1,…,w_k]^\text{T}$为权值向量，也可以叫突触权值向量 ，模仿神经元的突触。3、 $\displaystyle w_0$为阈值。 $\displaystyle o=f$4、为简洁记令 $\displaystyle x_0=1$,于是有增广特征向量 $\displaystyle \boldsymbol{x}:=[x_0,x_1,…,x_k]^\text{T}$。5、增广权值向量$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$。6、定义输出 $\displaystyle o$，7、激活函数：硬限函数 $\displaystyle \mathrm{hardlims}(x)=\begin{cases} 1, &amp;x\geqslant0 \\\ -1, &amp;x&lt;0\end{cases}$。于是有：$$\begin{align}o=\mathrm{hardlims}\left(\boldsymbol{w}^\text{T}\boldsymbol{x}\right)\end{align}$$ 神经元 感知机学习与训练感知机学习算法：误差修正学习算法对于二分类增广数据集 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$，注意我们用的都是增广数据，$\displaystyle \boldsymbol{x}:=[1,x_1,…,x_k]^\text{T}$，$\displaystyle \boldsymbol{w}:=[w_0,w_1,…,w_k]^\text{T}$，同时 $\displaystyle y=+1\text{ or }-1$，表示 $\displaystyle c_1 \text{ or } c_2$类。 学习误差$\displaystyle y_t-o_t$于是有： 算法：误差修正学习算法1 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\eta_t(y_t-o_t)\boldsymbol{x}_t\end{array}\\$3 #end while 如果我们定义 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样取得新的数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$，一般称之为二分类规范增广数据集。在这个数据集下，线性可分意味着：存在一个 $\displaystyle \boldsymbol{w}$使得对于任意的 $\displaystyle \boldsymbol{z}_i$，$\displaystyle \boldsymbol{w}\boldsymbol{z}_i&gt;0$成立。 算法：误差修正学习算法2 $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+2\eta_t\mathbb{I}(\boldsymbol{w}_t^\text{T} \boldsymbol{z}\leqslant 0)\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：梯度下降算法分析之前，令 $\displaystyle \boldsymbol{z}=y\times\boldsymbol{x}$，这样我们把属于 $\displaystyle c_2$的特征变成了 $\displaystyle -\boldsymbol{x}$。定义目标函数： $\displaystyle J(\boldsymbol{w})=k \left(|\boldsymbol{w}^\text{T}\boldsymbol{z}|-\boldsymbol{w}^\text{T}\boldsymbol{z}\right),k&gt;0$ 通常令 $\displaystyle k=1$易知： $\displaystyle \min J(\boldsymbol{w})=0$时， $\displaystyle \boldsymbol{w}^\text{T}\boldsymbol{z}\geqslant 0$，于是分类正确。问题转化为：$$\begin{align}\min_{\boldsymbol{w}}J(\boldsymbol{w})\end{align}$$我们使用梯度下降算法：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_t\nabla_t\end{align}$$ 我们知道： $\displaystyle \nabla_t=\frac{\partial J}{\partial\boldsymbol{w}_t}=k[\boldsymbol{z}_t\times\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-\boldsymbol{z}_t]$ 代入3式得：$$\begin{align} \boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t \end{align}$$当 $\displaystyle k=1$时，就是误差修正学习算法。 算法：梯度下降算法（Gradient Descent） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$$2\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\boldsymbol{z}_t=y_t\times\boldsymbol{x}_t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\alpha_tk\left[\mathrm{sgn}\left(\boldsymbol{w}_t^\text{T}\boldsymbol{z}_t\right)-1\right]\boldsymbol{z}_t\end{array}\\$3 #end while 感知机学习算法：最小均方误差算法取任意正数 $\displaystyle \nu_i$，有 $\displaystyle \boldsymbol{\nu}=[\nu_1,…,\nu_k]^\text{T}$。定义误差： $\displaystyle \boldsymbol{\epsilon}=\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}$ $$\begin{align} \mathrm{MES}(\boldsymbol{w}) =\boldsymbol{\epsilon}^\text{T}\boldsymbol{\epsilon} =\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right] \end{align}$$容易知道问题化为$$\begin{align}\min_{\boldsymbol{w}} \mathrm{MES}(\boldsymbol{w})\end{align}$$知道这就最小二乘解： $\displaystyle \hat{\boldsymbol{w}}=\left[\boldsymbol{X}^\text{T}\boldsymbol{X}\right]^{-1}\boldsymbol{X}^\text{T}\boldsymbol{\nu}=\boldsymbol{X}^\dagger \boldsymbol{\nu}$。求解伪逆的计算量大。我们使用梯度下降方法，知道$$\begin{align}\nabla\mathrm{MES}(\boldsymbol{w})=2\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}-\boldsymbol{\nu}\right]\end{align}$$ 算法：最小均方误差算法（Least Mean-Square Error） $1\displaystyle \boldsymbol{w}_0=\boldsymbol{X}^\text{T}\boldsymbol{\nu},\boldsymbol{\nu}&gt;0$$2\displaystyle \alpha_0=\alpha$$3\displaystyle \text{while }\boldsymbol{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\alpha_t=\alpha\div t\\\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-2\alpha_t\boldsymbol{X}^\text{T}\left[\boldsymbol{X}\boldsymbol{w}_t-\boldsymbol{\nu}\right]\\\end{array}\\$4 # end while$5\displaystyle \text{ if }\,\boldsymbol{\epsilon}_{end}=\boldsymbol{X}\boldsymbol{w}_{end}-\boldsymbol{\nu}\geqslant 0$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性可分’)}\\\end{array}$$\displaystyle \,\,\,\,\,\text{else }$ : $\displaystyle\quad\begin{array}{|lc}\text{print(‘线性不可分’)}\\\end{array}$6 # end if 感知器收敛定律如果样本集合线性可分，那么感知器存在收敛解。证明：1、我们使用规范增广数据集 $\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n} $2、假定解向量 $\displaystyle \hat{\boldsymbol{w}}$，则对任意的 $\displaystyle \boldsymbol{z}$有 $\displaystyle \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}&gt;0$。我们注意到对于 $\displaystyle \delta&gt;0$ ，$\displaystyle \delta\cdot\hat{\boldsymbol{w}}$也是解向量。3、我们令 $\displaystyle \boldsymbol{z}_t$是所有错分样本， 有$$\begin{align}\boldsymbol{w}_t\boldsymbol{z}_t\leqslant 0\end{align}$$误差修正学习算法中令 $\displaystyle \eta_t=\frac{1}{2}$于是有：$$\begin{align}\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\end{align}$$另外我们还令规范增广特征向量最大长度，与解向量最小内积$$\begin{align}\beta=\max_t ||\,\boldsymbol{z}_t||\end{align}$$$$\begin{align}\alpha=\min_t \hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t&gt;0\end{align}$$4、现在我们把解向量 $\displaystyle \delta\hat{\boldsymbol{w}}$考虑进来于是：$$\begin{align}&amp;\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{z}_t\\&amp;\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}=\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t\\&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}+\boldsymbol{z}_t||^2\end{align}$$5、范数公式 $\displaystyle ||\boldsymbol{x}+\boldsymbol{y}||^2=||\boldsymbol{x}||^2+ 2&lt;\boldsymbol{x},\boldsymbol{y}&gt;+||\boldsymbol{y||^2},\,&lt;\boldsymbol{x},\boldsymbol{y}&gt;=\boldsymbol{x}^\text{T}\boldsymbol{y}$于是我们有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2=||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2+2 \boldsymbol{w}_t^\text{T}\boldsymbol{z}_t-2\delta\cdot\hat{\boldsymbol{w}}^\text{T}\boldsymbol{z}_t+||\boldsymbol{z}_t||^2\end{align}$$6、根据上面的1、2、3的分析，有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-2\delta\cdot\alpha+\beta^2\end{align}$$7、为了简洁令： $\displaystyle \delta=\frac{\beta^2}{\alpha}$于是有：$$\begin{align}&amp;||\boldsymbol{w}_{t+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_t-\delta\cdot\hat{\boldsymbol{w}}||^2-\beta^2\end{align}$$8、考虑上式 $\displaystyle t=\{0,…,m\}$。并累加得：$$\begin{align}&amp;0\leqslant||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\leqslant ||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2-m\cdot\beta^2\end{align}$$9、可以看到随着 $\displaystyle m$的不断增加，范数 $\displaystyle ||\boldsymbol{w}_{m+1}-\delta\cdot\hat{\boldsymbol{w}}||^2\to 0$，于是：$$\begin{align}m_{max}= \frac{||\boldsymbol{w}_0-\delta\cdot\hat{\boldsymbol{w}}||^2}{\beta^2}\end{align}$$10、如果我们令 $\displaystyle \boldsymbol{w}_0=\boldsymbol{0}$,有：$$\begin{align}m_{max}=\frac{\delta^2}{\beta^2}||\,\hat{\boldsymbol{w}}||=\frac{\beta^2}{\alpha^2}||\,\hat{\boldsymbol{w}}||\end{align}$$这时感知机的解收敛于： $\displaystyle \frac{\beta^2}{\alpha}\cdot\hat{\boldsymbol{w}}$。史称感知机固定增量收敛定律。#### 评述1、 历史说明：分类是科学之始，为解决分类问题，人类殚精竭虑。其中模仿神经元的感知器就是其中之一。2、上帝给世界分了两类 $\displaystyle c_1$和 $\displaystyle c_2$。人类观测到了 $\displaystyle \{\boldsymbol{x}_i,y_i\}_{i=1}^{n}$。为了解决问题，人类思考了最简单的方法：劈下一刀，不就两半了。于是有了对称硬限函数。3、有个负号还是很麻烦，于是定义了 $\displaystyle \boldsymbol{z}=y\times \boldsymbol{x}$ 就有了新的数据集$\displaystyle \{\boldsymbol{z}_i,y_i\}_{i=1}^{n}$ 劈下一刀，变成了的折一下，然后裁一刀的问题：$$\begin{align} \forall \boldsymbol{z}_i,\boldsymbol{w}^\text{T}\boldsymbol{z}_i&gt;0\end{align}$$4、满足上式的问题，我们叫线性可分。并且根据学习规则和对问题认识，人类很块发现了感知机固定增量收敛定律。5、但是好景不长，人类的智者很快发现 线性不过是人类的YY，非线性才是上帝的YY。人类继续着征程： 我们的征途是星辰大海！ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-04-05-感知器-机器学习/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>感知机</tag>
        <tag>人工神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-11-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-11-隐马尔可夫模型/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清隐马尔可夫模型的问题。若有错误，请大家指正。关键词: 隐马尔可夫模型,前向-后向算法,维特比算法,鲍姆-韦尔奇算法 一、马尔可夫模型1.1、基本概念1.1.1、符号我们开始讨论，离散时间离散状态，也就是离散随机序列。假定：离散时间： $\displaystyle \{1,\cdots,T\}$；离散状态：$\displaystyle x_t\in \{1,\cdots,c,\cdots,C\}$。我们称 $$\begin{align}p(\bm{x}_{1:T})=p(x_1)\prod_{t=2}^Tp\big(x_t\mid x_{t-1}\big)\end{align}$$为一阶马尔可夫模型。下面来初步认识一下这个模型。 1.1.2、转移概率转移概率: 从状态 $\displaystyle i$到状态 $\displaystyle j$的概率 $\displaystyle A_{ij}=p\big(x_t=j\mid x_{t-1}=i\big)$，于是有转移矩阵$\displaystyle \bm{A}=\big[A_{ij}\big]$，且有 $\displaystyle \sum_{j=1}^KA_{ij}=\bm{I}^\text{T}\bm{a}_i=1$ 也就说 $\displaystyle \bm{A}$的每一行相加等于1。这样的矩阵我们称之为随机矩阵 $\displaystyle \textit{(stochastic matrix)}$。注意这个时候我们定义的转移矩阵与离散的时间没有关系。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-11-隐马尔可夫模型/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率图</tag>
        <tag>隐马尔可夫模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图基础]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-10-%E6%A6%82%E7%8E%87%E5%9B%BE%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-10-概率图基础/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Knowledge is the antidote to fear.摘要：本文意在理解与分析图模型。若有错误，请大家指正。关键词: 图模型,图论,有向图,无向图 一、问题来源有向图模型($\displaystyle \textit{Directed Graphical Models}$)又称贝叶斯网络($\displaystyle \textit{Bayes Nets}$)。 对于用简单方法来训练复杂系统，我基本知道两个原则：模块化和抽象化。在机器学习领域，我是计算概率的辩护者。因为我相信概率论用它深入和迷人的方式执行了两个原则——因子分解与平均。在我看来，充分利用这两种机制是机器学习的前进方向。—— $\displaystyle \textit{Michael Jordan 1997}$ $\displaystyle \textit{(quted in(Frey 1998)}$ 假设我们观察多个相关变量，例如文档中的单词、图像中的像素或微阵列中的基因。 1、我们如何能简洁地表示联合分布 $\displaystyle p\big(\bm{x}\mid \bm{\theta}\big)$？2、在合理的计算时间内，给定其他，我们如何利用这个分布来推断一组变量?3、我们如何通过合理的数据量来学习这个分布的参数? 这些问题是概率建模、推断和学习的核心，也是概率图的主题。我们参照 $\displaystyle \textit{Matlab}$的向量形式定义如下符号：$\displaystyle \bm{x}_{1:t-1}=x_t,\cdots,x_{t-1}$。定义如下术语：条件概率表格 CPTs $\displaystyle \textit{(conditional probability tables)}$、条件概率密度 CPD$\displaystyle \textit{(conditional probability distribution)}$、条件独立 CI$\displaystyle \textit{(Conditional independence)}$ 我们来考察一下马尔可夫模型：给定现在，未来独立于过去。 这叫做一阶马尔可夫假设(二元模型假设)$$\begin{align}p(\bm{x})=p(x_1)\prod_{t=2}^Tp\big(x_t\mid x_{t-1}\big)\end{align}$$尽管一阶马尔可夫假设对于定义一维序列的分布很有用，但是如何定义二维图像的分布，或者三维视频，或者更一般的：任意变量集合(例如属于某些生物通路的基因)？这正是我们引入图模型的切入点。 一个图模型是应用条件独立假设来表达联合概率分布的方法。是一套能简洁紧凑地表达变量关系的工具。下面我们将不厌其烦的对图论术语下定义(这些术语都是顾名思义的)和显然的。 二、基本概念2.1、图模型的基本符号图 $graph$： $\displaystyle \mathcal{G}=\{V,E\}$节点集 $nodes$： $\displaystyle V(\mathcal{G})$边集 $edges$： $\displaystyle E(\mathcal{G})=\{st\mid s,t\in V\}$阶 $order$： $\displaystyle n=n(\mathcal{G})=\mid\mathcal{G}\mid$边数 $edges numbers$： $\displaystyle e=e(\mathcal{G})=\parallel\mathcal{G}\parallel$ 2.2、节点与边的符号【节点】：引入一个符号 $\displaystyle v_s$，表示第 $\displaystyle s$个节点，这意味着我们也可以用 $\displaystyle s$表示第 $\displaystyle s$个节点。我们将会根据上下文，灵活使用这两种表示符号。 【边】：引入另外一个符号来表示边 有向边：$\displaystyle s\to t=v_s\to v_t\Rightarrow\mathcal{G}(s,t)=1\Rightarrow \\&lt;st>\in E(\mathcal{G})$无向边：$\displaystyle s-t=v_s-v_t\Rightarrow \mathcal{G}(s,t)=1\land\mathcal{G}(s,t)=1\Rightarrow(st)\in E(\mathcal{G})$某种边：$\displaystyle s\rightleftarrows t=v_s\rightleftarrows v_t\Rightarrow \mathcal{G}(s,t)=1\lor\mathcal{G}(s,t)=1\Rightarrow st\in E(\mathcal{G})$没有边：$\displaystyle \require{cancel} s\cancel{\rightleftarrows}t\Rightarrow\mathcal{G}(s,t)=0\Rightarrow st \notin E(\mathcal{G})$ 【分类】如果一个图的所有边都是有向边，则称其为有向图 $\displaystyle \textit{(Directed Graph)}$；如果都是无向边，则称其为无向图 $\displaystyle \textit{(Undirected Graph)}$；如果皆有之，则称其为混合图$\displaystyle \textit{(Mixed Graph)}$ 2.3、关于节点认识2.3.1、有向图下的节点基本概念一个节点的 $Parent$父节点集: $\displaystyle pa(s)=\{t\mid\mathcal{G}(t,s)=1,t\in V(\mathcal{G})\}$一个节点的 $Child$子节点集: $\displaystyle ch(s)=\{t\mid\mathcal{G}(s,t)=1,t\in V(\mathcal{G}) \}$一个节点的 $Family$族节点集: $\displaystyle fam(s)=\{s\}\cup Pa(s)$ 一个节点是 $root$根: $\displaystyle \exists pa(s)=\emptyset\to \textit{s is a root}$一个节点是 $leaf$叶: $\displaystyle \exists ch(s)=\emptyset\to \textit{s is a leaf}$ 一个节点的 $Ancestors$祖节点集： $\displaystyle an(s)=\{t\mid t\leadsto s\}$一个节点的 $Descendants$孙节点集： $\displaystyle de(s)=\{t\mid s\leadsto t\}$ 拓扑序 $\displaystyle \textit{(Topological Ordering)}$一个节点下标序列满足 $\displaystyle x_s\to x_t\Rightarrow s&lt;t$，也就是说父节点下标比子节点的小。这种下标排序成为图 $\displaystyle \mathcal{G}$的一个拓扑序。 2.3.2、任意图下的节点基本概念邻接点 $Neighbour$: $\displaystyle nb(s)=\{t\mid\mathcal{G}(s,t)=1\lor \mathcal{G}(t,s)=1,t\in V(\mathcal{G}) \}$ 边界点 $Boundary$: $\displaystyle bd(s)= pa(s)\cap nb(s)$ 节点度 $degree$： $\displaystyle d(v)=\mid E(v)\mid$ 含义是关联到节点 $\displaystyle v$的边的条数。特别的，对于有向图。有入度in-degree（父节点数量），出度out-degree（子节点数量）。 2.4、关于边的认识路 $\displaystyle \textit{(Path)}$有节点集合 $\displaystyle P=\{v_1,\cdots,v_k\},k\geqslant 3$， 若有 $\displaystyle \forall v_i\in P\subseteq V(\mathcal{G})\Rightarrow v_i\to v_{i+1}\lor v_i-v_{i+1}$存在，那么就说该节点集合 $\displaystyle P$在图 $\displaystyle \mathcal{G}$中形成了一条路径，记为 $\displaystyle \mathcal{G}_P$。 迹 $\displaystyle \textit{(Trail)}$有节点集合 $\displaystyle T=\{v_1,\cdots,v_k\},k\geqslant 3$， 若有 $\displaystyle \forall v_i\in T\subseteq V(\mathcal{G})\Rightarrow v_i\rightleftarrows v_{i+1}$存在，那么就说该节点集合 $\displaystyle T$在图 $\displaystyle \mathcal{G}$中形成了一条迹，记为 $\displaystyle \mathcal{G}_T$。 环 $\displaystyle \textit{(Cycle)}$节点集合 $\displaystyle C=\{v_1,\cdots,v_k\}$是图 $\displaystyle \mathcal{G}$的一条路，若还有 $\displaystyle v_k\to v_1\lor v_k-v_{1}$存在，这称节集合 $\displaystyle C$在图 $\displaystyle \mathcal{G}$中的形成了一个环，记为 $\displaystyle \mathcal{G}_C$。如果一个图不包含环，称该图为无环图 $\displaystyle \textit{(Acyclic Graph)}$。 圈 $\displaystyle \textit{(Loop)}$节点集合 $\displaystyle L=\{v_1,\cdots,v_k\}$是图 $\displaystyle \mathcal{G}$的一条迹，若有 $\displaystyle v_k\rightleftarrows v_1$存在，这称节点集合 $\displaystyle L$在图 $\displaystyle \mathcal{G}$中形成了一个圈，记为 $\displaystyle \mathcal{G}_L$。 连通非空图 $\displaystyle \mathcal{G}$的任意两个节点都有一条路存在，就成 $\displaystyle \mathcal{G}$是连通的。 无圈图是森林，特别的有向无圈图是多重树，如果每个节点至多只有一个父节点，则该有向图为森林，连通的森林是树。 弦图 $\displaystyle \mathcal{G}$中的一个圈 $\displaystyle \mathcal{G}_L$，在 $\displaystyle L$中不连贯的节点的边成为弦。 2.5、导出子图与团导出子图 $\displaystyle \textit{(Induced Subgraph)}$在概率图中，我们关心的是导出子图：若 $\displaystyle A\subseteq V(\mathcal{G})$, 由 $\displaystyle A$生产一个图 $\displaystyle \mathcal{G}_A$，且 $\displaystyle E(\mathcal{G}_A)=\{st\mid \forall st\in E(\mathcal{G})\land s,t\in A\} $，我们称 $\displaystyle \mathcal{G}_A$是 $\displaystyle A$在图 $\displaystyle \mathcal{G}$中的导出子图。也就是说 $A= V(\mathcal{G}_{A})\subseteq V(\mathcal{G})$， $E(\mathcal{G}_A)\subseteq E(\mathcal{G})$ 团 $\displaystyle \textit{(clique)}$若导出子图 $\displaystyle \mathcal{G}_A$中，这个命题成立：$\displaystyle \forall s,t \in A\to \mathcal{G}_A(s,t)=1$；也就是说 $\displaystyle A$中的任意节点都有一条边，就称 $\displaystyle A$为团。特别的 $\displaystyle \forall B\supset A,B\subseteq V(\mathcal{G})$， $\displaystyle B$不是团，我们就称 $\displaystyle A$为极大团 $\displaystyle \textit{(Maximal Clique)}$。 2.6、有向图与联合分布有向图中，联合概率分布根据链式法则可以表示为一系列条件概率的乘积。我们将条件概率分布的条件的随机变量，作为边的起点。这样有: 有向图与联合分布 三、有向无环图模型$\displaystyle \textit{ (DGM or DAG) }$任意图模型的核心是一组条件独立假设。我们发现有不同方法来描述有向无环图模型 $\displaystyle \textit{(Directed Aclyclic Graphy)}$的这些条件独立假设，最后我们可以证明它们说的是一个意思。 3.1、有向马尔可夫性质【有序马尔可夫性质 $\displaystyle O$】: 给定一个有向无环图的拓扑序 $\displaystyle \{1,\cdots,k,\cdots,K\}$，则有 $\displaystyle p(\bm{x})=\prod_{k=1}^Kp\big(x_k\mid Pa(x_k)\big)$。也就是说：给定父节点，当前节点与前置节点独立$$\begin{align}O=\left\{S\mid S:\forall x_k\in V(\mathcal{G})\to x_k\perp_\mathcal{G} \bm{x}_{pred(k)-pa(k)}\mid \bm{x}_{pa(k)}\right\}\end{align}$$ 【有向局部马尔可夫性质 $\displaystyle L$】: 给定父节点，当前节点与非后代节点独立$$\begin{align}L=\{S\mid S:\forall x_t\in V(\mathcal{G})\to x_t\perp_\mathcal{G} \bm{x}_{nd(t)-pa(t)}\mid \bm{x}_{pa(t)}\}\end{align}$$其中 $\displaystyle nd(t)=V(\mathcal{G})-\{t\}\cup de(t)$，且有$\displaystyle pred(t)\subseteq nd(t)$。 【全局马尔可夫性质 $\displaystyle G$】$$\begin{align}G=\{S \mid S:\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C\land A,B,C\subseteq V(\mathcal{G})\}\end{align}$$这些性质统称有向马尔可夫性质，它们之间是等价的。没错是等价的！！！ 3.2、有向马尔可夫性质定理$$\begin{align}O\Longleftrightarrow L \Longleftrightarrow G\end{align}$$ 3.3、有向分离与贝叶斯球算法3.3.1、迹的有向分离 如何找到全局马尔可夫性质，下面我们引入有向分离 $\displaystyle \textit{(D-Separation)}$的概念,首先，我们说一条迹 $\displaystyle T$被节点集 $\displaystyle E\textit{ (containing the evidence)}$有向分离，那么该迹至少满足下面一条：1、 $\displaystyle T$ 含有链迹 $\displaystyle x\to y\to z$ 或者 $\displaystyle x\leftarrow y\leftarrow z$，且 $\displaystyle y\in E$ 2、 $\displaystyle T$ 含有叉迹 $\displaystyle x \swarrow ^y \searrow z$，且 $\displaystyle y\in E$ 3、$\displaystyle T$ 含有撞迹 $\displaystyle x \searrow _y \swarrow z$,且 $\displaystyle y,\notin E \land de(y)\cap E=\emptyset$ 3.3.2、节点集的有向分离接下来，我们说给定已观测节点集 $\displaystyle E$，一节点集合 $\displaystyle A$与另一节点集 $\displaystyle B$有向分离，意思是说，对于每一条 $\displaystyle a\in A$到 $\displaystyle b\in B$的迹被节点集 $\displaystyle E\textit{ (containing the evidence)}$有向分离。 3.3.3、有向无环图的条件独立性质最后，我们就能把图的分离性定义与分布的条件独立性质结合起来：$$\begin{align}\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_E \iff \textit{ A is d-separation from B given E}\end{align}$$ 我们来详细考察一下：【链迹】考察联合分布 $\displaystyle p(x,y,z)=p(x)p(y\mid x)p(z\mid y)$，然后我们考察基于 $\displaystyle y$的条件分布：$$\begin{align}p(x,z\mid y)=\frac{p(x)p(y\mid x)p(z\mid y)}{p(y)}=\frac{p(x,y)p(z\mid y)}{p(y)}=p(x\mid y)p(z\mid y)\iff x \perp z\mid y\end{align}$$ 【叉迹】考察联合分布 $\displaystyle p(x,y,z)=p(y)p(x\mid y)p(z\mid y)$，同样我们考察 $\displaystyle y$的分布：$$\begin{align}p(x,z\mid y)=\frac{p(y)p(x\mid y)p(z\mid y)}{p(y)}=p(x\mid y)p(z\mid y)\iff x \perp z\mid y\end{align}$$ 【撞迹】考察联合分布 $\displaystyle p(x,y,z)=p(x)p(z)p(y\mid x,z)$，同样我们考察 $\displaystyle y$的分布：$$\begin{align}p(x,z\mid y)=\frac{p(x)p(z)p(y\mid x,z)}{p(y)}\iff x \require{cancel} \cancel{\perp} z\mid y\end{align}$$但是，我们发现$$\begin{align}p(x,z)=\int p(x)p(z)p(y\mid x,z) dy=p(x)p(z)\iff x\perp z\end{align}$$我们把撞迹的这种现象叫解释消除 $\displaystyle \textit{(explaining away)}$ 3.3.4、贝叶斯球算法贝叶斯球算法 $\displaystyle \textit{(shachter 1998)}$是一种判断节点集是否有向分离的简单方法： 贝叶斯球算法 3.4、马尔可夫毯与全条件下面我们将考察单个节点与其他节点的独立性质。我们定义图中所有与 $\displaystyle t$节点独立的其他节点集称为 $ t$的马尔可夫毯 Markov Blanket ，记为 $\displaystyle mb(t)$。 特别的在有向无环图中一个节点的马尔可夫毯是父节点、子节点、同父节点 (co-parents) (或者说它子节点的父节点)的集合。即：$$\begin{align}mb(t)=ch(t)\cup pa(t) \cup copa(t)\end{align}$$ 为何同父节点在马尔科夫毯中？注意到： $$\begin{align}p\big(x_t\mid \bm{x}_{-t}\big)= \frac{ p\big(x_t ,\bm{x}_{-t}\big)}{ p\big( \bm{x}_{-t}\big)}\end{align}$$ 其中所有不包含 $\displaystyle x_t$的项将在分子和分母之间被消掉，所以在这个视角下只剩下含有 $\displaystyle x_t$的概率密度的乘积。这个一点使用贝叶斯球算法的链迹、叉迹、撞迹、阻塞、反弹的概念判定独立不难理解。因此有： $$\begin{align} p\big(x_t\mid \bm{x}_{-t}\big) \propto p \big(x_t\mid \bm{x}_{pa(t)}\big)\prod_{s\in ch(t)}p \big(x_s\mid \bm{x}_{pa(s)}\big) \end{align}$$ 得到的表达式被称为 $\displaystyle t$节点的全条件 (Full Conditional) ，当我们学习吉布斯抽样时这一结论是非常重要的。 四、图模型与联合概率分布图模型是一种语言，联合概率分布也是一种语言。它们可以表达相同的事物。正如几何与代数的关系。下面我们将引入一系列定义，以方便寻找它们的联系。 4.1、图模型的条件独立性质与 $\displaystyle \text{I-Map}$我们即将引入 $\displaystyle \text{I-Map}$的概念与定义，以方便简洁表达。这种手法是发展数学的动力之一。我们知道：任意图模型 $\displaystyle \mathcal{G}$的核心是一组条件独立假设集合，可以记为 $$\begin{align}\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C\end{align}$$ 我们根据这一叙述可以定义一个“陈述” $\displaystyle \textit{(Senantics)}$。 $\displaystyle S:\bm{x}_A\perp_\mathcal{G}\bm{x}_B\mid \bm{x}_C$, $\displaystyle A,B,C\subseteq V(\mathcal{G})$。然后我们把“所有条件独立陈述的集合”定义为：$\displaystyle \mathcal{I}(\mathcal{G})=\{S\}$。也就是说：$\displaystyle\mathcal{I}(\mathcal{G})$是图模型 $\displaystyle \mathcal{G}$的所有条件独立陈述的集合。 同样样给出一个联合概率分布 $\displaystyle p$的所有条件独立陈述的集合，记为： $\displaystyle \mathcal{I}(p)$。 当然这些陈述是可以判断真假的，是真命题。至于为什么没有直接使用“命题的集合”这个的术语，考虑到我们是在给现实世界建模，而模型是否为真，还不一定，使用“陈述”这一术语是恰当的。 特别的：“一个联合分布 $\displaystyle p$满足一个图模型 $\displaystyle \mathcal{G}$的所有条件独立假设”。这句话可以写为：$$\begin{align}\mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)\end{align}$$这种情况下，我们就说“图 $\displaystyle \mathcal{G}$是分布 $\displaystyle p$的一个 $\displaystyle\text{I-Map}$”。很快我们就会发现关于 $\displaystyle\mathcal{I}(\mathcal{G})$定义中的“所有”并不是必须的。也就说一个 $\displaystyle p$的 $\displaystyle\text{I-Map}$可以有很多个，例如完全图是任意分布的 $\displaystyle p$的 $\displaystyle\text{I-Map}$。 4.2、因子分解同样，我们引入因子分解 $\displaystyle \textit{(factorization)}$的概念与定义。注意因子分解应该理解为一个动词。 若有向无环图模型 $\displaystyle \mathcal{G} $，有随机变量集(节点集) $\displaystyle \mathcal{X}=V(\mathcal{G})=\{x_i\}_{i=1}^n$，如果联合分布 $\displaystyle p$能表示为如下乘积形式：$$\begin{align}\displaystyle p(\bm{x})=\prod_{i=1}^np\big(x_i\mid \bm{x}_{pa(i)}\big)\end{align}$$那么我们称联合分布 $\displaystyle p$基于图 $\displaystyle \mathcal{G}$在同一空间 $\displaystyle \mathcal{X}$ 上因子分解 ($\displaystyle \textit{ p over the same space } $$\displaystyle \mathcal{X}$ $\displaystyle \textit{ factorizes according to }\mathcal{G}$)，记为 $\displaystyle F$。 4.3、条件独立因子分解定理 $$\begin{align}\mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)\Longleftrightarrow p(\bm{x})=\prod_{i=1}^np\big(x_i\mid \bm{x}_{pa(i)}\big)\end{align}$$ 也就是说一个联合分布 $\displaystyle p$满足一个图模型 $\displaystyle \mathcal{G}$的所有条件独立假设，与联合分布 $\displaystyle p$基于图 $\displaystyle \mathcal{G}$在同一空间 $\displaystyle \mathcal{X}$ 上因子分解等价。 于是我们有：$$\begin{align}O\Longleftrightarrow L \Longleftrightarrow G \Longleftrightarrow F\end{align}$$ 五、无向图模型(Undirected Graphical Model(UGM))5.1、UGMs的特点无向图模型 Undirected Graphical Model(UGM) ，也称为马尔可夫随机场 Markov random field(MRF) 或马尔可夫网络。它不要求我们指定边方向，而且对于一些问题，如图像分析和空间统计，更自然。 UGMs 相对 DGMs 的主要优势是:(1)它们是对称的，因此对于某些领域(如空间或关系数据)更“自然”;(2)定义 $\displaystyle p \big(\bm{y}\mid \bm{x}\big)$条件密度的判别 UGMs (即条件随机场 Conditional random ﬁelds, or CRFs )，比判别 DGMs 更有效。 与 DGMs 相比， UGMs 的主要缺点是:(1)参数的可解释性更小，模块化更少。(2)参数估计在计算成本上更加昂贵。 5.2、UGMs的条件独立性质UGMs 通过简单的图分离来定义 CI 关系。对于节点 $ A$、 $ B$和 $ C$的集合，如果在图 $ \mathcal{G}$中 $ C$把 $ A$从 $ B$分离，我们就说 $\displaystyle \bm{x}_A \perp_{\mathcal{G}} \bm{x}_b\mid \bm{x}_c$。这意味着，当我们删除 $ C$中所有节点，如果没有任何路(Path)连接 $ A$中任何节点到 $ B$中的任何节点，就称 CI 性质。这被称为 UGMs 的全局马尔可夫性质 (global Markov property) 。 在一个图中，使与节点 $ t$有条件独立性质最小节点集称为 $ t$的马尔可夫毯 Markov blanket ；我们将用 $ mb(t)$表示。在形式上，马尔可夫毯满足以下性质: $$\begin{align}t\perp V(\mathcal{G})-cl(t)\mid mb(t)\end{align}$$ 其中 $\displaystyle cl(t)=mb(t)\cup \{t\}$是节点 $ t$的闭包 (closure) 。易知，在一个 UGM 中，节点的马尔可夫毯是它紧邻的集合。这叫做无向局部马尔可夫性质。 从局部马尔可夫性质，我们可以很容易看出如果两个节点之间没有直接连接的边，给定其余节点，它们是条件独立的。这叫做成对马尔可夫性质(Pairwise Markov Property) 记为 $ P$ 。 $$\begin{align}s\perp t \mid V(\mathcal{G})-\{s,t\}\iff \mathcal{G}(s,t)=0\end{align}$$ 很明显，全局马尔可夫性质包含了局部马尔可夫性质，它意味着成对马尔可夫性质。虽然不那么明显，但仍然是正确的(假设对所有 $ x$的 $ p(x)&gt;0$，即 $ p$是一个正密度)，这时成对马尔可夫性蕴含全局马尔可夫性质，因此所有这些马尔可夫性质都是等价的，如图19.3所示(参见Koller和Friedman 2009,p119)。这一结果的重要性在于，它通常更容易根据经验来评估成对条件独立性；可以使用这种成对条件独立性语句构造一个图形，而从这个图中提取的全局独立性语句也成立。 $$\begin{align} G\Longrightarrow L \Longrightarrow P \mathop{===\Longrightarrow}^{\forall x\to p(x)&gt;0} G\end{align}$$ 或者说$$\begin{align}\forall x\to p(x)&gt;0 \Rightarrow G \iff L \iff P\end{align}$$ 5.3、Hammersley-Clifford定理由于没有与无向图相关的拓扑排序，我们不能使用链式法则来表示 $ p \big(\bm{x}\big)$。因此，我们将概率密度与每个节点关联起来替换方法是，将势函数或因子与图中每个极大团联系起来。我们将团 $ c$势函数记为 $ \psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)$。势函数可以是其参数的任何非负函数。然后定义联合分布与团势函数乘积成比例。令人惊讶的是，可以通过这样的方式来表示任何可以由无向图模型表达条件独立性质的正分布。我们更正式地陈述这个结果如下 【 Hammersley-Clifford 定理】一个正分布 $ p \big(\bm{x}\big)&gt;0$ 能满足一个无向图模型 $ \mathcal{G}$的条件独立性质，当且仅当 $ p$能够被每个极大团因子的乘积表示。 $$\begin{align}\forall x\to p(x)&gt;0\Rightarrow \mathcal{I}(p)=\mathcal{I}(\mathcal{G})\iff p \big(\bm{x}\mid \bm{\theta}\big)=\frac{1}{Z(\bm{\theta})}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 其中 $ \mathcal{C}$是图 $ \mathcal{G}$ 所有极大团的集合。 $ Z(\bm{\theta})$是配分函数：$$\begin{align}Z(\bm{\theta})=\sum_{\bm{x}}\prod_{c\in \mathcal{C}}\psi_c \big(\bm{x}_c\mid \bm{\theta}_c\big)\end{align}$$ 请注意，配分函数确保了整体分布总和为 $ 1$。证明从未发表过，但可以在例如(Koller and Friedman 2009)中找到。 六、比较有向图模型和无向图模型哪一个模型有更强的“表达能力”，是有向图模型还是无向图模型？为了形式化这个问题，回顾一下：如果 $ \mathcal{I}(\mathcal{G})\subseteq \mathcal{I}(p)$，则 $ \mathcal{G}$是一个分布 $ p$的 $\text{I-Map}$。如果 $ \mathcal{I}(\mathcal{G})= \mathcal{I}(p)$，那么就称 $ \mathcal{G}$为 $ p$的完美映射 (Perfect Map) $ \text{P-Map}$，换句话说，图可以表示分布的所有(且唯一)条件独立性质。问题就变为：对于不同分布集合，是有向图模型还是无向图模型是完美映射。从这个意义上说，两者都不是比另一个更强大的表示语言。 一些条件独立关系的例子可以完全由有向图模型建模，而无向图模型不能。考虑一个 $ v$结构$ A \searrow _C \swarrow B$，这表示 $ A\perp B$ 且 $\require{cancel} A\cancel{\perp} B\mid C$。如果我们去掉箭头，得到 $ A-C-B$，这表示 $\require{cancel} A\cancel{\perp} B$ 且 $ A\perp B\mid C$。这种去掉箭头的操作显然是不对的。事实上，没有无向图模型能精准完全唯一地表达由 $ v$结构编码的两个条件独立性质。一般的，无向图模型的条件独立性质是单调的，这里的单调有如下含义：如果 $ A \perp B \mid C$，则 $ A\perp B \mid \big(C\cup D\big)$。但在有向图模型中，条件独立性质是非单调的，因为存在附加变量的条件作用可以消除由于解释消除 $\displaystyle \textit{(explaining away)} $生产的条件独立性质。 图模型比较 一些条件独立关系的例子可以完全由无向图模型建模的，而有向图模型不能。可以考虑如图(a)所示的 $ 4$环。图(b)显示了用有向图模型建模的尝试，它能正确表达 $ A\perp C \mid B,D$，但表达的 $ B\perp D\mid A$是错误的。图(c)是另一个不正确的有向图模型，它正确地编码了 $ A\perp C \mid B,D$，但是编码的 $ B\perp D$是错误的。事实上，没有有向图模型能够精准完全唯一地表达由这个由无向图模型编码的条件独立陈述集合。 有些分布可以完全由有向图模或无向图模型建模；得到的图称为可分解 (decomposable) 或弦 (chordal) 。粗略地说，这意味着：如果我们把每个最大团的所有变量一起坍缩生成“大变量” (“mega-variables”,) ，结果图将是一棵树。当然，如果图已经是树了(包括链作为特殊情况)，它将是弦。有关详细信息，我们以后详细介绍。 七、总结1、概率是一个好工具，这个工具的核心在于概率的条件独立性质。或者说高级一点条件期望。没有条件期望，概率论不过是测度论的一个表现形式。正是我们定义了条件概率，概率论才枝深叶茂起来。但是表达条件概率不仅仅用数学公式，还可以用图。于是一门新的学科诞生了：概率图。2、无论是无向图、有向图，其核心都是一组条件概率假设(或者条件概率陈述)。而在非常宽泛条件下，一组条件概率假设和一个图表意是一致。这就奠定了概率图应用的基础。3、有了图模型这一强有力的可视化建模工具，我们足矣为各种复杂场景建模，星辰大海就在前方，我们将继续探索。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-10-概率图基础/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广义线性模型]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-09-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-09-广义线性模型/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清广义线性模型的问题。若有错误，请大家指正。关键词: 广义线性模型,广义线性混合模型,指数族,probit 回归 一、 简述我们现在遇到了各种各样的概率分布：高斯分布、伯努利分布、学生 $\displaystyle \text{t}$分布、均匀分布、伽玛分布等等，这些都是广义分布的广义函数，称为指数族。在本章中，我们将讨论这个家族的各种性质。这使我们能够得到具有非常广泛适用性的定理和算法。 我们将会看到，如何方便地将指数家族中的任何成员作为一个类条件密度，以便做一个生成分类器。此外，我们还将讨论如何建立判别模型，其中的响应变量有指数族分布，其均值是输入的线性函数；这被称为广义线性模型，并将 $\displaystyle \textit{logistic } $回归中的概念推广到其他类型的响应变量中。 二、 指数族在定义指数族之前，我们先提几个重要原因: •可以证明，在一定的规律性条件下，指数族是唯一具有有限大小统计量的分布族，这意味着我们可以将数据压缩成一个固定大小的摘要，而不会丢失信息。这对于在线学习特别有用，我们稍后会看到。 •指数族是有共轭先验分布存在的唯一分布族，这简化了后验计算。 •在满足一些约束的假设下，指数族是熵最大的一个分布家族。 •指数族是广义线性模型的核心。 •指数族是变分推断的核心。 2.1、定义参数为 $\displaystyle \bm{w} $，变量为 $\displaystyle \bm{x} $ 的指数分布族定义为下面形式的概率分布集合：$$\begin{align}p(\bm{x}\mid \bm{w})&amp;=\frac{1}{Z(\bm{w})}h(\bm{x})\exp\left[\bm{w}^\text{T}\bm{T}(\bm{x})\right]=h(\bm{x})\exp\big[\bm{w}^\text{T}\bm{T}(\bm{x})-A(\bm{w})\big]\\end{align}$$特别的为了适合上述形式，参数通常是需要变形 $\displaystyle \bm{\eta}=\bm{\eta}(\bm{w})$，于是有：$$\begin{align}p(\bm{x}\mid \bm{w})&amp;=\frac{1}{Z(\bm{w})}h(\bm{x})\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})\right]\\\\&amp;=h(\bm{x})\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})-A(\bm{\eta}(\bm{w}))\right]\\\\&amp;=\exp\left[\bm{\eta}(\bm{w})^\text{T}\bm{T}(\bm{x})-A\left(\bm{\eta}(\bm{w})\right)+B(\bm{x})\right]\\\\&amp;=h(\bm{x})g(\bm{\eta})\exp\left[\bm{\eta}(\bm{w})\cdot\bm{T}(\bm{x})\right]\end{align}$$其中：$\displaystyle Z(\bm{w})=\int_{\mathcal{X}^n}h(\bm{x})\exp\left[\bm{\eta^\text{T}(\bm{w})}\bm{T}(\bm{x})\right]\mathrm{d}\bm{x}$ $\displaystyle A(\bm{w})=\ln Z(\bm{w}) $1、$\displaystyle \bm{w} $叫做自然参数 $\displaystyle \textit{(natural parameters)} $、$\displaystyle \bm{\eta(\bm{w})} $叫做规范参数 $\displaystyle \textit{(canonical parameters)} $ 。如果 $\displaystyle \dim(\bm{w})&lt;\dim(\bm{\eta}) $，称分布为曲线指数族 $\displaystyle \textit{(curved exponential family)} $。这意味比参数更多的充分统计量。 2、 $\displaystyle \bm{T}(\bm{x})\in\mathbb{R}^k $叫做充分统计向量 $\displaystyle \textit{(vector of sufficient statistics)} $。 3、$\displaystyle Z(\bm{w}) $叫做配分函数 $\displaystyle \textit{(partition function)} $ 4、$\displaystyle A(\bm{w}) $叫做对数配分函数 $\displaystyle \textit{(log partition function)} $或者累积量函数 $\displaystyle \textit{(cumulant function)} $ 5、$\displaystyle h(\bm{x}) $叫做放缩常量 $\displaystyle \textit{(scaling constant)} $，通常是1。 6、特别的 $\displaystyle \bm{T}(\bm{x})=\bm{x} $ 称为自然指数族 $\displaystyle \textit{(natural exponential family)} $或者规范形 $\displaystyle \textit{(canonical form)} $ 2.2、对数配分函数指数族的一个重要性质是对数配分函数的导数可以用来生成充分统计量的累积量。所以 $\displaystyle A \big(\bm{\eta}\big)$有时被称为累积量函数。$$\begin{align}A(\bm{\eta})=\ln \bigg[\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}\bigg]\end{align}$$于是有$$\begin{align}\dot{A}(\bm{\eta})=\frac{\partial A}{\partial \bm{\eta}}&amp;=\frac{\displaystyle\frac{\partial }{\partial \bm{\eta}}\bigg[\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}\bigg]}{\displaystyle\int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]d \bm{x}}=\frac{\displaystyle \int h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})\big]\bm{T}d \bm{x}}{\exp \big[A(\bm{\eta})\big]}\\\\&amp;=\int \bm{T} \cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\bm{T}(\bm{x})-A(\bm{\eta})\big]d \bm{x}\\\\&amp;=\int \bm{T} \cdot p \big(\bm{x}\big)d \bm{x}=\mathrm{E}\big[\bm{T}(\bm{x})\big]\end{align}$$还有：$$\begin{align}\ddot{A}(\bm{\eta})=\frac{\partial^2 A}{\partial \bm{\eta}\partial\bm{\eta}^\text{T}}&amp;=\frac{\partial }{\partial \bm{\eta}^\text{T}}\bigg[\int \bm{T} \cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\cdot\bm{T}-A(\bm{\eta})\big]d \bm{x}\bigg]\\\\&amp;=\int \bm{T}^\text{T}\cdot h(\bm{x})\exp \big[\bm{\eta}^\text{T}\cdot\bm{T}-A(\bm{\eta})\big]\big[\bm{T}-\dot{A}(\bm{\eta})\big]d \bm{x}\\\\&amp;=\int \bm{T}^\text{T}\big[\bm{T}-\dot{A}(\bm{\eta})\big]p (\bm{x})d \bm{x}\\\\&amp;=\mathrm{E}\big[\bm{T}^\text{T}\bm{T}\big]- \mathrm{E}\big[\bm{T}^\text{T}\big]\mathrm{E}\big[\mathrm{E}\big[\bm{T}\big]\big]=\mathrm{E}\big[\bm{T}^\text{T}\bm{T}\big]-\mathrm{E}^\text{T}\big[\bm{T}\big]\mathrm{E}\big[\bm{T}\big]\\\\&amp;=\mathrm{cov}\big[\bm{T}(\bm{x})\big]\end{align}$$也就是说$$\begin{align}\nabla^2A \big(\bm{\eta}\big)=\mathrm{cov}\big[\bm{T}(\bm{x})\big]\end{align}$$ 由于协方差是正定的，我们看到一个 $\displaystyle A \big(\bm{\eta}\big)$是一个凸函数(参见7.3.3)。注意上面的推导交换了积分和求导顺序，这需要满足一致收敛条件。这显然是满足的。 2.3、例子让我们考虑一些例子来让事情更清楚些。 2.3.1、 $\displaystyle \textit{0-1}$分布有 $\displaystyle x\in\{0,1\}$的 $\displaystyle \textit{0-1}$分布写成指数族形式：$$\begin{align}\mathrm{Ber}\big(x\mid \mu\big)=\mu^x(1-\mu)^{1-x}=\exp \big[x\ln \mu+(1-x)\ln (1-\mu)\big]=\exp \big[\bm{\eta}^\text{T}\bm{T}(x)\big]\end{align}$$ 其中：$\displaystyle \bm{\eta}=\big[\ln \mu,\ln (1-\mu)\big]^\text{T}$$\displaystyle \bm{T}(x) =\big[\mathbb{I}\big(x=0\big),\mathbb{I}\big(x=1\big)\big]^\text{T}$然后这个表示是过于完备以至于很啰嗦，因为特征之间有线性关系，这很不简洁。$$\begin{align}\bm{I}^\text{T}\bm{T}(x)=\mathbb{I}\big(x=0\big)+\mathbb{I}\big(x=1\big)=1\end{align}$$ 因为 $\displaystyle \bm{\eta}$不是唯一的，这要求我们 $\displaystyle \bm{\eta}$最好是最简洁的。也就是说分布均值和唯一的 $\displaystyle \bm{\eta}$联系。这样我们可以定义：$$\begin{align}\mathrm{Ber}\big(x\mid \mu\big)=(1-\mu)\exp \bigg[x\ln \bigg(\frac{\mu}{1-\mu}\bigg)\bigg]\end{align}$$现在 $\displaystyle \eta=\ln \bigg(\frac{\mu}{1-\mu}\bigg)$这是 $\displaystyle \textit{log-odds ratio} $，$\displaystyle T(x)=x$， $\displaystyle Z=1/(1-\mu)$。于是有：$$\begin{align}A(\eta)=\ln Z=\ln \bigg(\frac{1}{1-\mu}\bigg)=\ln \big[1+\mathrm{e}^\eta\big]\end{align}$$来自规范参数的均值 $\displaystyle \mu$$$\begin{align}\mu=\dot{A}(\eta)=\mathrm{sigm}\big(\eta\big)=\frac{1}{1+\mathrm{e}^{-\eta}}=\eta^{-1}(\mu)\end{align}$$ 2.3.2 、分类分布令 $\displaystyle x_i=\mathbb{I}(x=i)$，分类分布指数族形式是：$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\prod_{i=1}^c\mu_i^{x_i}=\exp \Bigg[\sum_{i=1}^cx_i\ln \mu_i\Bigg]\\\\&amp;=\exp \Bigg[\sum_{i=1}^{c-1}x_i\ln \mu_i+\bigg(1-\sum_{j=1}^{c-1}x_j\bigg)\ln \bigg(1-\sum_{j=1}^{c-1}\mu_j\bigg)\Bigg]\\\\&amp;=\exp \Bigg[\sum_{i=1}^{c-1}x_i\ln \bigg(\frac{\mu_i}{1-\sum_{j=1}^{c-1}\mu_j}\bigg)+\ln\bigg(1-\sum_{j=1}^{c-1}\mu_j\bigg)\Bigg]\\\\&amp;=\exp\Bigg[\sum_{i=1}^{c-1}x_i\ln \bigg(\frac{\mu_i}{\mu_c}\bigg)+\ln \mu_c\Bigg]\end{align}$$ 其中 $\displaystyle \mu_c=1-\sum_{j=1}^{c-1}\mu_j$，有：$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\exp \big[\bm{\eta}^\text{T}\bm{T}\big(\bm{x}\big)-A \big(\bm{w}\big)\big]\end{align}$$ 其中$\displaystyle \bm{\eta}=\big[\ln \frac{\mu_1}{\mu_c},\cdots,\ln \frac{\mu_{c-1}}{\mu_c}\big]^\text{T}$$\displaystyle \bm{T}\big(\bm{x}\big)=\bm{x}=\big[\mathbb{I}(x=1),\cdots,\mathbb{I}(x=c-1)\big]^\text{T}$同样，我们可以通过使用的规范参数来恢复均值参数$$\begin{align}\mu_i=\frac{\mathrm{e}^{\eta_i}}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}\end{align}$$易知：$$\begin{align}\mu_c=1-\frac{\sum_{i=1}^{c-1}\mathrm{e}^{\eta_i}}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}=\frac{1}{1+\sum_{j=1}^{c-1}\mathrm{e}^{\eta_j}}\end{align}$$亦有$$\begin{align}A \big(\bm{\eta}\big)=\ln \bigg(1+\sum_{i=1}^{c-1}\mathrm{e}^{\eta_i}\bigg)\end{align}$$如果我们定义 $\displaystyle \eta_c=0\to \mu_{i=c}=0$，那么有 $\displaystyle \bm{\eta}:=\big[\bm{\eta}^\text{T},0\big]^\text{T}$， $\displaystyle \bm{T}:=\big[\bm{T}^\text{T},\mathbb{I}\big(x=c\big)\big]^\text{T}$$$\begin{align}\mathrm{Cat}\big(\bm{x}\mid \bm{\mu}\big)&amp;=\exp \big[\bm{\bm{\eta}^\text{T}T}\big(\bm{x}\big)-A \big(\bm{\eta}\big)\big]\end{align}$$其中 $\displaystyle A \big(\bm{\eta}\big)=\ln \bigg(\sum_{i=1}^{c}\mathrm{e}^{\eta_i}\bigg)\\\\$，特别的有：$$\begin{align}\bm{\mu}&amp;=\mathcal{S}\big(\bm{\eta}\big)=\bm{\eta}^{-1}\big(\bm{\mu}\big)\end{align}$$ 这样我们就通过指数族建立了 $\displaystyle \textit{sigm}$ 函数与 $\displaystyle \textit{0-1} $分布、 $\displaystyle \textit{softmax} $函数与分类分布的联系。换句话说 $\displaystyle \textit{sigm} $ 函数、 $\displaystyle \textit{softmax} $ 函数是连接函数的逆函数。这也是sigm和sofmax函数在机器学习，人工智能广泛应用的原因，因为只要分类问题，就必然涉及。 2.3.3、单变量高斯单变量高斯可以写成指数族形式，如下: $$\begin{align}\mathcal{N}\big(x\mid \mu,\sigma^2\big)&amp;=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{1}{2\sigma^2}\big(x-\mu\big)^2\big]\\\\&amp;=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{\mu^2}{2\sigma^2}\big]\exp \big[-\frac{1}{2\sigma^2}x^2+\frac{\mu}{\sigma^2}x\big]\\\\&amp;=\frac{1}{Z \big(\bm{\eta}\big)}\exp \big[\bm{\eta}^\text{T}\bm{T}(x)\big]\end{align}$$ 其中:$\displaystyle \bm{\eta}=\big[\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\big]^\text{T}$$\displaystyle \bm{T}(x)=\big[x,x^2\big]^\text{T}$$\displaystyle Z \big(\bm{\eta}\big)=\big(2\pi\sigma^2\big)^{-1/2}\exp \big[\frac{\mu^2}{2\sigma^2}\big] $$\displaystyle A \big(\bm{\eta}\big)=\ln Z \big(\bm{\eta}\big)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\ln \big(-2\eta_2\big)-\frac{1}{2}\ln 2\pi$ 2.2.4、非例子并非所有的分布都属于指数族。例如均匀分布 $\displaystyle x\sim \mathrm{U}[a,b]$ 就不是，因为分布支撑集 $\displaystyle \mathcal{X}$取决于参数。此外学生 $\displaystyle T$分布也不属于，因为它没有必要的形式。$\displaystyle \textit{Pitman - Koopman - Darmois} $定理指出，在一定规范条件下，指数家族是唯一具有有限统计量的分布族。(在这里，大小有限与数据集的大小无关。)这个定理要求的条件之一是，分布支撑集 $\displaystyle \mathcal{X}$不依赖于参数。对于这样分布的简单示例：均匀分布$$\begin{align}p(x\mid w)=\mathrm{U}(x\mid w)=\frac{\mathbb{I}\big(0\leqslant x\leqslant w\big)}{w}\end{align}$$似然函数是：$$\begin{align}p \big(\mathcal{D}\big)=w^{-n}\mathbb{I}\big(0\leqslant\max\{x_i\}\leqslant w\big)\end{align}$$ 因此充分统计量是 $\displaystyle n$和 $\displaystyle T=\max_{i}x_i$。它是有限的，但均匀分布并不在指数家族中。因为它的支撑集 $\displaystyle \mathcal{X}=[0,w]$ 依赖于参数 $\displaystyle w$。 2.4、指数族的 $\displaystyle \textit{MLE}$指数家族模型似然函数：$$\begin{align}p \big(\mathcal{D}\mid \bm{w}\big)&amp;=\frac{1}{Z^n(\bm{w})}\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\sum_{i=1}^n \bm{T}(\bm{x}_i)\bigg]\\\\&amp;=\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 易见似然函数也是指数族，它的充分统计量是 $\displaystyle n$和$$\begin{align}\bm{T}\big(\mathcal{D}\big)=\bigg[\sum_{i=1}^n T_1(\bm{x}_i),\cdots,\sum_{i=1}^nT_k(\bm{x}_i)\bigg]^\text{T}\end{align}$$ 例如伯努利分布我们有 $\displaystyle \bm{T}=\bigg[\sum_{i=1}^n \mathbb{I}(\bm{x}_i)\bigg]$和单变量高斯分布 $\displaystyle \bm{T}=\big[\sum_{i=1}^nx_i,\sum_{i=1}^nx_i^2\big]$(我们也需要知道样本大小 $\displaystyle n$) 现在我们给定 $\displaystyle \mathrm{idd}$数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$，用标准指数家族模型来计算 $\displaystyle \textit{MLE} $。对数似然函数：$$\begin{align}\ell(\bm{\eta})=\ln p \big(\mathcal{D}\mid \bm{\eta}\big)=\bigg[\sum_{i=1}^n\ln h(\bm{x}_i)\bigg]+\bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 因为 $\displaystyle -A \big(\bm{\eta}\big)$关于 $\displaystyle \bm{\eta}$是凹的， $\displaystyle \bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})$关于 $\displaystyle \bm{\eta}$是线性的。故对数似然是凹的，因此有唯一的全局最大值。为了得到这个最大值，利用关于对数分配函数导数是充分统计量期望的结论有：$$\begin{align}\frac{\partial \ell}{\partial \bm{\eta}}=\bm{T}(\mathcal{D})-n \mathrm{E}\big[\bm{T}(\bm{x})\big]=\bm{0}\end{align}$$有：$$\begin{align}\mathrm{E}\big[\bm{T}(\bm{x})\big]=\frac{1}{n}\bm{T}\big(\mathcal{D}\big)\end{align}$$ 充分统计量的经验平均值必须等于模型充分统计量的理论期望，即 $\displaystyle \hat{\bm{\eta}}$必须满足上式。这叫做时刻匹配 $\displaystyle \textit{(moment matching)} $。例如在伯努利分布中，我们有 $\displaystyle T(\mathcal{D})=\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)$有：$$\begin{align}\mathrm{E}\big[\bm{T}(\bm{x})\big]=\hat{\mu}=\frac{1}{n}\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)\end{align}$$ 2.5、指数族的贝叶斯我们已经看到，如果先验概率是共轭的，那么精确贝叶斯分析是相当简单的。非正式这意味着先验 $\displaystyle p \big(\bm{\eta}\mid \bm{\tau}\big)$与似然函数 $\displaystyle p \big(\mathcal{D}\mid \bm{\eta}\big)$有相同形式。为了这个有意义，我们要求似然函数的充分统计量是有限，所以我们可以写 $\displaystyle p \big(\mathcal{D}\mid \bm{\eta}\big)=p \big(\bm{T}(\mathcal{D})\mid \bm{\eta}\big)$。这表明，唯一有共轭先验的分布族是指数族。稍后我们将推导先验和后验。 2.5.1、似然函数我们多角度地写出指数族似然函数：$$\begin{align}p \big(\mathcal{D}\mid \bm{w}\big)&amp;=\frac{1}{Z^n(\bm{w})}\bigg[\prod_{i=1}^n h(\bm{x}_i)\bigg]\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})\bigg]\\\\&amp;\propto\exp \bigg[\bm{\eta}^\text{T}(\bm{w})\bm{T}(\mathcal{D})-nA \big(\bm{\eta}\big)\bigg]\\\\&amp;\propto\exp \bigg[n\bm{\eta}^\text{T}\bar{\bm{T}}-nA \big(\bm{\eta}\big)\bigg]\end{align}$$ 其中 $\displaystyle \bar{\bm{T}}=\frac{1}{n}\bm{T}(\mathcal{D})$ 2.5.2、先验共轭先验有如下形式：$$\begin{align}p \big(\bm{\eta}\mid \kappa_0,\bm{\tau}_0\big)\propto g^{\kappa_0}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{\tau}_0\big]\end{align}$$ 令 $\displaystyle \bm{\tau}=\kappa_0\bar{\bm{\tau}}_0$，这样我们分离出先验的虚数据集大小 $\displaystyle \kappa_0$，和虚数据集充分统计量均值 $\displaystyle \bar{\bm{\tau}}_0$。在规范形式中，先验成为：$$\begin{align}p \big(\bm{\eta}\mid \kappa_0,\bar{\bm{\tau}}_0\big)\propto \exp \big[\kappa_0\bm{\eta}^\text{T}\bar{\bm{\tau}}_0-\kappa_0A(\bm{\eta})\big]\end{align}$$ 2.5.3、后验有后验$$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}\big)= p \big(\bm{\eta}\mid \kappa_n,\bm{\tau}_n\big)=p \big(\bm{\eta}\mid \kappa_0+n,\bm{\tau}_0+\bm{T}\big)\end{align}$$ 上式更新了超参。在规范形式下变成了$$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}\big)= p \big(\bm{\eta}\mid \kappa_n,\bar{\bm{\tau}}_n\big)=p \big(\bm{\eta}\mid \kappa_0+n,\frac{ \kappa_0\bar{\bm{\tau}}_0+n\bar{\bm{T}}}{\kappa_0+n}\big)\end{align}$$ 因此我们看到后验超参是先验超参均值和充分统计量平均值一个凸组合。 2.5.4、后验预测密度下面我们鉴于过去的数据集 $\displaystyle \mathcal{D}_t$，推导未来观测 $\displaystyle \mathcal{D}_{t+1}$的一个通用的预测密度的表达式。为简洁记，我们将把充分统计量与数据集大小写在一起： $\displaystyle \tilde{\bm{\tau}}_0=[\kappa_0;\bm{\tau}_0]$、 $\displaystyle \tilde{\bm{T}}_t=\big[n;\bm{T}\big(\mathcal{D}_t\big)\big]$、 $\displaystyle \tilde{\bm{T}}_{t+1}=\big[n;\bm{T}\big(\mathcal{D}_{t+1}\big)\big]$。先验、后验有：$$\begin{align}p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0\big)=\frac{1}{Z( \tilde{\bm{\tau}}_0)}g^{\kappa_0}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{\tau}_0\big]\end{align}$$ $$\begin{align}p \big(\bm{\eta}\mid \mathcal{D}_t\big)&amp;= \frac{p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0\big)p \big(\mathcal{D}_t\mid \bm{\eta}\big)}{p(\mathcal{D}_t)}=p \big(\bm{\eta}\mid \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t\big)\\\\&amp;=\frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}g^{\kappa_0+n_t}(\bm{\eta})\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t\big]\big]\end{align}$$ $\displaystyle p \big(\mathcal{D}_{t+1}\mid \bm{\eta}\big)=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg]g^{n_{t+1}}\big(\bm{\eta}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{T}_{t+1}\big]$那么后验预测为：$$\begin{align}p \big(\mathcal{D}_{t+1}\mid \mathcal{D}_t\big)&amp;=\int p \big(\mathcal{D}_{t+1}\mid \bm{\eta}\big)p \big(\bm{\eta}\mid \mathcal{D}_t\big)d \bm{\eta}\\\\&amp;=\int\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg]g^{n_{t+1}}\big(\bm{\eta}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\bm{T}_{t+1}\big]\frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}g^{\kappa_0+n_t}(\bm{w})\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t\big]\big]d \bm{\eta}\\\\&amp;=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg] \frac{1}{Z( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t)}\int g^{\kappa_0+n_t+n_{t+1}}\big(\bm{w}\big)\exp \big[\bm{\eta}^\text{T}(\bm{w})\big[\bm{\tau}_0+\bm{T}_t+\bm{T}_{t+1}\big]\big]d \bm{\eta}\\\\&amp;=\bigg[\prod_{i=1}^{n_{t+1}} h(\bm{x}_i)\bigg] \frac{Z\big( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t+\tilde{\bm{T}}_{t+1}\big)}{Z\big( \tilde{\bm{\tau}}_0+\tilde{\bm{T}}_t\big)}\end{align}$$的可能性和后部有一个类似的形式。因此如果 $\displaystyle n_t = 0$，这就变成了 $\displaystyle \mathcal{D}_{t+1}$的边际分布，这是我们熟悉后验的归一乘以一个常数。 2.5.5、伯努利分布举例作为一个简单的例子，我们用新表示法来重新讨论一下：贝塔-伯努利模型。似然函数是$$\begin{align}p \big(\mathcal{D}\mid \mu\big)=(1-\mu)^n\exp \bigg[\ln \bigg(\frac{\mu}{1-\mu}\bigg)\sum_{i=1}^n x_i\bigg]\end{align}$$ 共轭先验是：$$\begin{align}p \big(\mu\mid \kappa_0,\tau_0\big)&amp;\propto (1-\mu)^{\kappa_0}\exp \bigg[\ln \bigg(\frac{\mu}{1-\mu}\bigg)\tau_0\bigg]=\mu^{\tau_0}\big(1-\mu\big)^{\kappa_0-\tau_0}\end{align}$$ 如果我们定义 $\displaystyle a_0=\tau_0+1$和 $\displaystyle b_0=\kappa_0-\tau_0+1$，我们可以看到这是一个贝塔分布。 我们可以推导出后验，其中 $\displaystyle T=\sum_{i=1}^n \mathbb{I}\big(x_i=1\big)$是充分统计量:$$\begin{align}p \big(\mu\mid \mathcal{D}\big)&amp;\propto \mu^{\tau_0+T}\big(1-\mu\big)^{\kappa_0-\tau_0+n-T}\\\\&amp;=\mu^{\tau_n}\big(1-\mu\big)^{\kappa_n-\tau_n}\\\\&amp;=\mu^{a_n-1}\big(1-\mu\big)^{b_n-1}\end{align}$$其中 $\displaystyle a_n=\tau_n+1$、 $\displaystyle b_n=\kappa_n-\tau_n+1$ 我们可以推导出后验的预测分布。假设 $\displaystyle p (\mu)=\mathrm{Beta}\big(\mu\mid a_0,b_0\big)$，并让 $\displaystyle T_t=T(\mathcal{D}_t)$是硬币正面在过去的数量。我们可以预测给定一个未来序列 $\displaystyle \mathcal{D}_{t+1}$的出现正面的概率。令这个序列的充分统计量 $\displaystyle T_{t+1}=\sum_{i=1}^m \mathbb{I}\big(x_i^{t+1}=1\big)$$$\begin{align}p \big(\mathcal{D}_{t+1}\mid \mathcal{D}_t\big)&amp;=\int_0^1 p \big(\mathcal{D}_{t+1}\mid \mu\big)p \big(\mu\mid \mathcal{D}_t\big)d\mu\\\\&amp;=\int_0^1 p \big(\mathcal{D}_{t+1}\mid \mu\big)\mathrm{Beta} \big(\mu\mid a_{t},b_{t}\big)d\mu\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma{a_t}\Gamma(b_t)}\int_0^1 \mu^{a_t+T_{t+1}-1}\big(1-\mu\big)^{b_t+m-T_{t+1}-1}\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma{a_t}\Gamma(b_t)}\int_0^1 \mu^{a_{t+1}-1}\big(1-\mu\big)^{b_{t+1}-1}\\\\&amp;=\frac{\Gamma(a_t+b_t)}{\Gamma(a_t)\Gamma(b_t)}\frac{\Gamma(a_{t+1})\Gamma(b_{t+1})}{\Gamma(a_{t+1}+b_{t+1})}\end{align}$$其中$\displaystyle a_{t+1}=a_t+T_{t+1}=\tau_0+T_t+1+T_{t+1}$$\displaystyle b_{t+1}=\kappa_n-\tau_n+1=\kappa_0+n-(\tau_0+T_t)+1+m-T_{t+1}$ 2.6、指数族与最大熵原理 $\displaystyle \textit{(Maximum Entropy)} $虽然指数家族很方便，但对它的使用有更深层次的理由吗？事实存在这样的情况：如果用最少约束来假设数据，特别是假设某些特征或函数的期望$$\begin{align}\int \bm{\phi}(\bm{x})p(\bm{x})d \bm{x}=\mathrm{E}\big[\bm{\phi}\big]=\bm{\zeta}\end{align}$$ $\displaystyle \bm{\zeta}\in \mathbb{R}^k$是已知常数向量， $\displaystyle \bm{\phi}(\bm{x})$是一个任意向量函数，即要求满足于分布矩与指定函数经验矩相匹配的约束条件，那么最大熵原理 $\displaystyle \textit{maxent} $告诉我们应该选择最大熵分布(最接近于均匀分布的那个)。有约束条件的熵最大化$$\begin{align}J(p)=&amp; \mathrm{H}[p]=-\int p (\bm{x})\ln p(\bm{x}) d \bm{x}\\\\&amp;\mathrm{s.t.}\begin{cases}\displaystyle p(\bm{x})\geqslant 0\\\\\displaystyle \int p(\bm{x})d \bm{x}= 1\\\\\displaystyle \int \bm{\phi}(\bm{x})p(\bm{x})d \bm{x}=\bm{\zeta}\end{cases}\end{align}$$我们需要使用拉格朗日乘数法，那么拉格朗日算符更新为：$$\begin{align}F(p)=-p (\bm{x})\ln p(\bm{x})+\lambda p(\bm{x})+\bm{\eta}^\text{T}\big[\bm{\phi}(\bm{x})p(\bm{x})\big]\\\\\end{align}$$ 我们可以用变分法计算函数 $\displaystyle p$，欧拉-拉格朗日方程是：$$\begin{align}-1-\ln p(\bm{x})+\lambda+\bm{\eta}^\text{T}\bm{\phi}(\bm{x})=0\iff p(\bm{x})=\exp[\lambda-1]\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]\end{align}$$亦有 $\displaystyle p(\bm{x})\propto\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]$、给出了归一化常数 $\displaystyle Z=\int \exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]d \bm{x}$ 有：$$\begin{align}p(\bm{x})=\frac{1}{Z}\exp \big[\bm{\eta}^\text{T}\bm{\phi}(\bm{x})\big]\end{align}$$因此，最大熵分布 $\displaystyle p(x)$具有指数族的形式，也称为吉布斯分布 $\displaystyle \textit{(Gibbs Distribution)} $。当然我们也可以写成离散形式，这是很容易的，略。 三、广义线性模型$\displaystyle \textit{(GLMs)} $3.1、概要我们熟悉了一下指数族，建立广义模型的初衷是要解决，经典线性模型$$\begin{align}y=\bm{w}^\text{T}\bm{x}+e\sim\mathcal{N}(\mu,\sigma^2)\end{align}$$的缺点： 1、因变量 $\displaystyle y$是连续的且服从高斯分布。 2、方差是固定的。 广义线性模型 于是内尔得和韦德伯恩（Nelder &amp; Wedderburn，1972）提出了广义线性模型。我们对指数族的形式稍加修改。$$\begin{align}p\left(y\mid\eta,\delta\right)=\exp\left[\frac{y \cdot\eta-A(\eta)}{\delta}+c\left(y,\delta\right)\right]\end{align}$$其中 $\displaystyle \delta$是散度参数，通常是1、 $\displaystyle c\left(y,\delta\right)$是归一化参数、 $\displaystyle A(\eta) $ 是分配函数、 $\displaystyle \eta$是连接函数，所谓规范连接函数可以查阅维基百科。同时我们令连接函数：$\displaystyle g(\mu)=\eta=\bm{w}^\text{T}\bm{x}$ $\displaystyle \mu=\mathrm{E}\left[y\mid \bm{x}\right]=\dot{A}\left(\eta\right)=g^{-1}(\eta)$ $\displaystyle \mathrm{Var}\left[y\right]=\ddot{A}\left(\eta\right)\cdot\delta$于是有$$\begin{align}\left\{\begin{matrix}g(\mu)=\eta=\bm{w}^\text{T}\bm{x}\\\\e=y-\mu\\\\e\sim f \end{matrix}\right.\end{align}$$ 再次写出模型：$$\begin{align}p \left(y\mid \bm{x} ,\bm{w} ,\delta\right)=\exp\left[\frac{ y\cdot\bm{w}^\text{T}\bm{x}-A(\bm{w}^\text{T}\bm{x})}{\delta}+c\left(y,\delta\right)\right]\end{align}$$ 经过以上分析,可以知道广义线性模型的两个重要不同：1、连接函数：是因变量 $\displaystyle y$的期望的一个转换，此转换的变量 $\displaystyle g(\mu)$是回归参数 $\displaystyle \bm{w}$的一个线性函数 $\displaystyle \bm{x}^\text{T}\bm{w}$。 2、方差是因变量 $\displaystyle y$期望的函数： $\displaystyle\mathrm{Var}\left[y\right]=\ddot{A}\left(g(\mu)\right)\cdot\delta$ 3.2、对数似然函数广义线性模型有一个很吸引人的特性，即它可与逻辑斯蒂回归使用的方法相同。对于数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i,y_i\}_{i=1}^{n}$我们有对数似然函数： $$\begin{align}\ell(\bm{w})&amp;=\ln p \left(\mathcal{D}\mid \bm{w}\right)=\frac{1}{\delta}\left[\bm{y}^\text{T}\bm{X}\bm{w}-\sum_{i-1}^{n}A(\bm{w}^\text{T}\bm{x}_i)\right]+\sum_{i=1}^{n}c\left(y_i,\delta\right)\\\\&amp;=\frac{1}{\delta}\left[\bm{y}^\text{T}\bm{\eta}-\bm{I}^\text{T}A(\bm{\eta})\right]+\bm{I}^\text{T}c\left(\bm{y},\delta\right)\end{align}$$ 有梯度：$$\begin{align}&amp;\nabla\ell=\frac{\partial{\ell}}{\partial{\bm{w}}}=\frac{1}{\delta}\left[\bm{X}^\text{T}\bm{y}-\sum_{i=1}^{n}A’\cdot \bm{x}_i\right]=\frac{1}{\delta}\left[\bm{X}^\text{T}\bm{y}-\bm{X}^\text{T}\bm{\mu} \right]=\frac{1}{\delta}\bm{X}^\text{T}\left[\bm{y}-\bm{\mu} \right]\end{align}$$ 有海赛矩阵：$$\begin{align}&amp;\bm{H}\left(\ell\right)=\frac{\partial^2{\ell}}{\partial{\bm{w}\partial\bm{w}^\text{T}}}=-\frac{1}{\delta}\bm{X}^\text{T}\frac{\partial{\bm{\mu}}}{\partial{\bm{\eta}^\text{T}}}\frac{\partial{\bm{\eta}}}{\partial{\bm{w}^\text{T}}}=-\frac{1}{\delta}\bm{X}^\text{T}\bm{S}\bm{X}\\\\\end{align}$$ 其中：$\displaystyle \bm{S}=\frac{\partial{\bm{\mu}}}{\partial{\bm{\eta}^\text{T}}}=\mathrm{diag}\left[\frac{\partial \mu_i}{\partial\eta_i}\right]=\mathrm{diag}\left[\frac{\partial g^{-1}(\eta_i)}{\partial\eta_i}\right]$ $\displaystyle \frac{\partial{\bm{\eta}}}{\partial{\bm{w}^\text{T}}}=\frac{\partial \bm{X}\bm{w}}{\partial\bm{w}^\text{T}}=\bm{X} $ 3.3、牛顿-拉弗迭代法。为了求解模型，现在我们开考虑一下求极值问题,我们要求这样一个数量函数的极值：$$\begin{align}\max_{\bm{x}} f\left(\bm{x}\right)\end{align}$$ 我们有梯度： $\displaystyle \nabla f=\frac{\partial f}{\partial\bm{x}}=0$ 同时有海赛矩阵 $\displaystyle \bm{H}\left(f\right)=\frac{\partial f}{\partial\bm{x}\bm{x}^\text{T}}$。 于是我们有梯度的泰勒展开：$$\begin{align}\nabla\left(\bm{x}_t\right)=\nabla\left(\bm{x}_t\right)+\bm{H}\left(\bm{x}_t\right)\left[\bm{x}_{t+1}-\bm{x}_t\right]+\bm{r}\left(\bm{x}_t\right)=0\end{align}$$ 忽略余项，可以求得： $\displaystyle \bm{x}_{t+1}=\bm{x}_{t}-\bm{H}^{-1}\left(\bm{x}_t\right)\nabla \left(\bm{x}_t\right)$简写为：$$\begin{align}\bm{x}:=\bm{x}-\bm{H}^{-1}\nabla\end{align}$$ $\displaystyle \\\\$ 3.4、极大似然分析$$\begin{align}\bm{w}_{t+1}&amp;=\bm{w}_t -\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\left[\bm{y}-\bm{\mu}_t\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\bm{w}_t-\bm{X}^\text{T}\left[\bm{y}-\bm{\mu}_t\right]\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\left[\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]\right]\\\\&amp;=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\bm{\zeta}_t\end{align}$$其中：$\displaystyle \bm{\eta}_t=\bm{X}\bm{w}_t$$\displaystyle \bm{\mu}_t=g^{-1}\left(\bm{\eta}_t\right)$$\displaystyle \bm{S}_t=\frac{\partial\bm{\mu}_t}{\partial\bm{\eta}_t^\text{T}}=\mathrm{diag}\left[\frac{\partial g^{-1}(\eta_i^t)}{\partial\eta_i^t}\right]$$\displaystyle \bm{\zeta}_t=\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]$ 现在我们加以总结，历史上我们称这种算法为：迭代加权最小二乘法（IRLS） 算法：迭代加权最小二乘法（Iteratively reweighted least squares） 1 $\displaystyle \bm{w}_0=g(\bar{y})$2 $\displaystyle \text{while }\bm{w}=\text{converged}$ : $\displaystyle\quad\begin{array}{|lc}\bm{\eta}_t=\bm{X}\bm{w}_t\\\\\displaystyle\bm{\mu}_t=g^{-1}\left(\bm{\eta}_t\right)\\\\\displaystyle\bm{S}_t=\frac{\partial\bm{\mu}_t}{\partial\bm{\eta}_t^\text{T}}\\\\\displaystyle\bm{\zeta}_t=\bm{\eta}_t-\bm{S}_t^{-1}\left[\bm{y}-\bm{\mu}_t\right]\\\\\displaystyle\bm{w}_{t+1}=\left[\bm{X}^\text{T}\bm{S}_t\bm{X}\right]^{-1}\bm{X}^\text{T}\bm{S}_t\bm{\zeta}_t\end{array}\\\\$3 #end while 如果我们扩展这个推导来处理非规范连接函数，我们就会发现海赛矩阵有另一个术语。然而结果表明海赛矩阵期望与公式相同；使用海赛矩阵期望(称为 $\displaystyle \textit{Fisher} $信息矩阵)来替代实际的海赛矩阵，称为 $\displaystyle \textit{Fisher}$评分方法。 在此基础上，我们可以简单地将上述过程修改为高斯先验的 $\displaystyle \textit{MAP} $估计，即我们只需修改目标、梯度和海赛矩阵，就像我们对逻辑斯蒂回归增加 $\displaystyle \ell_2$正则一样。 4、评述使用广义线性模型我们解决了指数族分布的通用贝叶斯模型。这里我们跳过了多元高斯分布的模型。这涉及到高维，和多元统计的wishart分布。需要更为高级的工具： 1、格拉斯曼代数2、微分形式3、外微分 稍后，我们将一一提及它们。继续我们的星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-09-广义线性模型/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>广义线性模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[狄利克雷-多项式模型(Dirichlet-Multionmial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-08-%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 简介: 本文总结了狄利克雷-多项式模型的部分和我自己的一些体会，我们将再一次熟悉贝叶斯方法的基本概念、流程、特点。把我们思维进化到更高的维度。摘要：本文意在狄利克雷-多项式模型的问题。若有错误，请大家指正。关键词: 狄利克雷-多项式模型,分类分布,狄利克雷分布 一、狄利克雷-多项式模型(dirichlet-multionmial model)分类分布(Categorical distribution) 或者又叫1-C分布(multinoulli distribution) $$\begin{align}\bm{x}\sim \mathrm{Cat}(\bm{x}\mid\bm{\mu})\end{align}$$其中 $\displaystyle \bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\\\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1$ 在这里有必要解释一下符号问题。首先分类分布是对0-1分布的推广，这句话可能不好理解。我们可以这样思考，0-1分布是抛硬币。而分类分布类似于掷骰子。为了让符号统一我们使用小写 $\displaystyle c$表示有C个分类，例如 $\displaystyle c=6$可以类比于掷骰子。这样很多问题就好理解了。变量 $\displaystyle \bm{x}=\begin{bmatrix} x_1\\\ \vdots \\x_j\\\vdots\\x_c \end{bmatrix}$ 实例或者一个观测 $\displaystyle \bm{x}_i=\begin{bmatrix} x_{i1}\\\ \vdots \\x_{ij}\\\vdots\\x_{ic} \end{bmatrix}$ 例子： $\displaystyle \bm{x}_s=\begin{bmatrix} 0\ \\\vdots \\1\\\vdots\\0 \end{bmatrix} $ 1、分类分布(multinoulli distribution)概率质量函数：$$\begin{align}\mathrm{PMF：} \mathrm{Cat}(\bm{x}\mid\bm{\mu})=\prod_{i=1}^{c}\mu_i^{x_i}\end{align}$$ 其中： $\displaystyle \bm{x}=[x_1,x_2,…,x_c]^\text{T}\,,\bm{\mu}=[\mu_1,\mu_2,…,\mu_c]^\text{T}\,,\bm{x}\in\{0,1\}^c\,,\sum_{j=1}^{c}x_j=\bm{I}^\text{T}\bm{x}=1\,,\sum_{i=1}^{c}\mu_i=\bm{I}^\text{T}\bm{\mu}=1$这个表示方法也称为 $\displaystyle 1\,of\,c$编码方法。这个方法方便计算。 其他表示方法：$\displaystyle \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)},x\in\{1,…,c\}$,这个示性函数表示方法，有重要应用,降低表示维度，节约了有限的数学符号。 2、均值与方差我们知道向量函数微分结果： $\displaystyle \bm{f}(\bm{x})=\mathrm{e}^{\bm{A}\bm{x}} $ 有 $\displaystyle \frac{\partial\bm{f}}{\partial \bm{x}^\text{T}}=\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\bm{A}$。且有： $\displaystyle f(\bm{x})=\bm{\mu}^\text{T}\mathrm{e}^{\bm{A}\bm{x}}$，易得：$$\begin{align} &amp;\frac{\partial{f}}{\partial{\bm{x}}} =\mathrm{diag}\left[\mathrm{e}^{\bm{A}\bm{x}}\right]\bm{A}\bm{\mu}\\ &amp;\frac{\partial^2{f}}{\partial{\bm{x}}^2} =\bm{A}^\text{T}\mathrm{diag}[\mathrm{e}^{\bm{A}\bm{x}}]\mathrm{diag}[\bm{\mu}]\bm{A}\end{align}$$ 我们又知道特征函数： $\displaystyle \varphi_\bm{x}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{x}}]=\sum_{\bm{I}^\text{T}\bm{x}=1}p_j\mathrm{e}^{\mathrm{i}t x_j}=\sum_{j=1}^{c}\mu_i\mathrm{e}^{\mathrm{i}t x_j}=\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}$。于是可分析得到分类分布 $\displaystyle \bm{x}\sim \mathrm{Cat}(\bm{x}\mid \bm{\mu})$的期望与方差(协方差矩阵)： $\displaystyle \mathrm{E}[\bm{x}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}\right]\bm{X}\bm{\mu}=\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{x}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{x}]\mathrm{E}^\text{T}[\bm{x}]$$\displaystyle =\left.\bm{X}^\text{T}\mathrm{diag}[\mathrm{e}^{\mathrm{i}\bm{X}\bm{t}}]\mathrm{diag}[\bm{\mu}]\bm{X}\right| _{\bm{t}=\bm{0}}-\bm{\mu}\bm{\mu}^\text{T}\\=\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}$ 其中： $\displaystyle \bm{X}=\bm{I}_{c\times c}$是 $\displaystyle c\times c$维的单位矩阵。也就说遍历了 $\displaystyle \bm{x}$所有可能取值组成的矩阵。为了表示简洁，我们在其中选用了$\displaystyle c\times c$维的单位矩阵。 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^{n}$于是有似然函数$$\begin{align}\mathrm{L}(\bm{\mu})=p(\mathcal{D}\mid\bm{\mu})&amp;=\prod_{i=1}^{n}\prod_{j=1}^{c}\mu_j^{x_j}=\prod_{j=1}^{c}\mu_j^{\sum_{i=1}^{n}x_{ij}}=\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中： $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$表示第 $\displaystyle j$个分类在 $\displaystyle n$次观测中发生了 $\displaystyle k_j$次。同时我们知道 $\displaystyle \sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1,\,\sum_{j=1}^{c}k_j=\bm{I}^\text{T}\bm{k}=n$ 4、对数似然函数$$\begin{align}\ell(\bm{\mu},\lambda)&amp;\propto\ln\prod_{j=1}^{c}\mu_j^{k_j}+\lambda(\sum_{j=1}^{c}\mu_j-1)\\&amp;=\bm{k}^\text{T}\ln\bm{\mu}+\lambda[\bm{I}^\text{T}\bm{\mu}-1]\end{align}$$ 5、求极大似然估计$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}&amp;=\frac{\bm{k}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}&amp;=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$分析可得 $\displaystyle \bm{\mu}_{MLE}=-\frac{\bm{k}}{\lambda} $ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}-1=0$得： $\displaystyle \frac{\bm{I}^\text{T}\bm{k}}{\lambda}=-1$由于 $\displaystyle \bm{I}^\text{T}\bm{k}=n$可得：$$\begin{align}\lambda=-n\end{align}$$ $$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 其中 $\displaystyle \mu_j^{MLE}=\frac{k_j}{n},\,k_j=\sum_{i=1}^{n}x_{ij}$。 6、多项式分布我们定义 $\displaystyle k_j=\sum_{i=1}^{N}x_{ij}$是分类 $\displaystyle j$在 $\displaystyle n$次观测中发生的次数。同时令 $\displaystyle \bm{k}=[k_1,…,k_j,…,k_c]^\text{T}$ 则有：$$\begin{align}\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)=\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 其中 $\displaystyle \sum_{j=1}^{c}k_j=n\,,\sum_{j=1}^{c}\mu_j=1$ 我们有特征函数：$\displaystyle \varphi_\bm{k}(\bm{t})=\mathrm{E}[\mathrm{e}^{\mathrm{i}\bm{t}^\text{T}\bm{k}}]=\sum_{\bm{I}^\text{T}\bm{k}=n}p_i\mathrm{e}^{\mathrm{i}t k_j}=\sum_{\bm{I}^\text{T}\bm{k}=n}\frac{n!}{k_1!k_2!…k_c!}\prod_{j=1}^{c}\left(\mu_j\mathrm{e}^{\mathrm{i}t}\right)^{k_j}=\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^n$。于是可得多项式分布的均值与协方差矩阵 $\displaystyle \mathrm{E}[\bm{k}]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(\bm{t})}}{\partial{\bm{t}}}\right| _{\bm{t}=\bm{0}}=-(\mathrm{i})^2n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}=n\bm{\mu}$ $\displaystyle \mathrm{cov}[\bm{k}]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(\bm{t})}}{\partial{\bm{t}}^2}\right| _{\bm{t}=\bm{0}}-\mathrm{E}[\bm{k}]\mathrm{E}^\text{T}[\bm{k}]$$=\left.n\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-1}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\mathrm{diag}[\bm{\mu}]\right| _{\bm{t}=\bm{0}}+\left.n(n-1)\left(\bm{\mu}^\text{T}\mathrm{e}^{\mathrm{i}\bm{t}}\right)^{n-2}\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\left[\mathrm{diag}\left[\mathrm{e}^{\mathrm{i}\bm{t}}\right]\bm{\mu}\right]^\text{T}\right| _{\bm{t}=\bm{0}}-n^2\bm{\mu}\bm{\mu}^\text{T}$$=n\mathrm{diag}[\bm{\mu}]+n(n-1)\bm{\mu}\bm{\mu}^\text{T}-n^2\bm{\mu}\bm{\mu}^\text{T}\\=n\left[\mathrm{diag}[\bm{\mu}]-\bm{\mu}\bm{\mu}^\text{T}\right]$ 7、共轭先验分布把 $\displaystyle \bm{\mu}$作为变量。观察似然函数，我们知道$$\begin{align}p(\bm{\mu})\propto p(\mathcal{D}\mid\bm{\mu})= \prod_{j=1}^{c}\mu_j^{k_j}\end{align}$$ 我们知道狄利克雷分布 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\Gamma(a_0)}{\Gamma(a_1)\Gamma(a_2)…\Gamma(a_c)}\prod_{j=1}^{c}\mu_j^{a_j-1}$。而这正是我们需要的共轭先验，即后验与先验有相同的函数形式：$$\begin{align}\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\end{align}$$ 其中： $\displaystyle \mu_j\in[0,1]\,\sum_{j=1}^{c}\mu_j=\bm{I}^\text{T}\bm{\mu}=1\,,a_0=\bm{I}^\text{T}\bm{a}=a_1+…+a_c,\,a_j&gt;0$ 利用 $\displaystyle \Gamma,\,\mathrm{B}$函数性质容易知道：$\displaystyle \mathrm{E}(\bm{\mu})=\frac{\bm{a}}{\bm{I}^\text{T}\bm{a}}=\frac{\bm{a}}{a_0}$ $\displaystyle \mathrm{cov}(\bm{\mu})=\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{\bm{a}-\bm{1}}{a_0-c}$ 为了方便使用，我们把它写成离散形式：$\displaystyle \mathrm{E}(\mu_j)=\frac{a_j}{a_0}$ $\displaystyle \mathrm{var}(\mu_j)=\frac{(a_0-a_j)a_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{cov}(\mu_i,\mu_j)=\frac{-a_ia_j}{a_0^2(a_0+1)}$ $\displaystyle \mathrm{mode}[\mu_j]=\mathop{\mathrm{argmax}}_{\mu_j}\,\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{a_i-1}{a_0-c}$ 8、后验分布我们用似然函数乘以狄利克雷先验得到后验分布：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})\propto\mathrm{Mu}(\bm{k}\mid \bm{\mu},n)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\end{align}$$ 归一化得：$$\begin{align}p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})\end{align}$$ 1、在线学习我们发现狄利克雷分布也具有再生性质，和在线性学习性质，我们假设，陆续观测到两个的数据集 $\displaystyle \mathcal{D}_1\,\mathrm{D}_2$。1、当我们观察到 $\displaystyle \mathcal{D}_1$时，有$$ p(\bm{\mu}\mid\mathcal{D}_1)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_1+\bm{a}) $$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\bm{\mu}) p(\bm{\mu}\mid\mathcal{D}_1) $$易得： $$\begin{align}p(\bm{\mu}\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_2+\bm{k}_1+\bm{a})\end{align}$$ 2、后验分析最大后验估计$$\begin{align}\bm{\mu}_{MAP}=\mathrm{mode}[\bm{\mu}]=\mathop{\mathrm{argmax}}_{\bm{\mu}}\,\mathrm{Dir}(\bm{\mu}\mid \bm{k}+\bm{a})=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}\end{align}$$ 极大似然估计：$$\begin{align}\bm{\mu}_{MLE}=\frac{\bm{k}}{n}\end{align}$$ 后验均值:$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}\end{align}$$ 后验均值是先验均值和最大似然估计的凸组合。知道 $\displaystyle \bm{\mu}=\frac{\bm{a}}{a_0}$$$\begin{align}\mathrm{E}(\bm{\mu}\mid\mathcal{D})=\frac{\bm{k}+\bm{a}}{n+a_0}=\frac{a_0}{n+a_0}\bm{\mu}_0+\frac{n}{n+a_0}\bm{\mu}_{MLE}\end{align}$$我们发现 $\displaystyle a_0$可以理解为先验对于后验的等价样本大小。 $\displaystyle \frac{a_0}{n+a_0}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计，知道 $\displaystyle \mathrm{mode}[\bm{\mu}_0]=\frac{\bm{a}-\bm{1}}{a_0-c}$$$\begin{align}\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c}=\frac{a_0-c}{n+a_0-c}\mathrm{mode}[\bm{\mu}_0]+\frac{n}{n+a_0-c}\bm{\mu}_{MLE}\end{align}$$ 3、 拉格朗日乘数法首先为了让符号有意义我们定义向量点除运算 $\displaystyle \frac{1}{\bm{a}}=1./\bm{a}=[1/a_1,…,1/a_n]^\text{T}$下面我们用拉格朗日乘数法在推理一遍，我们知道 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1 $，于是有：$$\begin{align}\ell(\bm{\mu},\lambda)=\ln \prod_{j}^{c}\mu_j^{k_j+a_j-1}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)=[\bm{k}+\bm{a}-\bm{1}]^\text{T}\ln\bm{\mu}+\lambda(\bm{I}^\text{T}\bm{\mu}-1)\end{align}$$求解得：$$\begin{cases}\displaystyle\frac{\partial{\ell}}{\partial{\bm{\mu}}}=\frac{\bm{k}+\bm{a}-\bm{1}}{\bm{\mu}}+\lambda\bm{I}=\bm{0}\\\displaystyle\frac{\partial{\ell}}{\partial{\lambda}}=\bm{I}^\text{T}\bm{\mu}-1=0\end{cases}$$ 知道： $\displaystyle \bm{\mu}_{MAP}=-\frac{\bm{k}+\bm{a}-\bm{1}}{\lambda}$ 代入 $\displaystyle \bm{I}^\text{T}\bm{\mu}=1$得： $\displaystyle \lambda=n+a_0-c$$$\bm{\mu}_{MAP}=\frac{\bm{k}+\bm{a}-\bm{1}}{n+a_0-c} $$ 写成离散形式有$$\begin{align}\mu_j^{MAP}=\frac{k_j+a_j-1}{n+a_0-c}\end{align}$$ 4、后验协方差矩阵$$\begin{align}\mathrm{cov}(\bm{\mu})=\frac{(n+a_0)\mathrm{diag}[\bm{k}+\bm{a}]-[\bm{k}+\bm{a}][\bm{k}+\bm{a}]^\text{T}}{(n+a_0)^2(n+a_0+1)}\end{align}$$当 N足够大时： $$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降 5、后验预测分布开始分析之前，我们回忆一下B函数：$\displaystyle \mathrm{B}(\bm{a})=\int_{\bm{x}\in [0,1]^c}x_i^{a_i-1}\mathrm{d}\bm{x}\,,a_i&gt;0\,and\,\sum_{i-1}^{n}x_i=1$且有： $\displaystyle \mathrm{B}(\bm{a})=\frac{\prod_{i=1}^{n}\Gamma(a_1)\cdots \Gamma(a_n)}{\Gamma(a_0)}$。同时为了简化符号，我们令后验分布 $\displaystyle p(\bm{\mu}\mid\mathcal{D})=\mathrm{Dir}(\bm{\mu}\mid \bm{a})$现在我们令下一次观测 $\displaystyle \bm{x}_{n+1}=\tilde{\bm{x}}$。我们现在想知道 $\displaystyle \tilde{\bm{x}}$各种情况下概率，以辅助决策。考虑到 $\displaystyle \tilde{\bm{x}}\in\{0,1\}^c \,,\bm{\mu}\in[0,1]^c=\mathcal{U}$。我们有：$$\begin{align}p(\tilde{\bm{x}}\mid \mathcal{D})&amp;=\int_{\mathcal{U}}p(\tilde{\bm{x}}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\mathrm{Cat}(\bm{x}\mid\bm{\mu})\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{\tilde{x}_j}\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\int_{\mathcal{U}}\prod_{j=1}^{c}\mu_j^{a_j+\tilde{x}_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma\left(\bm{I}^\text{T}[\bm{a}+\bm{x}]\right)}=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}\Gamma(a_j+\tilde{x}_j)}{\Gamma(a_0+1)}\\&amp;=\frac{\Gamma(a_0)}{\displaystyle \prod_{j=1}^{c}\Gamma(a_j)}\frac{\displaystyle \prod_{j=1}^{c}a_j^{\tilde{x}_j}\prod_{j=1}^{c}\Gamma(a_j)}{a_0^{\tilde{x}_j}\Gamma(a_0)}=\prod_{j=1}^{c}\frac{a_j}{a_0}^{\tilde{x}_j}=\prod_{j=1}^{c}\mathrm{E}^{\tilde{x}_j}(\bm{\mu}\mid\mathcal{D})\\&amp;=\mathrm{Cat}\left(\tilde{\bm{x}}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于 $\displaystyle plug-in\,\mathrm{E}[\mu\mid\mathcal{D}]$。我们也可以分析出类似贝塔-伯努利模型的结论。 6、多试验后验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么分类 $\displaystyle j$发生 $\displaystyle s_j$次的概率。写成向量形式 $\displaystyle \bm{s}$。于是有：$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_{\mathcal{U}}\frac{m!}{s_1!…s_c!}\prod_{j=1}^{c}\mu_j^{s_j}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 我们称 $\displaystyle \mathrm{Dm}(\bm{s}\mid \mathcal{D},m)=\frac{m!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}$为狄利克雷-多项式分布(Dirichlet-multionmial distribution)。后验预测分布的均值与协方差矩阵问题 后验预测分布均值：$$\begin{align} \mathrm{E}[s_j\mid \mathcal{D},m] &amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=\int_{\mathcal{U}}\mathrm{E}(s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}=\int_{\mathcal{U}}m\mu_j\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\ &amp;=m\frac{\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+1)}=m\frac{a_j}{a_0}\end{align}$$ $\displaystyle \mathrm{E}[\bm{s}\mid \mathcal{D},m]=m\frac{\bm{a}}{a_0}$ 后验预测分布方差：$$\begin{align} \mathrm{var}[s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s^2_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{E}(s_j^2)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=\int_{\mathcal{U}}[m\mu_j+m(m-1)\mu_j^2]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}^2[s_j]\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{\Gamma(a_j+2)\Gamma(a_0)}{\Gamma(a_j)\Gamma(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=m\frac{a_j}{a_0}+m(m-1)\frac{(a_j+1)a_j}{a_0(a_0+2)}-m^2\frac{a_j^2}{a_0^2}\\&amp;=\frac{ma_j(a_0-a_j)}{a_0^2}\frac{a_0+m}{a_0+1}\end{align}$$ 后验预测分布协方差：$$\begin{align} \mathrm{cov}[s_i,s_j\mid \mathcal{D},m]&amp;=\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\sum_{\bm{I}^\text{T}\bm{s}=1}s_is_j\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}\mathrm{cov}(s_i,s_j)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\mathrm{E}[s_i]\mathrm{E}[s_j]\\&amp;=\int_{\mathcal{U}}[-m\mu_i\mu_j]\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{\Gamma(a_i+1)\Gamma(a_j+1)\Gamma(a_0)}{\Gamma(a_i)\Gamma(a_j)\Gamma(a_0+2)}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=-m\frac{a_ia_j}{(a_0+1)a_0}-\frac{m^2a_ia_j}{a_0^2}\\&amp;=\frac{-ma_ia_j}{a_0^2}\frac{a_0+m}{a_0+1}\\\end{align}$$ 后验预测分布协方差矩阵：$\displaystyle \mathrm{cov}[\bm{s}\mid \mathcal{D},m]=m(m+a_0)\frac{a_0\mathrm{diag}[\bm{a}]-\bm{a}\bm{a}^\text{T}}{a_0^2(a_0+1)}$ 对于 $plug−in\,\bm{\mu}_{MAP}$插值 $\displaystyle \mathrm{Mu}(\bm{s}\mid \bm{\mu}_{MAP},m)$，我们知道 $\displaystyle \bm{\mu}_{MAP}=\frac{\bm{a}-\bm{1}}{a_0-c}$其协方差矩阵为：$\displaystyle \mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]=m\left[\mathrm{diag}[\bm{\mu}_{MAP}]-\bm{\mu}_{MAP}\bm{\mu}_{MAP}^\text{T}\right]$ $\displaystyle \mathrm{var}[s_j\mid \bm{\mu}_{MAP},m]=\frac{m(a_j-1)(a_0-a_j+1-c)}{(a_0-c)^2}$ $\displaystyle \mathrm{cov}[s_i,s_j\mid \bm{\mu}_{MAP},m]=\frac{-m(a_i-1)(a_j-1)}{(a_0-1)^2}$比较大小容易证明：$$\begin{align}\mathrm{cov}[\bm{s}\mid \mathcal{D},m]\geqslant\mathrm{cov}[\bm{s}\mid \bm{\mu}_{MAP},m]\end{align}$$所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论。 7、数据集后验预测与边缘似然函数有数据集 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，这有$$\begin{align} p(\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_t\mid\bm{\mu})p(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^t}\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ $$\begin{align} p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid \bm{a}) &amp;=\int_\mathcal{U}p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid\mathcal{D}_t,\bm{a})\mathrm{d}\bm{\mu}\\&amp;=\int_\mathcal{U}\prod_{j=1}^{c}\mu_j^{k_j^{t+1}}\frac{1}{\mathrm{B}(\bm{k}_t+\bm{a})}\prod_{j=1}^{c}\mu_j^{k_j+a_j-1}\mathrm{d}\bm{\mu}\\&amp;=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{k}_t+\bm{a})}\end{align}$$ 所以有：$$\begin{align} p(\mathcal{D}_{t+1}\mid\mathcal{D}_t,\bm{a})=\frac{p(\mathcal{D}_{t+1}\,\mathcal{D}_t\mid\bm{a})}{ p(\mathcal{D}_t\mid\bm{a})}=\frac{\mathrm{B}(\bm{k}_{t+1}+\bm{k}_t+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$ 9、评述通过狄利克雷-多项式模型，我们从离散二维拓展到了离散多维。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Cat}(x\mid\bm{\mu})=\prod_{j=1}^{c}\mu_j^{\mathbb{I}(x=j)}, x\in\{1,..,c\},\bm{\mu}\in[0,1]^c $ 这个分布由 $\displaystyle \bm{\mu}$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \bm{\mu}$。于是人类搞了个假设空间 $\displaystyle \mathcal{H}=\{\bm{\mu}_i\}$，为了找到上帝的那个 $\displaystyle \hat{\bm{\mu}}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。并且动用了自己的经验和感觉，人类假设 $\displaystyle \bm{\mu}\sim\mathrm{Dir}(\bm{\mu}\mid \bm{a})=\frac{1}{\mathrm{B}(\bm{a})}\prod_{j=1}^{c}\mu_j^{a_j-1}$。另外我们还可以结合，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}…\mathcal{D}_{\infty}$，来给假设空间的每个 $\displaystyle \bm{\mu}$更新概率。于是这个表达式横空出世 $\displaystyle p(\bm{\mu}\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\bm{\mu})p(\bm{\mu}\mid \mathcal{D}_{t})$。于是我们得到了： $$p(\bm{\mu}\mid\mathcal{D}_t)=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_t+\bm{a})$$这样我们就把假设空间的 $\displaystyle \bm{\mu}$都给了个概率。这样我们就有关于 $\displaystyle \bm{\mu}$决策的信息。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$p(\tilde{x}\mid \mathcal{D})=\int_{\mathcal{U}}p(\tilde{x}\mid \bm{\mu})p(\bm{\mu}\mid\mathcal{D})\mathrm{d}\bm{\mu}=\mathrm{Cat}\left(\tilde{x}\mid\mathrm{E}[\bm{\mu}\mid\mathcal{D}]\right)$$于是基于对假设空间再次赋概，我们对上帝有了新的认识 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{\bm{x}}$的次数是$\bm{s}$次的概率$$\begin{align}p(\bm{s}\mid \mathcal{D},m)&amp;=\int_{\mathcal{U}}\mathrm{Mu}(\bm{s}\mid \bm{\mu},m)\mathrm{Dir}(\bm{\mu}\mid \bm{a})\mathrm{d}\bm{\mu}\\&amp;=\frac{n!}{s_1!…s_c!}\frac{\mathrm{B}(\bm{s}+\bm{a})}{\mathrm{B}(\bm{a})}\end{align}$$这样我们对 $\displaystyle \bm{s}$的可能值也赋概了。这个式子称之为狄利克雷-多项式分布。我们对上帝又有了新的认识 6、至此，由于 $\displaystyle p(\bm{\mu}\mid\mathcal{D}_t,\mathcal{D}_{t+1})=\mathrm{Dir}(\bm{\mu}\mid \bm{k}_{t+1}+\bm{k}_t+\bm{a})$。当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。因为$$\mathrm{cov}(\bm{\mu})\approx \frac{(n)\mathrm{diag}[\bm{k}]-\bm{k}\bm{k}^\text{T}}{nnn}=\left[\frac{\mu_i^{MLE}(1-\mu_i^{MLE})^{\mathbb{I}(i=j)}{\mu_j^{MLE}}^{\mathbb{I}(i\neq j)}}{n}\right]_{c\times c} $$ 它随着我们数据的增加以 $\displaystyle \frac{1}{n}$的速度下降，我们发现人类的认识 $\displaystyle \hat{\bm{\mu}}$会越来越逼近上帝的那个 $\displaystyle \bm{\mu}$概率。也就是说$$\begin{align}p(\hat{\bm{\mu}}\to \bm{\mu})\to1\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-08-狄利克雷-多项式模型/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>分类分布</tag>
        <tag>狄利克雷-多项式模型</tag>
        <tag>狄利克雷分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝塔-伯努利模型(Beta-Binomial Model)]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-07-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B003%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝塔-伯努利模型(beta-binomial model)伯努利分布： $$\begin{align}x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1]\end{align}$$ 1、0-1分布(bernoulli distribution)概率质量函数可以表示为：$$\begin{align}\mathrm{PMF}：\mathrm{Ber}(x\mid \mu)=\mu^x(1-\mu)^{1-x}\end{align}$$其他表示方法：$\displaystyle \mathrm{Ber}(x\mid \mu)=\mu^{\mathbb{I}(x=1)}(1-\mu)^{\mathbb{I}(x=0)} $ $\displaystyle \mathrm{Ber}(x\mid \mu)=\begin{cases}\mu&amp;\text{if }x=1\\\nu=1-\mu&amp;\text{if }x=0\end{cases} $ 2、均值与方差：知道 $\displaystyle \varphi_x(t)=\mu\mathrm{e}^{\mathrm{i}t}+\nu$ $\displaystyle \mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\mu$ $\displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\mu(1-\mu)=\mu\nu$ 3、似然函数(likelihood)有数据集： $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$于是有似然函数 $$\begin{align}\mathrm{L}(\mu)=p(\mathcal{D}\mid\mu)=p(\boldsymbol{X}\mid\mu)=\prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 其中 $\displaystyle N_1=\sum_{i=1}^{n}\mathbb{I}(x_i=1)$， $\displaystyle N_0=\sum_{i=1}^{n}\mathbb{I}(x_i=0)$ 4、对数似然函数$$\begin{align}\mathcal{L}(\mu)&amp;=\ln p(\mathcal{D}\mid\mu)=\ln \prod_{i=1}^{n}\mu^{x_i}(1-\mu)^{1-x_i}=\sum_{i=1}^{n}\left(x_i\ln\mu+(1-x_i)\ln(1-\mu)\right)\\&amp;=\boldsymbol{I}^\text{T}\boldsymbol{x}\ln\mu+\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})\ln(1-\mu)\end{align}$$ 5、求极大似然估计：$$ \frac{\partial{\mathcal{L}}}{\partial{\mu}}=\frac{1}{\mu}\boldsymbol{I}^\text{T}\boldsymbol{x}-\frac{1}{1-\mu}\boldsymbol{I}^\text{T}(\boldsymbol{I}-\boldsymbol{x})=0\\\mu\boldsymbol{I}^\text{T}\boldsymbol{I}=\boldsymbol{I}^\text{T}\boldsymbol{x}$$$\begin{align}\mu_{MLE}=\frac{\boldsymbol{I}^\text{T}\boldsymbol{x}}{\boldsymbol{I}^\text{T}\boldsymbol{I}}=\frac{1}{n}\sum_{i=1}^{n}x_i=\bar{x}\end{align}$极大似然估计分析：我们知道 $\displaystyle x\in\{0,1\}$，在0-1分布的n次试验中, $\displaystyle x=1$的次数为 $\displaystyle k$。于是$$\begin{align}\mu_{MLE}=\frac{1}{n}\sum_{i=1}^{n}=\frac{k}{n}\end{align}$$现在我们假设抛硬币3次，3次都是正面朝上，那么 $\displaystyle n=k=3$，那么 $\displaystyle \mu_{MLE}=1$。这种情况下，极大似然的结果预测所有未来的观察值都是正面向上。常识告诉我们这是不合理的。事实上，这就是极大似然中过拟合现象的一个极端例子。下面我们引入 $\displaystyle \mu$的先验分布，我们会得到一个更合理的结论。 6、二项式分布(binomial distribution)我们现在把 $\displaystyle k$作为随机变量 $\displaystyle k=x_1+x_2+…+x_n$，于是我们有 $\displaystyle k\in\{N_1\}$，易知二项式分布和0-1分布的似然函数有相同的形式，是正比关系。最大化它们的 $\displaystyle \mu$都是 $\displaystyle \frac{k}{n}$。 $$\begin{align}\mathrm{L}(\mu)\propto\mathrm{Bin}(k\mid \mu,n)=\mathrm{C}_{n}^k\,\mu^k(1-\mu)^{n-k}\end{align}$$ 为了与后面符号衔接：我们令 $\displaystyle k=N_1,n=N,N_0=n-k$于是又有：$$\begin{align}p(\mathcal{D}\mid\mu)=\mathrm{L}(\mu)\propto\mathrm{Bin}(N_1\mid \mu,N_1+N_0)=\mathrm{C}_{N}^{N_1}\,\mu^{N_1}(1-\mu)^{N_0}\end{align}$$ 我们知道 $\displaystyle \mathrm{C}_{n}^{k}=\frac{n!}{(n-k)!k!}$。同时我们有离散随机变量特征函数$\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x_i}]=\sum_{i=1}^{\infty}p_i\mathrm{e}^{\mathrm{i}t x_i}$，注意虚数 $\displaystyle \mathrm{i}$和变量$\displaystyle i$的区别。我们令 $\displaystyle \nu=1-\mu$于是有：$$\begin{align}\varphi_k(t)=(\mu\mathrm{e}^{\mathrm{i}t}+\nu)^n\end{align} $$$\displaystyle \mathrm{E}[k]=\sum_{k=0}^{n}k\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=n\mu$$\displaystyle \mathrm{var}[k]=\sum_{k=0}^{n}(k-\mathrm{E}[k])^2\mathrm{Bin}(k\mid \mu,n)=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[k]=n\mu(1-\mu)=n\mu\nu$ 7、共轭先验分布如果先验和似然函数有相同形式，那就非常方便：这样后验也有相同形式。这个时候我们称之为共轭先验(conjugate prior)。现在，我们把 $\displaystyle \mu$做为一个变量观察似然函数得到$$\begin{align}p(\mu)\propto\mu^{\rho_1}(1-\mu)^{\rho_2}\end{align}$$这样计算后验就很容易了： $\displaystyle p(\mu\mid\mathcal{D})\propto p(\mathcal{D}\mid \mu)p(\mu)=\mu^{N_1}(1-\mu)^{N_0}\mu^{\rho_1}(1-\mu)^{\rho_2}=\mu^{N_1+\rho_1}(1-\mu)^{N_0+\rho_2}$ 在给出先验分布之前，我们看看这个两个函数。之后我们会补充 $\displaystyle \Gamma(x),\mathrm{B}(a,b)$函数的性质的证明。$\displaystyle \Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u, x&gt;0$$\displaystyle \mathrm{B}(a,b)=\int_{0}^{1}x^{a-1}(1-x)^{b-1}\mathrm{d}x,a&gt;0,b&gt;0$$\displaystyle \Gamma(x+1)=x\Gamma(x)$$\displaystyle \Gamma(n+1)=n!$$\displaystyle \mathrm{B}(a,b)=\mathrm{B}(b,a)$$\displaystyle \mathrm{B}(a,b)=\frac{b-1}{a+b-1}\mathrm{B}(a,b-1),a&gt;0,b&gt;1$ $\displaystyle \mathrm{B}(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ 然后我们知道贝塔分布： $\displaystyle \mathrm{Beta}(\mu\mid a,b)=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $。这正是我们要的共轭先验。 $$\begin{align}\mathrm{Beta}(\mu\mid a,b)=\frac{1}{\mathrm{B}(a,b)}\mu^{a-1}(1-\mu)^{b-1}\end{align}$$利用 $\displaystyle \Gamma$函数性质易知：$\displaystyle \mathrm{E}[\mu]=\frac{a}{a+b} $$\displaystyle \mathrm{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}$$\displaystyle \mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,\mathrm{Beta}(\mu\mid a,b)=\frac{a-1}{a+b-2},a&gt;0,b&gt;0$ 8、后验分布我们用似然函数乘以贝塔先验得到后验分布：$$\begin{align}p(\mu\mid\mathcal{D})\propto\mathrm{Bin}(N_1\mid\mu, N_1+N_0)\mathrm{Beta}(\mu \mid a,b)\end{align}$$ 归一化得：$$\begin{align}p(\mu\mid\mathcal{D})=\mathrm{Beta}(\mu\mid N_1+a,N_0+b)\end{align}$$ 1、在线学习容易证明 $\displaystyle \mathrm{Beta}$分布具有再生性质。于是我们可以假设我们有两个数据集 $\displaystyle \mathcal{D}_1,\mathcal{D}_2$。于是有：1、当我们观察到 $\displaystyle \mathrm{D}_1$时，有$$p(\mu\mid\mathcal{D}_1)=\mathrm{Beta}(\mu\mid N_1^1+a,N_0^1+b)$$2、当我们观察到 $\displaystyle \mathcal{D}_2$时，有$$p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)\propto p(\mathcal{D}_2\mid\mu) p(\mu\mid\mathcal{D}_1)$$易得：$$\begin{align}p(\mu\mid\mathcal{D}_1,\mathcal{D}_2)=\mathrm{Beta}(\mu\mid N_1^1+N_1^2+a,N_0^1+N_0^2+b)\end{align}$$这表明 该贝叶斯推断具有在线学习的良好性质。 2、后验均值与众数最大后验估计：$$\begin{align}\mu_{MAP}=\mathrm{mode}[\mu]=\mathop{\mathrm{argmax}}_{\mu}\,p(\mu\mid\mathcal{D})=\frac{N_1+a-1}{N+a+b-2}=\frac{k+a-1}{n+a+b-2}\end{align}$$极大似然估计： $$\begin{align} \mu_{MLE}=\frac{N_1}{N}=\frac{k}{n} \end{align}$$ 后验均值: $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{N_1+a}{N+a+b}=\frac{k+a}{n+a+b}\end{align}$$我们发现众数和均值不同。我们还发现 后验均值是先验均值和最大似然估计的凸组合。下面我们下证明这一点。 令先验均值$\displaystyle \frac{a}{a+b}=\mu_0,a+b=c$。 $$\begin{align}\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+a}{n+a+b}=\frac{c}{n+c}\mu_0+\frac{n}{n+c}\mu_{MLE}=\lambda\mu_0+(1-\lambda)\mu_{MLE}\end{align}$$ 我们发现 $\displaystyle c$可以理解为先验对于后验的等价样本大小。 $\displaystyle \lambda=\frac{c}{n+c}$是先验对于后验的等价样本大小比率。 同理可知最大后验估计是先验众数和最大似然估计的凸组合，而且更加倾向于最大似然估计：$$\begin{align}\mu_{MAP}=\frac{c-2}{n+c-2}\mathrm{mode}[\mu_0]+\frac{n}{n+c-2}\mu_{MLE}=\eta\mathrm{mode}[\mu_0]+(1-\eta)\mu_{MLE}\end{align}$$ 3、后验方差$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]=\frac{(N_1+a)(N_0+b)}{(N_1+a+N_0+b)^2(N_1+a+N_0+b+1)}\end{align}$$当 $\displaystyle N$足够大时：$$\begin{align}\mathrm{var}(\mu\mid\mathcal{D}]\approx\frac{N_1N_0}{NNN}=\frac{\mu_{MLE}(1-\mu_{MLE})}{n}\end{align}$$后验标准差 $$\begin{align}\sigma=\sqrt{\frac{\mu_{MLE}(1-\mu_{MLE})}{n}}\end{align}$$1、它随着我们数据的增加以 $\displaystyle \sqrt{\frac{1}{n}}$的速度下降。2、 $\displaystyle \mu=\mathop{\mathrm{argmax}}_{\mu}\sigma(\mu)=0.5$3、 $\displaystyle \mu=\mathop{\mathrm{argmin}}_{\mu}\sigma(\mu)=0\,or\,1$ 4、后验预测分布(posterior predictive distribution)上述，我们讨论了未知参数的推断，现在我们来讨论预测问题。在这之前：我们令后验分布 $\displaystyle p(\mu\mid\mathcal{D})=\mathrm{Beta}(a,b)$以简化符号 。考虑 $\displaystyle x_{n+1}$发生了，那么我们想预测 $\displaystyle x_{n+1}=0\,or\,1$的概率。为了简记，令 $\displaystyle \tilde{x}$为一随机变量，代表 $\displaystyle n$次观测后的值。同时考虑到 $\displaystyle \tilde{x}\in\{0,1\}$于是有：$$\begin{align}p(\tilde{x}\mid \mathcal{D})&amp;=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D})\mathrm{d}\mu\\&amp;=\int_0^1\mu^\tilde{x}(1-\mu)^{1-\tilde{x}}\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\int_0^1\mu^{a+\tilde{x}-1}(1-\mu)^{b+1-\tilde{x}-1}\mathrm{d}\mu\\&amp;=\frac{\Gamma(a+b)}{\Gamma{a}\Gamma(b)}\frac{\Gamma(a+\tilde{x})\Gamma(b+1-\tilde{x})}{\Gamma(a+b+1)}\\&amp;=\frac{a^{\tilde{x}}b^{1-\tilde{x}}}{(a+b)^{\tilde{x}}(a+b)^{1-\tilde{x}}}\\&amp;=\left(\frac{a}{a+b}\right)^{\tilde{x}}\left(\frac{b}{a+b}\right)^{1-\tilde{x}}\\&amp;=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}]\right)\end{align}$$ 所以可以看到，后验预测分布等价于plug-in$\displaystyle \mathrm{E}[\mu\mid\mathcal{D}]$。这里的plug-in x是插值近似 plug-in approximation的意思。 5、过拟合与黑天鹅悖论如果我们使用 $\displaystyle plug-in\,\mu_{MLE}$：$$\begin{align}p(\tilde{x}\mid \mathcal{D})\approx\mathrm{Ber}\left(\tilde{x}\mid\mu_{MLE}\right)\end{align}$$1、当数据集较小，例如 $\displaystyle n=3,k=0,then,\mu_{MLE}=\frac{0}{3}=0$。这样我们预测 $\displaystyle \tilde{x}=0$的概率为 $\displaystyle 0$。这叫零数问题(zero count problem)，或者叫数据匮乏问题(sparse data problem)。这种问题在小样本情况，经常发生。当然有人就要说了，现在是大数据时代，干嘛关注这个问题？如果我们基于特定的目的，划分数据：例如特定的人做特别的事——个人购物推荐。这种情况下即使是在大数据时代，数据也不会很大。所以 $$即使在大数据当道的时代，贝叶斯方法依然是有用的！[Jordan2011]$$2、零数问题类似于黑天鹅事情。3、我们来用贝叶斯方法来解决这个问题： 使用均匀分布先验 $\displaystyle \mathrm{Beat}(1,1)=\mathrm{U}[0,1]$，plug-$\displaystyle in\,\mathrm{E}[\mu\mid\mathcal{D}]$给出了拉普拉斯继承规则(laplace’s rule of succession):$$\begin{align}p(\tilde{x}=1\mid\mathcal{D})=\mathrm{E}[\mu\mid\mathcal{D}]=\frac{k+1}{n+2}\end{align}$$可以理解为：在实践中，通常在数据集中加一应该是正确的做法。称之为加一平滑(add-one smoothing)4、注意到：plug-in$\displaystyle \mu_{MAP}$没有这个性质。 6、多试验预测考虑这个问题，我们又观察到了 $\displaystyle m$个数据，那么发生 $\displaystyle \tilde{x}=1$的次数是 $\displaystyle s$次的概率。$$\begin{align}p(s\mid\mathcal{D},m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid a.b)\mathrm{d}\mu\\&amp;=\frac{\mathrm{C}_m^s}{\mathrm{B}(a.b)}\int_0^1\mu^{s+a-1}(1-\mu)^{m-s+b-1}\mathrm{d}\mu\\&amp;=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}\end{align}$$我们称 $\displaystyle \mathrm{Bb}(s\mid a,b,m)=\mathrm{C}_m^s\frac{\mathrm{B}(s+a,m-s+b)}{\mathrm{B}(a.b)}$为贝塔-二项式分布(beta-binomial distribution)。易知(使用积分表达式,同时利用 $\displaystyle \mathrm{Bin}(s\mid \mu,m)$容易求得)：$\displaystyle \mathrm{E}[s\mid\mathcal{D},m]=m\frac{a}{a+b}$ $\displaystyle \mathrm{var}[s\mid\mathcal{D},m]=\frac{mab}{(a+b)^2}\frac{a+b+m}{a+b+1}$ $\displaystyle \mathrm{var}[s\mid\mu_{MAP}]=\frac{m(a-1)(b-1)}{(a+b-2)^2}$(插值求得的)于是有：$$\begin{align} \mathrm{var}[s\mid\mathcal{D},m]\geqslant\mathrm{var}[s\mid\mu_{MAP}]\end{align}$$至于证明，可以分析这个式子： $\displaystyle f(a,b)=ab \left( a+b-2 \right) ^{2} \left( a+b+m \right) - \left( a-1 \right) \left( b-1 \right) \left( a+b \right) ^{2} \left( a+b+1 \right) \geqslant 0,(a&gt;0,b&gt;0)$可以使用微积分一阶导数，二阶导数证明。所以 贝叶斯预测的概率密度更宽、拥有长尾，从而避免了过拟合和黑天鹅悖论$$ 9、评述通过贝塔-伯努利模型，我们熟悉了贝叶斯方法的基本概念、流程、特点。一旦我们熟悉了伯努利、二项、贝塔、均匀、贝塔-伯努利分布后，很多关键的东西就可以用文字表述清楚了。 1、上帝给 $\displaystyle x$选择了一个分布：$\displaystyle x\sim \mathrm{Ber}(\mu), x\in\{0,1\},\mu\in[0,1] =\mu^x(1-\mu)^{1-x} $ 这个分布由 $\displaystyle \mu$确定。 2、犹豫种种原因，人类也知晓了 $\displaystyle x$的分布类型，但是不知道 $\displaystyle \mu$。于是人类搞了假设空间 $\displaystyle \mathcal{H}=\{\mu_i\}$，为了找到上帝的那个 $\displaystyle \hat{\mu}$，于是人类的征程开始了。 3、人类的智者，选择了用概率来解决这个难题。因为我们可以根据我们在某个时刻，不断知晓的信息流 $\displaystyle \mathcal{D}_t,\mathcal{D}_{t+1}$，来给假设空间的每个 $\displaystyle \mu$更新概率。于是这个表达式横空出世 $\displaystyle p(\mu\mid \mathcal{D}_{t+1},\mathcal{D}_t)\propto p(\mathcal{D}_{t+1}\mid\mu)p(\mu\mid \mathcal{D}_{t})$。于是我们得到了：$$p(\mu\mid\mathcal{D}_t)=\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b) $$这样我们就把假设空间的 $\displaystyle \mu$的都给了个概率。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 4、上帝微微一笑， 人类继续开始征程：令 $\tilde{x}$为一随机变量，代表 n次观测后的值，于是得到：$$ p(\tilde{x}\mid \mathcal{D}_t)=\int_0^1p(x\mid \mu)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu=\mathrm{Ber}\left(\tilde{x}\mid\mathrm{E}[\mu\mid\mathcal{D}_t]\right)$$于是基于对假设空间赋概，我们对未来的可能值也赋概。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 5、上帝开始感觉人类是个聪明的物种，甚为满意！ 我们又观察到了 m个数据，那么发生 $\tilde{x}=1$的次数是$s$次的概率$$\begin{align}p(s\mid\mathcal{D}_t,m)&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)p(\mu\mid\mathcal{D}_t)\mathrm{d}\mu\\&amp;=\int_0^1\mathrm{Bin}(s\mid\mu,m)\mathrm{Beta}(\mu\mid N_1^t+a,N_0^t+b)\mathrm{d}\mu\\&amp;=\mathrm{Bb}(s\mid N_1^t+a,N_0^t+b,m)\end{align}$$这样我们对 $\displaystyle s$的可能值也赋概了。至于人类相信那一个。贝叶斯方法说，基于不同的目的，我们要重新开发一门学问，叫决策论。 6、至此，当 $\displaystyle \lim_{t\to\infty}\mathcal{D}_t$人类基本可以看到上帝裸体了。 7、靠那个贝叶斯是谁，怎么总要出来嘟囔一下。决策论是啥？被冷落的贝叶斯微微一笑。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-07-机器学习笔记03/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大数据</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>大数据分析</tag>
        <tag>贝塔-伯努利模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯统计学概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-03-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B002%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、贝叶斯统计学框架经典统计学利用总计和样本信息来做统计分析，而贝叶斯统计学还加入了先验信息。下面我们用单参数一维随机变量加以说明： 1、记号以一维随机变量为例：频率学派中，依赖参数的概率密度(质量)函数表示为 $\displaystyle p_\beta(x) $或者 $\displaystyle p(x\,;\beta) $。表示在参数空间 $\displaystyle \mathcal{B}=\{\beta_i\} $中，不同 $\displaystyle \beta $对应不同密度概率(质量)函数。而在贝叶斯学派中，表示为 $\displaystyle p(x\mid \beta) $，代表了随机变量 $\displaystyle \beta $给定某个值时，总体 $x$的条件分布。而频率学派中不认为$\displaystyle \beta $是随机变量。它们认为上帝不玩骰子。 2、先验概率根据参数 $\displaystyle \beta $的先验信息确定先验分布$$\displaystyle p(\beta) $$ 3、样本的产生与似然函数贝叶斯观点认为：一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $产生要分两步：1、上帝从先验分布 $\displaystyle p(\beta) $中选了一个 $\displaystyle \beta_k $我们人类不知道，但是可以$设想$。2、从总体分布 $\displaystyle p(x\mid\beta_k) $产生一个样本 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $，这是具体的，我们人类能看到的。(按照频率学派观点，这里我们蕴含了 $\displaystyle x_i $是随机变量，且独立同分布的，贝叶斯认为这不需要，不过通常情况下，我们是使用IID，因为这样方便。)：$$\mathrm{L}(\beta_k)=p(\boldsymbol{x}\mid \beta_k)=\prod_{i=1}^{n}p(x_i\mid\beta_k) $$我们称 $\displaystyle \mathrm{L}(\beta_i) $为似然函数，它综合了总体信息和样本信息。 4、样本与参数的联合分布由于 $\displaystyle \beta_k $是上帝选的，我们人类$设想$的，它仍然是未知的，所以要把这个未知考虑进来，也是就 $\displaystyle \beta $的先验信息，对 $\displaystyle \beta $的一切可能加以考虑，而不仅仅是 $\displaystyle \beta_k $。这样我们人类就有了 $\displaystyle \boldsymbol{x} $和 $\displaystyle \beta $的联合分布：$$p(\boldsymbol{x},\beta)=p(\boldsymbol{x}\mid \beta)p(\beta) $$ 5、贝叶斯推断在没有样本信息时，人类只能根据先验分布 $\displaystyle p(\beta) $对 $\beta$做出推断。现在我们人类有了 $\displaystyle p(\boldsymbol{x},\beta) $，这样我们就可以做出新的推断了。1、先分解： $\displaystyle p(\boldsymbol{x},\beta)=p(\beta\mid\boldsymbol{x})p(\boldsymbol{x}) $。2、其中 $\displaystyle p(\boldsymbol{x})=\int_{\mathcal{B}}p(\boldsymbol{x},\beta)\mathrm{d}\beta=\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta$ ，它与$\beta$无关，或者说 $\displaystyle p(\boldsymbol{x}) $不含 $\displaystyle \beta $的任何信息。因此能用来推断的仅仅是条件分布：$$ p(\beta\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta)p(\beta)}{\displaystyle\int_{\mathcal{B}} p(\boldsymbol{x}\mid \beta)p(\beta) \mathrm{d}\beta} $$这就是贝叶斯公式的概率密度函数形式。 $\displaystyle p(\beta\mid\boldsymbol{x}) $史称后验分布，它集中了总体、样本、先验的一切信息，又排除了一切与 $\displaystyle \beta $无关的信息之后得到的结果。所以基于后验分布 $\displaystyle p(\beta\mid\boldsymbol{x}) $对 $\displaystyle \beta $进行统计推断是更为有效，也是最合理的。考虑离散情形：$$p(\beta_k\mid\boldsymbol{x})=\frac{p(\boldsymbol{x},\beta_k)}{p(\boldsymbol{x})}=\frac{p(\boldsymbol{x}\mid\beta_k)p(\beta_k)}{\displaystyle \sum_{\beta\in \mathcal{B}}p(\boldsymbol{x}\mid \beta)p(\beta)}$$ 自然语言表述的贝叶斯定理：$$\text{posterior}\propto\text{likeihood}\times\text{prior} $$或者说：$$p(\beta\mid\boldsymbol{x})\propto p(\boldsymbol{x}\mid \beta)p(\beta) $$ 6、贝叶斯统计分析的关键问题：1、确定先验分布： $\displaystyle p(\beta) $2、求联合分布： $\displaystyle p(\boldsymbol{x},\beta) $3、求后验分布： $\displaystyle p(\beta\mid \boldsymbol{x})$ 二、一个精彩的入门例子下面我们来通过一个入门的例子说明，贝叶斯定理是如何工作的。 [例子1.0] 为了提高相亲的成功率，小美考虑打扮一下自己，于是决定买一件羊绒大衣。预计要花费2000块。但是对相亲效果的影响，闺蜜们有2种意见： $\displaystyle \beta_1 $：相亲成功率提高到90%$\displaystyle \beta_2 $：相亲成功率提高到70% 小美当然希望 $\displaystyle \beta_1 $发生，有一个喜欢自己的男朋友，这笔花费还是值得的。根据一个好朋友的情况，先验概率：小美认为 $\displaystyle \beta_1 $的可信度只有40%， $\displaystyle \beta_2 $的可信度是60%。即： $$p(\beta_1)=0.4,\quad p(\beta_2)=0.6 $$小美不想花冤枉钱，于是她做了一个测试：把自己看中大衣，ps一下照片，给5个男性朋友看，结果：$$A: 5个男性朋友都认为小美更漂亮了 $$小美对测试很满意，于是她改变了看法，由二项分布知：$$ p(A\mid \beta_1)=0.9^5=0.590,\quad p(A\mid\beta_2)=0.7^5=0.168 $$由全概率公式 $\displaystyle p(A)=p(A\mid \beta_1)p(\beta_1)+p(A\mid \beta_2)p(\beta_2)=0.337 $。于是有后验概率 $$p(\beta_1\mid A)=\frac{p(A\mid \beta_1)p(\beta_1)}{p(A)}=0.7,\quad p(\beta_2)=\frac{p(A\mid \beta_2)p(\beta_2)}{p(A)}=0.3 $$这个概率综合了小美主观和实验的结果获得，要比小美之前认识的更有吸引力，更贴近实际。经过测试后，小美对买大衣有了兴趣，但是毕竟2000块还是很多的，于是小美再ps了一张图片，给她的男性朋友，结果如下：$$B: 10个男性朋友中，有9个都认为小美更漂亮了 $$$$ p(B\mid \beta_1)=C_{10}^{9}0.9^90.1=0.387,\quad p(B\mid\beta_2)=C_{10}^{9}0.7^90.3=0.121 $$由全概率公式 $\displaystyle p(B)=p(B\mid \beta_1)p(\beta_1)+p(B\mid \beta_2)p(\beta_2)=0.307 $。于是小美再次更新了自己的看法$$p(\beta_1\mid B)=\frac{p(B\mid \beta_1)p(\beta_1)}{p(B)}=0.883,\quad p(\beta_2)=\frac{p(B\mid \beta_2)p(\beta_2)}{p(B)}=0.117 $$小美经过两次测试，$\displaystyle \beta_1(相亲成功率提高到90\%) $的概率上升到了0.883，可以下决心买了。 三、共轭先验分布在叙述前，我们声明一下符号： $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X})=\frac{h(\boldsymbol{X},\boldsymbol{\beta})}{m(\boldsymbol{X})}=\frac{p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{\displaystyle\int_{\mathcal{B}}p(\boldsymbol{X}\mid\boldsymbol{\beta})\pi(\boldsymbol{\beta})\mathrm{d}\boldsymbol{\beta}} $ 1、共轭族定义设 $\displaystyle \boldsymbol{\beta} $是总体分布 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})$的参数向量， $\displaystyle \mathcal{F},\mathcal{P} $表示函数族。如果对任意的 $\displaystyle p(\boldsymbol{x}\mid\boldsymbol{\beta})\in\mathcal{F} $，存在先验分布函数$\displaystyle \pi(\boldsymbol{\beta})\in\mathcal{P} $，且 $\displaystyle \pi(\boldsymbol{\beta}\mid \boldsymbol{X}) \in \mathcal{P}$。就是说 $\displaystyle \mathcal{P} $是 $\displaystyle \mathcal{F} $的共轭族、称 $\displaystyle \pi(\boldsymbol{\beta}) $是共轭先验分布。 2、一维随机变量共轭先验分布例子1、方差已知下，一维高斯分布均值的先验分布是高斯分布。为了理解，我们先举一个简单的例子： $\displaystyle x\mid \mu\sim\mathcal{N}(x\mid \mu,\sigma) $，设 $\displaystyle \sigma $已知。有一组样本观测值 $\displaystyle \boldsymbol{x}=[x_1,x_2,…,x_n] $或者说有数据集 $\displaystyle \mathcal{D}=\{x_i\}_{i=1}^{n}$。 我们现在开始分析：1、样本似然函数 $\displaystyle p(\mathcal{D}\mid\mu)=p(\boldsymbol{x}\mid\mu)=\prod_{i=1}^{n}p(x_i\mid\mu)=(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2}(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})\right]$2、取 $\displaystyle \mu\sim\mathcal{N}(\mu\mid \bar{\mu},\delta) $为先验分布，其中 $\displaystyle \bar{\mu},\delta $是已知的。接下来我们将看到它是共轭的。3、于是有联合分布：$\displaystyle p(\mathcal{D},\mu)=p(\mathcal{D}\mid \mu)p(\mu)=\frac{1}{(2\pi)^{\frac{n+1}{2}}\sigma^{-\frac{n}{2}}\delta^{-1}}\exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] $4、应用自然语言的贝叶斯定理，我们有后验分布：$$\begin{align}p(\mu\mid\mathcal{D})=\propto&amp; p(\boldsymbol{x}\mid \mu)p(\mu)\propto \exp\left[-\frac{1}{2}[(\boldsymbol{x}-\mu\boldsymbol{I})^\text{T}\sigma^{-2}(\boldsymbol{x}-\mu\boldsymbol{I})+(\mu-\bar{\mu})^2\delta^{-2}]\right] \\\propto&amp;\exp\left[-\frac{1}{2}\left((\frac{n}{\sigma^2}+\frac{1}{\delta^2})\mu^2-2(\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2})\mu+\frac{\boldsymbol{x}^\text{T}\boldsymbol{x}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2}\right)\right]\\\propto &amp;\exp\left[-\frac{1}{2}(A\mu^2-2B\mu+C)\right]\\\propto &amp;\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right]\end{align}$$ 其中 $\displaystyle A=\frac{n}{\sigma^2}+\frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2},B=\frac{\boldsymbol{x}^\text{T}\boldsymbol{I}}{\sigma^2}+\frac{\bar{\mu}}{\delta^2} =\frac{\bar{x}}{\bar{\sigma}^2}+\frac{\bar{\mu}}{\delta^2}$ 在这里我们一般把 $\displaystyle \exp\left[\boldsymbol{w}^\text{T}\boldsymbol{g}(\boldsymbol{x})\right] $称为正态分布的核。于是$$\displaystyle p(\mu\mid\mathcal{D})=p(\mu\mid\boldsymbol{x})=\frac{1}{(2\pi A^{-1})^{\frac{1}{2}}}\exp\left[-\frac{1}{2}\frac{(\mu^2-B/A)^2}{A^{-1}}\right] $$也就是说后验分布是：$$\mu\mid\mathcal{D}\sim\mathcal{N}(\mu\mid \frac{B}{A},A^{-1}) $$ $\displaystyle \frac{B}{A}=\frac{\delta^2}{\bar{\sigma}^2+\delta^2}\bar{x}+\frac{\bar{\sigma}^2}{\bar{\sigma}^2+\delta^2}\bar{\mu}=\lambda\bar{x}+(1-\lambda)\bar{\mu} $$\displaystyle \frac{1}{\delta^2}=\frac{1}{\bar{\sigma}^2}+\frac{1}{\delta^2} $这就证明了：正态方差已知，它的均值的共轭先验分布是正态分布 3、若干技巧总结1、贝叶斯分析非常依赖于去求后验分布,如果按照定理，分母有一个积分，事实上它是一个数。于是我们经常应用$$\text{posterior}\propto\text{likelihood}\times\text{prior} $$这个式子分析，求得解后，在做归一化处理。就能得到posterior的表达式。2、在高斯分布下，我们经常需要配平方，以及观察随机变量的二次项(二次项的逆就是方差），一次项( 二次项的逆乘以一次项就是均值）。这是一个很重要的技巧。 四、充分统计量1、直观理解：就是不损失信息的统计量就是充分统计量。也就是说 $\displaystyle p_\beta(\boldsymbol{x}\mid T(\boldsymbol{x}))=p(\boldsymbol{x}\mid T(\boldsymbol{x})) $。 2、在这里我们只给出定理：设有样本 $\displaystyle \boldsymbol{x}=[x_1.x_2,…,x_n] $。样本密度 $\displaystyle p(x\mid \beta) $。有一个函数 $\displaystyle T: \boldsymbol{x}\mapsto \mathbb{R}$。 $\displaystyle t=T(\boldsymbol{x}) $它的密度为 $\displaystyle p(t\mid\beta) $。 $\displaystyle \mathcal{P}=\{\pi(\beta)\} $是$\beta$的某个先验分布族。如果对任意的 $\displaystyle \pi(\beta)\in\mathcal{P} $有 $$\displaystyle \pi(\beta\mid T(\boldsymbol{x}))=\pi(\beta\mid\boldsymbol{x}) $$这是 $\displaystyle T(\boldsymbol{x}) $是$\beta$的充分统计量的充要条件。 3、似然函数理解：$$\displaystyle \mathrm{L}(\beta)=p(\boldsymbol{x}\mid \beta)=h(\boldsymbol{x})g(T(\boldsymbol{x})\mid\beta)\propto g(T(\boldsymbol{x})\mid\beta) $$其中$h$与$\beta$无关，因此似然函数与$g(T(\boldsymbol{x})\mid\beta)$成比例，那么按照似然原理，有关$\beta$的推断可以有$T$给出。史称因子分解定理。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-03-06-机器学习笔记02/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大数据</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵高斯分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-11-%E7%9F%A9%E9%98%B5%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 Each day has enough trouble of its own.摘要：本文主要总结了矩阵高斯分布的若干基本问题，和我自己的一些体会。若有错误，请大家指正。关键词: 矩阵高斯分布,矩阵分布,统计学,概率论 一、标准矩阵高斯分布1、问题表述为了就研究数据集分布，我们将涉及：【矩阵分布问题】，当然矩阵分布是指的它所有元素的联合分布。 研究独立同分布的数据集 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^n$的分布，我们将其写成数据矩阵： $\displaystyle \bm{X}=\big[\bm{x}_1,\bm{x}_2 \cdots \bm{x}_n\big]^\text{T}$。其中 $\displaystyle \bm{x}\in\mathbb{R}^k$且它的元素是相互独立的一元标准高斯分布： $\displaystyle x_i\sim\mathcal{N}(0,1)$。于是有： $$\begin{align}\mathrm{vec}\big(\bm{X}^\text{T}\big)\sim \mathcal{N}\big(\mathrm{vec}\big(\bm{0}_{n\times k}^\text{T}\big),\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 特别的我们用矩阵简洁的表示为： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$其中1、$\displaystyle \mathrm{vec}\big(\bm{X}_{n\times k}^\text{T}\big)=\big[\bm{x}_1^\text{T},\bm{x}_2^\text{T} \cdots \bm{x}_n^\text{T}\big]^\text{T}$，即 $\displaystyle \bm{X}$转置以后，按列拉成向量。 2、张量积 $\displaystyle \bm{A}\otimes\bm{B}=\big[a_{ij}\bm{B}\big]$。于是 $\displaystyle \bm{\varSigma}_{nk\times nk}=\bm{E}_n\otimes \bm{E}_k$ 3、 $\displaystyle \mathrm{tr}\big(\bm{A}^\text{T}\bm{B}\big)=\mathrm{vec}\big(\bm{A}\big)^\text{T}\mathrm{vec}\big(\bm{B}\big)$。于是 $\displaystyle \sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i=\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)=\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)$4、 $\displaystyle \mathrm{cov}\big[\mathrm{vec}\big(\bm{X}^\text{T}\big)\big]=\bm{E}_n\otimes \bm{E}_k=\bm{E}_{nk}$ 我们来简要说明一下：$$\begin{align}p\bigg(\mathrm{vec}\big(\bm{X}^\text{T}\big)\bigg)&amp;=p(\mathcal{D})=\prod_{i=1}^np(\bm{x}_i)=\prod_{i=1}^n\prod_{j=1}^kp(x_{ij})\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\bm{x}_1 ^\text{T}\bm{x}_1^\text{T}+\cdots+\bm{x}_n ^\text{T}\bm{x}_n^\text{T}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\big(\sum_{i=1}^n\bm{x}_i ^\text{T}\bm{x}_i\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp \left[-\frac{1}{2}\mathrm{vec}\big(\bm{X}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{X}\big)\right]\\&amp;=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 到目前为止，遗留的问题是 $\displaystyle \bm{E}_n\otimes \bm{E}_k$这个参数做何理解。为何要写成克罗内克积的形式。 2、特征函数下面我们求上述矩阵分布的特征函数：我们定义：$\displaystyle \bm{T}=[\bm{t}_1,\bm{t}_2\cdots \bm{t}_n]^\text{T}$, 且知道 $\displaystyle \varphi_{\bm{x}_i}(\bm{t}_i)=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{E}_k\bm{t}_i\big]=\exp\big[-\frac{1}{2}\bm{t}_i ^\text{T}\bm{t}_i\big]$。由独立随机变量联合分布特征函数等于这些随机变量的特征函数之积，知道$$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\varphi_{\mathrm{vec}\big(\bm{X}^\text{T}\big)}\big(\bm{T}\big)&amp;=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t})=\prod_{i=1}^n \varphi_{\bm{x}_i}(\bm{t}_i)\\&amp;=\exp\big[-\frac{1}{2}\big(\bm{t}_1 ^\text{T}\bm{t}_1+\bm{t}_2 ^\text{T}\bm{t}_2+\cdots+\bm{t}_n ^\text{T}\bm{t}_n\big)\big]\\&amp;=\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{T}\big)\big]\end{align}$$ 二、一般矩阵高斯分布1、分布形式现在我们开始考虑更一般的问题： $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}$，且 $\displaystyle \bm{W}=\bm{A}\bm{A}^\text{T}\,,\bm{V}=\bm{B}\bm{B}^\text{T}$，有： $$\begin{align}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\sim\mathcal{N}\big(\mathrm{vec}\big(\bm{M}^\text{T}\big),\bm{W}\otimes \bm{V}\big)\end{align}$$ $$\begin{align}\bm{Y}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\end{align}$$ 2、矩阵分布的特征函数下面，我们用特征函数来证明这一点：$$\begin{align}\varphi_{\bm{Y}}\big(\bm{T}\big)&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{vec}\big(\bm{T}^\text{T}\big)^\text{T}\mathrm{vec}\big(\bm{Y}^\text{T}\big)\big]\bigg]\\&amp;=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}\bm{Y}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{Y}\bm{T}^\text{T}\big)\big]\bigg]=\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{Y}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{T}^\text{T}\bm{A}\bm{X}\bm{B}^\text{T}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\mathrm{E}\bigg[\exp\big[i\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{X}\big)\big]\bigg]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\big[\bm{A}^\text{T}\bm{T}\bm{B}\big]^\text{T}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{B}^\text{T}\bm{T}^\text{T}\bm{A}\bm{A}^\text{T}\bm{T}\bm{B}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{B}\bm{B}^\text{T}\big)\big]\\&amp;=\exp\bigg[\mathrm{i}\mathrm{tr}\big[\bm{T}^\text{T}\bm{M}\big]\bigg]\times\exp\big[-\frac{1}{2}\mathrm{tr}\big(\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big)\big]\\&amp;=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 也就是说矩阵分布： $\displaystyle \bm{X}\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)$的特征函数是 $$\begin{align}\varphi_{\bm{X}}\big(\bm{T}\big)=\exp\bigg[\mathrm{tr}\big[\mathrm{i}\bm{T}^\text{T}\bm{M}-\frac{1}{2}\bm{T}^\text{T}\bm{W}\bm{T}\bm{V}\big]\bigg]\end{align}$$ 3、一般矩阵高斯分布密度我们知道： $$\begin{align}\bm{X}\sim \mathcal{N}\big(\bm{0},\bm{E}_n\otimes \bm{E}_k\big)=\big(2\pi\big)^{-nk/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{X}^\text{T}\bm{X}\big)\right]\end{align}$$ 由 $\displaystyle \bm{Y}=\bm{M}+\bm{A}\bm{X}\bm{B}^\text{T}\to \bm{X}=\bm{A}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{B}^{-\text{T}}$、微分形式、变量代换定理有： $\displaystyle \frac{\partial \bm{X}}{\partial \bm{Y}}=\big|\,\bm{A}\,\big|^{-k}\big|\,\bm{B}^\text{T}\big|^{-n}=\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}$代入即可得到 $$\begin{align}p\big(\bm{Y}\big)=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\big)\right]\end{align}$$这样我们就得到了密度：$$\begin{align}\bm{X}&amp;\sim\mathcal{N}\big(\bm{M},\bm{W}\otimes \bm{V}\big)\\&amp;=\big(2\pi\big)^{-nk/2}\big|\,\bm{W}\,\big|^{-k/2}\big|\,\bm{V}^\text{T}\big|^{-n/2}\exp\left[-\frac{1}{2}\mathrm{tr}\big(\bm{W}^{-1}\big[\bm{Y}-\bm{M}\big]\bm{V}^{-1}\big[\bm{Y}-\bm{M}\big]^\text{T}\big)\right]\end{align}$$ 4、一般矩阵高斯分布性质1、$\displaystyle \bm{x}_{i,:}\sim \mathcal{N}(\bm{\mu}_i,w_{ii}\bm{V})$2、 $\displaystyle \bm{x}_{:,j}\sim\mathcal{N}(\bm{\mu}_j,v_{jj}\bm{W})$3、 $\displaystyle\mathrm{cov}[\bm{x}_{i,:},\bm{x}_{j,:}]=w_{ij}\bm{V}$4、 $\displaystyle\mathrm{cov}[\bm{x}_{:,i},\bm{x}_{:,j}]=v_{ij}\bm{W}$ 这个性质是显而易见的，然后如果你没发现“显然”，请仔细阅读上面的内容。要理解上述内容我们需要补充向量矩阵微分、微分形式、和变量代换定理。 三、评述充分熟悉矩阵微分、微分形式(外微分)、和变量代换定理是我们把握高维世界的基本工具。多加练习，容易掌握。矩阵微分大师：许宝騄。外微分大师：陈省生。可以读读他们的书。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-11-矩阵高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>统计学</tag>
        <tag>矩阵高斯分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布的熵]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-10-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E7%9A%84%E7%86%B5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、若干引理1、引理1.01、连续随机向量函数考虑一般情况，我们有随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$。现在有函数 $\displaystyle \bm{y}=\bm{g}(\bm{x}):\mathbb{R}^k\mapsto\mathbb{R}^d$。即有：$$\begin{align}\bm{y}=\bm{g}(\bm{x})\end{align}$$若上述方程有唯一解：$$\begin{align}\bm{x}=\bm{h}(\bm{y})\end{align}$$则称函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$是 $\displaystyle \bm{y}=\bm{g}(\bm{x})$的反函数。同时我们有雅可比行列式：$$\begin{align}\bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]\end{align}$$ 2、变量代换引理【定理1.0】对于连续随机向量 $\displaystyle \bm{x}\sim f(\bm{x})$，函数 $\displaystyle \bm{y}=\bm{g}(\bm{x})$满足下列条件：1、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$有唯一反函数 $\displaystyle \bm{x}=\bm{h}(\bm{y})$2、 $\displaystyle \bm{y}=\bm{g}(\bm{x})$和 $\displaystyle \bm{x}=\bm{h}(\bm{y})$连续3、 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]$存在而且连续那么$$\begin{align}f(\bm{y})=\left\{\begin{array}{l}f_\bm{x}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|&amp;\text{ 若 }\bm{y}\in G \\\ 0 &amp;\text{ 若 }\bm{y}\notin G \end{array}\right.\end{align}$$其中 $\displaystyle G=\{\bm{y}\mid \bm{y}=\bm{g(\bm{x})},\,\bm{x}\in \mathbb{R}^k\}$。 $\displaystyle \left|\,\bm{J}\right|$是雅可比行列式的绝对值，请勿与行列式符号混淆 证明： 为了区分随机变量与随机变量实例，我们定义 $\displaystyle \bm{\xi}$是随机向量，而 $\displaystyle \bm{x}$是随机向量$\displaystyle \bm{\xi}$的实例；$\displaystyle \bm{\eta}$是随机向量，而 $\displaystyle \bm{y}$是随机向量$\displaystyle \bm{\eta}$的实例当 $\displaystyle \bm{y}\notin G$时， 显然有 $\displaystyle f_\bm{\eta}(\bm{y})=0$。当 $\displaystyle \bm{y}\in G$时，有：$$\begin{align}F_ \bm{\eta}(\bm{y})=P(\bm{\eta}\leqslant \bm{y})=\int_A f_ \bm{\xi}(\bm{x})\mathrm{d}\bm{x}\end{align}$$其中： $\displaystyle A=\bigcap_{j=1}^{d}\{\bm{x}\mid g_j(\bm{x})\leqslant y_j\}$，在上式中换元： $\displaystyle \bm{x}=\bm{h}(\bm{y})$得：$$\begin{align}F_ \bm{\eta}(\bm{y})=\int_C\mathbb{I}_G(\bm{y})\times f_ \bm{\xi}\left[\bm{h}(\bm{y})\right]\cdot\left|\bm{J}\right|\mathrm{d}\bm{y}\end{align}$$其中 $\displaystyle C=\prod_{j=1}^{d}(-\infty,y_j]$， $\displaystyle \mathbb{I}_G(\bm{y})$是 $\displaystyle G$的示性函数。由此当 $\displaystyle \bm{y}\in G$时：$$\begin{align}f_ \bm{\eta}(\bm{y})=f_\bm{\xi}\left[\bm{h}\left(\bm{y}\right)\right]\times\left|\,\bm{J}\right|\end{align}$$证毕。其中证明中最关键的地方在于： $\displaystyle A\to C$的转变中，函数增减涉及积分方向的问题。这一问题的清晰说明较为繁琐，可以参考《数学分析原理》229页定理10.9以及微分形式的积分。 2、引理2.0定义方阵的幂(可以是分数) $\displaystyle \bm{A}^n=\bm{U}\bm{\Lambda}^n\bm{U}^\text{T}$。其中 $\displaystyle \bm{A}=\bm{U}\bm{\Lambda}\bm{U}^\text{T}$是约当分解或者叫谱分解，简单说就是对角化。 1、马哈拉诺比斯变换引理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{I}_k\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。当然我们也可以通过特征函数的方法对马哈拉诺比斯变换引理加以证明。 二、熵对于连续随机变量有： $\displaystyle \mathrm{H}[\bm{x}]=\mathrm{E}[\mathrm{I}(\bm{x})]=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}$下面我们推导多元高斯分布的熵：$$\begin{align}\mathrm{H}[\bm{x}]&amp;=-\int p(\bm{x})\ln p(\bm{x})\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x})\ln \left[(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\right]\mathrm{d}\bm{x}\\&amp;=-\int p(\bm{x}) \left[\ln \left((2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\right)-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{x}) \left[(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\mathrm{d}\bm{x}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\int p(\bm{y})\times\bm{y}^\text{T}\bm{y}\mathrm{d}\bm{y}\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{1}{2}\sum_{i=1}^k\mathrm{E}[y_i^2]\\&amp;=\ln \left((2\pi)^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right)+\frac{k}{2}\\&amp;=\ln \left[(2\pi\mathrm{e})^{\frac{k}{2}}\left|\bm{\varSigma}\right|^{\frac{1}{2}}\right]\\&amp;=\frac{k}{2}\left(\ln2\pi+1\right)+\frac{1}{2}\ln\left|\bm{\varSigma}\right|\end{align}$$ 注意：推导中我们使用了马哈拉诺比斯变换引理。 三、评述1、在求解多元高斯分布的熵中，我们使用了变量代换，同时引用了马哈拉诺比斯变换引理。2、深层次的原理涉及到微分形式的积分。同时我们也可以浅层次的理解：使用特征函数导出马哈拉诺比斯变换引理3、好了我们不应止步，我们征途是星辰大海。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-10-多元高斯分布的熵/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元高斯分布]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-09-%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 本文主要总结了多元高斯分布的若干基本问题，和我自己的一些体会。欢迎大家留言讨论，如有错误，请批评指正。 一、一元高斯分布$$\begin{align}x\sim\mathcal{N}(x\mid \mu,\sigma^2)=\left(2\pi\right)^{-1/2}(\sigma^2)^{-1/2}\exp\left[-\frac{1}{2}(x-\mu)^2\sigma^{-2}\right]\end{align}$$ 1、一元高斯分布特征函数我们有特征函数： $\displaystyle \varphi(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tx}p(x)\mathrm{d}x=\mathrm{e}^{i t\mu-\frac{1}{2}t^2\sigma^2}$。$$\begin{align}\varphi(t)=\exp \left[\mathrm{i} t\mu-\frac{1}{2}t^2\sigma^2\right]\end{align}$$下面我们来证明这一点： 令 $\displaystyle z=\frac {x-\mu}{\sigma}$,于是有 $\displaystyle z\sim\mathcal{N}\left(z\mid 0,1\right)=\left(2\pi\right)^{-1/2}\exp \left[-\frac{1}{2}z^2\right]$：$$\begin{align}\varphi_z(t)&amp;=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\end{align}$$ 1、我们知道虚数 $\displaystyle \mathbb{z}=\mathrm{e}^{\mathrm{i}\theta}=\cos(\theta)+\mathrm{i}\sin(\theta)$, 虚数的模 $\displaystyle \left| \mathbb{z}\right|=1$。令 $\displaystyle A(t)=\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}$于是有：$$\left|\frac{\partial{A}}{\partial{t}}\right|=\left|\mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right|=\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2} $$而且有：$$\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \left|\frac{\partial{A}}{\partial{t}}\right|\mathrm{d}z=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z&lt;\infty$$由于 $\displaystyle \int_{-\infty}^{+\infty}\left|z\right|\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z$收敛：故由含参反常积分一致收敛的可微性质知函数 $\displaystyle \left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \frac{\partial{A}}{\partial{t}}\mathrm{d}z$关于 $\displaystyle t\in (-\infty,+\infty)$上一致收敛。所以我们可以在 $\displaystyle \varphi_z(t)$ 的积分号下求导(交换积分与求导顺序) ：$$\varphi’_z(t)=\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} \mathrm{i}z\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\mathrm{d}z $$2、现在对上式进行分布积分： $\displaystyle\begin{cases} u=\mathrm{e}^{\mathrm{i}tz}\\v=-\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $，同时 $\displaystyle \begin{cases} u’=\mathrm{i}t\mathrm{e}^{\mathrm{i}tz}\\v’=z\mathrm{e}^{-\frac{1}{2}z^2}\end{cases} $ 于是有：$$\begin{align}\varphi’_z(t)&amp;=i\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty} u\mathrm{d}v\\&amp;= \left.-i\left(2\pi\right)^{-1/2}\mathrm{e}^{\mathrm{i}tz-\frac{1}{2}z^2}\right| _{x=0}-t\left(2\pi\right)^{-1/2}\int_{-\infty}^{+\infty}\mathrm{e}^{\mathrm{i}tz}\mathrm{e}^{-\frac{1}{2}z^2}\mathrm{d}z\\&amp;=-t\varphi_z(t)\end{align}$$ 得到微分方程：$$\begin{cases}\varphi’_z(t)+t\varphi_z(t)=0\\\varphi_z(0)=1\end{cases}$$解得：$$\begin{align}\varphi_z(t)=\mathrm{e}^{-\frac{1}{2}t^2}\end{align}$$又因为： $\displaystyle x=\mu+\sigma z$$$\begin{align}\varphi_x(t)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}tx}\right)=\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\mu+\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\mathrm{E}\left(\mathrm{e}^{\mathrm{i}t\sigma z}\right)=\mathrm{e}^{\mathrm{i}t\mu}\varphi_z(\sigma t)=\mathrm{e}^{\mathrm{i}t\mu-\frac{1}{2}t^2\sigma^2}\end{align}$$ 二、多元高斯分布1、多元高斯分布我们知道一维的特征函数为：$$\begin{align}\varphi(t)=\exp \left[\mathrm{i}t\mu -\frac{1}{2}t^2\sigma^2\right]\end{align}$$ 多元情况： $$\begin{align}\bm{x}\sim\mathcal{N}(\bm{x}\mid\bm{\mu},\bm{\varSigma})=(2\pi)^{-k/2}\left|\bm{\varSigma}\right|^{-1/2}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$ 2、马哈拉诺比斯变换定理我们有任意高斯分布 $\displaystyle \bm{x}\sim\mathcal{N}\left(\bm{\mu},\bm{\varSigma}\right)$。我们称 $\displaystyle \bm{y}=\bm{\varSigma}^{-\frac{1}{2}}\left[\bm{x}-\bm{\mu}\right]$为马哈拉诺比斯变换。其中$$\begin{align}\bm{y}\sim\mathcal{N}\left(\bm{0},\bm{E}_p\right)\end{align}$$也就是说 $\displaystyle y_i$是标准高斯分布 $\displaystyle \mathcal{N}\left(0,1\right)$。 证明：知道：$$\begin{align}p(\bm{x})=(2\pi)^{-\frac{k}{2}}\left|\bm{\varSigma}\right|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(\bm{x}-\bm{\mu})^\mathrm{T}\bm{\varSigma}^{-1}(\bm{x}-\bm{\mu}) \right]\end{align}$$同时有： $\displaystyle \bm{x}=\bm{\varSigma}^{\frac{1}{2}}\bm{y}+\bm{\mu}$。 $\displaystyle \bm{J}=\mathrm{det}\left[\frac{\partial \bm{x}}{\partial \bm{y}^\text{T}}\right]=\left|\bm{\varSigma}\right|^{\frac{1}{2}}$有变量代换定理有：$$\begin{align}p(\bm{y})=(2\pi)^{-\frac{k}{2}}\exp \left[-\frac{1}{2}\bm{y}^\text{T}\bm{y}\right]\end{align}$$证毕。 独立随机变量联合分布特征函数等于这些随机变量的特征函数之积。于是我们有$$\begin{align}\varphi_y(t)=\exp \left[-\frac{1}{2}t^2\right]\to\varphi_{\bm{y}}(\bm{t})=\prod_{i=1}^p\exp \left[-\frac{1}{2}t_i^2\right]=\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{t}\right]\end{align}$$ 3、多元高斯分布特征函数接着我们使用特征函数的线性变换性质有：$$\begin{align}\varphi_{\bm{x}}(\bm{t})&amp;=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{x}\right]\bigg]=\mathrm{E}\bigg[\exp \left[\mathrm{i}\bm{t}^\text{T}\left(\bm{\varSigma}^{1/2}\bm{y}+\bm{\mu}\right)\right]\bigg]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\varphi_{\bm{y}}\bigg(\big[\bm{\varSigma}^{1/2}\big]^\text{T}\bm{t}\bigg)=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}\right]\exp \left[-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}^{1/2}\bm{\varSigma}^{1/2}\bm{t}\right]\\&amp;=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]\end{align}$$ 三、多元高斯分布的性质1、高斯随机向量的任意边缘依然是高斯分布有随机向量 $\displaystyle \bm{x}$是 $\displaystyle k$维的，且 $\displaystyle \bm{x}\sim \mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$ 现在我们从 $\displaystyle \{x_i\}_{i=1}^k$中任意选取 $\displaystyle p$个元素，令 $\displaystyle s:p\mapsto k$，则向量 $\displaystyle \tilde{\bm{x}}=[x_{s_1},x_{s_2}\cdots x_{s_p}]^\text{T}$仍然是高斯分布：$$\begin{align}\tilde{\bm{x}}\sim\mathcal{N}\big(\tilde{\bm{\mu}},\tilde{\bm{\varSigma}}\big)\end{align}$$其中： $\displaystyle \tilde{\bm{\mu}}=[\mu_{s_1},\mu_{s_2}\cdots \mu_{s_p}]^\text{T}$， $\displaystyle \tilde{\bm{\varSigma}}=[c_{ij}],i,j\in\{s\mid s{p}\}$即保留 $\displaystyle \bm{\varSigma}$的第 $\displaystyle s_1,s_2\cdots s_p$行和列的 $\displaystyle p$阶矩阵。 证明：我们有 $\displaystyle \bm{x}$的特征函数 $\displaystyle \varphi_{\bm{x}}(\bm{t})=\exp \left[\mathrm{i}\bm{t}^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{\varSigma}\bm{t}\right]$,我们令 $\displaystyle t_i=0,i\in \{s\mid k \lnot s(p)\}$有：$$\begin{align}\varphi_{\tilde{\bm{x}}}(\tilde{\bm{t}})=\exp \left[\mathrm{i}\tilde{\bm{t}}^\text{T}\tilde{\bm{\mu}}-\frac{1}{2}\tilde{\bm{t}}^\text{T}\tilde{\bm{\varSigma}}\tilde{\bm{t}}\right]\end{align}$$故而得证。 2、独立性与相关性等价有$\displaystyle \bm{x}_1\sim \mathcal{N}\big(\bm{\mu}_1,\bm{\varSigma}_1\big)$和 $\displaystyle \bm{x}_2\sim \mathcal{N}\big(\bm{\mu}_2,\bm{\varSigma}_2\big)$，我们有$\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0$，且有$\displaystyle \bm{x}=[\bm{x}_1,\bm{x}_2]^\text{T}$服从： $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu}, \bm{\varSigma}\big)$。其中： $\displaystyle \bm{\mu}=[\bm{\mu}_1,\bm{\mu}_2]^\text{T},\bm{\varSigma}=\begin{bmatrix} \bm{\varSigma}_1 &amp; \bm{0}\\\bm{0}&amp;\bm{\varSigma}_2 \end{bmatrix}$。 证明利用1、特征函数的唯一性定理2、独立随机变量联合分布的特征函数是它们特征函数之积。证明是显然的。$$\begin{align}\displaystyle \bm{x}_1\bot \bm{x}_2\iff \mathrm{cov}\big[\bm{x}_1,\bm{x}_2\big]=0\end{align}$$ 3、仿射(线性)变换不变性有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$， 仿射变换 $\displaystyle \bm{y}=\bm{A}\bm{x}+\bm{b}$，则 $\displaystyle \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big) $ 证明：由特征函数的仿射变换性质有：$$\begin{align}\varphi_{\bm{y}}\big(\bm{t}\big)&amp;=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}\big]\varphi_{\bm{x}}\big(\bm{A}’\bm{t}\big)=\exp\big[\mathrm{i}\bm{t}^\text{T}\bm{b}+\mathrm{i}\big(\bm{A}^\text{T}\bm{t}\big)^\text{T}\bm{\mu}-\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\big]\\&amp;=\exp\bigg[\mathrm{i}\bm{t}^\text{T}\big[\bm{A}\bm{\mu}+\bm{b}\big]–\frac{1}{2}\bm{t}^\text{T}\bm{A}\bm{\varSigma}\bm{A}^\text{T}\bm{t}\bigg]\end{align}$$又由特征函数唯一性定理知： $$\begin{align} \bm{y}\sim\mathcal{N}\big(\bm{A}\bm{\mu}+\bm{b},\bm{A}\bm{\varSigma}\bm{A}^\text{T}\big)\end{align}$$【推论1】：分解：多元高斯分布随机向量都可以经过仿射变换为独立随机变量，且它们是标准高斯随机变量。【推论2】：降维：随机向量的线性组合是高斯分布 $\displaystyle \iff$随机向量服从高斯分布 推论2的性质颇为惊奇，我们来推导一下，以窥细节：证明：【充分性】若有一维高斯分布 ：$$\begin{align}y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)\end{align}$$知其特征函数为：$$\begin{align}\varphi_y(t)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$现在对其观察角度加以变换：令 $\displaystyle t=1$同时把 $\displaystyle \bm{a}$看成任意有：$$\begin{align}\varphi_{\bm{x}}\big(\bm{a}\big)=\mathrm{E}\big[\mathrm{i}\bm{a}^\text{T}\bm{x}\big]=\mathrm{E}\big[\mathrm{i}y\big]=\varphi_y\big(1\big)=\exp\big[\mathrm{i}\bm{a}^\text{T}\bm{x}-\frac{1}{2}\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$【必要性】若有 $\displaystyle \bm{x}\sim\mathcal{N}\big(\bm{\mu},\bm{\varSigma}\big)$，则 $\displaystyle y=\bm{a}^\text{T}\bm{x}$的特征函数为：$$\begin{align}\varphi_y(t)=\varphi_{\bm{x}}\big(t \bm{a}\big)=\exp\big[\mathrm{i}t\bm{a}^\text{T}\bm{x}-\frac{1}{2}t^2\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big]\end{align}$$由此可见 $\displaystyle y\sim\mathcal{N}\big(\bm{a}^\text{T}\bm{x},\bm{a}^\text{T}\bm{\varSigma}\bm{a}\big)$证毕 4、条件分布我们考虑如下分布 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_2\mid \bm{x}_1$依然是高斯分布： $$\begin{align} \bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 证明：1、为了利用独立性与相关性等价的结论，我们使用线性变换来构造两个独立的新随机变量： $$\begin{align}\bm{y}_1&amp;=\bm{x}_1\\\bm{y}_2&amp;=\bm{T}\bm{x}_1+\bm{x}_2\end{align}$$ 欲使 $\displaystyle \bm{y}_1\bot \bm{y}_2$，则：$$\begin{align}\mathrm{cov}\big[\bm{y}_1,\bm{y_2}\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_1- \mathrm{E}\big[\bm{y}_1\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\bm{x}_1+\bm{x}_2- \bm{T}\bm{\mu}_1- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_1- \bm{\mu}_1\bigg)\bigg(\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)+\bm{x}_2- \bm{\mu}_2\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_1- \bm{\mu}_1\big)^\text{T}\Big]\bm{T}^\text{T}+\mathrm{E}\Big[\big(\bm{x}_1- \bm{\mu}_1\big)\big(\bm{x}_2- \bm{\mu}_2\big)^\text{T}\Big]\\&amp;=\bm{\varSigma}_{11}\bm{T}^\text{T}+\bm{\varSigma}_{12}=\bm{0}\end{align}$$于是有：$$\begin{align}\bm{T}=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\end{align}$$也就是说：$$\begin{align}\bm{y}_2=-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1+\bm{x}_2\end{align}$$这个线性变换是：$$\begin{align}\begin{bmatrix} \bm{y}_1 \\\ \bm{y}_2 \end{bmatrix}=\begin{bmatrix} \bm{I} &amp; \bm{0}\\ -\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}&amp;\bm{I}\end{bmatrix}\begin{bmatrix} \bm{x}_1 \\\\\bm{x}_2\end{bmatrix}\end{align}$$2、根据数字特征的性质，我们求出 $\displaystyle \bm{y}_1,\bm{y}_2$的期望和方差，由高斯分布的性质知道经过线性变换后的它们也是服从高斯分布的，从而我们可以确定其分布。容易知道：$\displaystyle \mathrm{E}\big[\bm{y}_1\big]=\bm{\mu}_2$，$\displaystyle \mathrm{cov}\big[\bm{y}_1\big]=\bm{\varSigma}_{11}$，$\displaystyle \mathrm{E}\big[\bm{y}_2\big]=\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1$。下面关键是求：$$\begin{align} \mathrm{cov}\big[\bm{y}_2\big]&amp;=\mathrm{E}\Bigg[\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)\bigg(\bm{y}_2- \mathrm{E}\big[\bm{y}_2\big]\bigg)^\text{T}\Bigg]\\&amp;=\mathrm{E}\Bigg[\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)\bigg(\bm{x}_2- \bm{\mu}_2+\bm{T}\big(\bm{x}_1- \bm{\mu}_1\big)\bigg)^\text{T}\Bigg]\\&amp;=\bm{\varSigma}_{22}+\bm{\varSigma}_{21}\bm{T}^\text{T}+\bm{T}\bm{\varSigma}_{12}+\bm{T}\bm{\varSigma}_{11}\bm{T}^\text{T}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{11}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\\&amp;=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$ 于是有：$$\begin{align}\bm{y}_2\sim\mathcal{N}\bigg(\bm{\mu}_2-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\mu}_1,\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$ 3、并且我们有： $\displaystyle \bm{J}\big(\bm{y}\to \bm{x}\big)=\bigg|\frac{\partial \bm{y}}{\partial \bm{x}^\text{T}}\bigg|=1$，再有 $\displaystyle \bm{y}_1,\bm{y}_2$独立，于是：$$\begin{align}p\big(\bm{x}_1,\bm{x}_2\big)=p\big(\bm{y}_1,\bm{y}_2\big)\bm{J}\big(\bm{y}\to \bm{x}\big)=p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)\end{align}$$ 现在我们可以求得 $\displaystyle \bm{x}_2\mid \bm{x}_1$的分布：$$\begin{align}p\big(\bm{x}_2\mid \bm{x}_1\big)=\frac{p\big(\bm{x}_1,\bm{x}_2\big)}{p\big(\bm{x}_1\big)}=\frac{p\big(\bm{y}_1\big)p\big(\bm{y}_2\big)}{p\big(\bm{y}_1\big)}=p\big(\bm{y}_2\big)=p_{\bm{y}_2}\big(\bm{x}_2- \bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{x}_1\big)\end{align}$$代入到密度函数，经过简单变换有：$$\begin{align}\bm{x}_2\mid \bm{x}_1\sim\mathcal{N}\bigg(\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big],\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\bigg)\end{align}$$条件数学期望和协方差矩阵是：$$\begin{align}\bm{\mu}_{2\mid1}&amp;=\mathrm{E}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\mu}_2+\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\big[\bm{x}_1- \bm{\mu}_1\big]\\\bm{\varSigma}_{2\mid 1}&amp;=\mathrm{cov}\big[\bm{x}_2\mid \bm{x}_1\big]=\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\end{align}$$证毕 当然我们也可以通过概率归一约束，和配平方加以推导。 5、高斯线性模型若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，则有：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}^{-1}=\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}$ $\displaystyle \bm{\mu}_{\bm{x}\mid \bm{y}}=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 证明：要想彻底说明这个问题，我们需要分块矩阵的若干引理：首先我们来证明一个引理 1、【引理1】单位矩阵引理：$$\begin{align} [\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$证明：$$\begin{align}&amp;[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E} \\&amp;\iff\bm{S}[\bm{E}-\bm{S}]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\big[\bm{S}^{-1}-\bm{S}\bm{S}^{-1}\big]^{-1}-\bm{S}[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}\\&amp;\iff\bm{S}^{-1}[\bm{E}-\bm{S}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{S}^{-1}\bm{S}\\&amp;\iff[\bm{S}^{-1}-\bm{E}][\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\\&amp;\iff\bm{E}=\bm{E}\end{align}$$证毕 2、【引理2】分块矩阵的逆证明：若分块矩阵的逆矩阵存在，：$$\begin{align}\bm{H}^{-1}=\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}\end{align}$$ 1、 【$\displaystyle \bm{A}$若可以逆】首先我们左乘一个矩阵，消除 $\displaystyle \bm{B} $ $$\begin{align} \left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{C} &amp; \bm{D}\end{array}\right]\end{align}$$ 第二步我们右乘一个矩阵，消除 $\displaystyle \bm{C} $ $$\begin{align}\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E}\end{bmatrix}= \left[\begin{array}{cc}\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0} \\\bm{0} &amp; \bm{D}\end{array}\right]\end{align}$$ 简写$\displaystyle \bm{U}\bm{H}\bm{V}=\bm{W} $，于是 $\displaystyle\bm{H}^{-1}=\bm{V}\bm{W}^{-1}\bm{U} $（这是显然的） $$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}=\begin{bmatrix} \bm{E} &amp; \bm{0} \\-\bm{D}^{-1}\bm{C} &amp; \bm{E} \end{bmatrix}\left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} &amp; \bm{0} \\\bm{0} &amp; \bm{D}^{-1}\end{array}\right]\left[\begin{array}{cc}\bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{array}\right]\end{align}$$简化得：$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp;-[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}\bm{B}\bm{D}^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{M}&amp;- \bm{M} \bm{B}\bm{D}^{-1}\\-\bm{D}^{-1}\bm{C}\bm{M}&amp; \bm{D}^{-1}+\bm{D}^{-1}\bm{C}\bm{M}\bm{B}\bm{D}^{-1}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1}$。 2、【同理若 $\displaystyle \bm{B}$可逆】$$\begin{align}\left[\begin{array}{cc}\bm{A} &amp; \bm{B} \\\bm{C} &amp; \bm{D}\end{array}\right]^{-1}&amp;= \left[\begin{array}{cc}\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1}&amp;-\bm{B}\bm{D}^{-1}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\\-[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}^{-1}\bm{C}&amp; [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\end{array}\right]\\&amp;=\left[\begin{array}{cc}\bm{A}^{-1}+\bm{A}^{-1}\bm{B}\bm{M}\bm{C}\bm{A}^{-1}&amp;- \bm{B}\bm{D}^{-1}\bm{M}\\-\bm{M} \bm{D}^{-1}\bm{C}&amp;\bm{M}\end{array}\right]\end{align}$$ 其中 $\displaystyle \bm{M}= [\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}$ 3、【引理3】分块矩阵行列式：由引理2，消除 $\displaystyle \bm{B}$后的结论知道：$$\begin{align}\det \begin{bmatrix} \bm{E} &amp; -\bm{B}\bm{D}^{-1} \\\bm{0} &amp; \bm{E}\end{bmatrix}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det \begin{bmatrix} \bm{A}-\bm{B}\bm{D}^{-1}\bm{C} &amp; \bm{0}\\\bm{C}&amp;\bm{D} \end{bmatrix}=\det[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]\det[\bm{D}]\end{align}$$ 我们也可以这样（也就是说右乘矩阵消除 $\displaystyle \bm{B}$）： $$\begin{align}\det \begin{bmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D} \end{bmatrix}\det \begin{bmatrix} \bm{E} &amp; -\bm{A}^{-1}\bm{B} \\\bm{0} &amp; \bm{E}\end{bmatrix}=\det \begin{bmatrix} \bm{A} &amp; \bm{0}\\\bm{C}&amp;\bm{D}-\bm{C}\bm{A}^{-1}\bm{B} \end{bmatrix}=\det[\bm{A}]\det[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]\end{align}$$ 也就是说：$$\begin{align}\begin{vmatrix} \bm{A} &amp; \bm{B}\\\bm{C}&amp;\bm{D}\end{vmatrix}=\big|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}\big|\big|\bm{D}\big|=\big|\bm{A}\big|\big|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\big|\end{align}$$ 我们也有$$\begin{align} \left|\bm{A}-\bm{B}\bm{D}^{-1}\bm{C} \right|=\left|\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}\right|\left|\bm{D}^{-1}\right|\big|\bm{A}\big|\end{align}$$ 4、【引理4】维度变换$\displaystyle [\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}$ $\displaystyle \bm{M}=[\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} =\bm{A}^{-1}+ \bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1}\bm{C}\bm{A}^{-1} $ $\displaystyle [\bm{A}-\bm{B}\bm{D}^{-1}\bm{C}]^{-1} \bm{B}\bm{D}^{-1}=\bm{A}^{-1}\bm{B}[\bm{D}-\bm{C}\bm{A}^{-1}\bm{B}]^{-1} $证明：我们挑一个做说明,为了应用【引理1】我们右乘 $\displaystyle \bm{A}$于是有：$$\begin{align} &amp;[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\\ &amp;\iff[\bm{A}+\bm{B}\bm{C}\bm{D}]^{-1}\bm{A}=\bm{A}^{-1}- \bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\bm{A}^{-1}\bm{A}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-\bm{A}^{-1}\bm{B}[\bm{C}^{-1}+\bm{D}\bm{A}^{-1}\bm{B}]^{-1}\bm{D}\\ &amp;\iff[\bm{E}+\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}=\bm{E}-[\bm{D}^{-1}\bm{C}^{-1}\bm{B}^{-1}\bm{A}+\bm{E}]^{-1}\\ &amp;\iff\big[\bm{E}-[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]\big]^{-1}-\big[[-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}]^{-1}-\bm{E}\big]=\bm{E}\,,\bm{S}=-\bm{A}^{-1}\bm{B}\bm{C}\bm{D}\\ &amp;\iff[\bm{E}-\bm{S}]^{-1}-[\bm{S}^{-1}-\bm{E}]^{-1}=\bm{E}\end{align}$$当然我们也可以直接对比引理2的两个结论得出。证毕 5、【引理5】高斯线性回归模型的联合分布我们知道 $\displaystyle p(\bm{x},\bm{y})=p(\bm{x})p(\bm{y}\mid \bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，现在令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 我们取对数：$$\begin{align}&amp;\ln p(\bm{z})=\ln p(\bm{x})+\ln p(\bm{y}\mid \bm{x})\\&amp;=-\frac{1}{2}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)^\mathrm{T}\bm{\varSigma}_{\bm{x}}^{-1}\big(\bm{x}-\bm{\mu}_{\bm{x}}\big)-\frac{1}{2}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)^\mathrm{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-[\bm{A}\bm{x}+\bm{b}]\big)+\\&amp;\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1} \end{bmatrix}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=-\frac{1}{2}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)^\text{T}\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}^{-1}\Bigg(\begin{bmatrix} \bm{x} \\\ \bm{y} \end{bmatrix}-\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\Bigg)\\&amp;+\ln\bigg[(2\pi)^{-(k+s)/2}\big|\bm{\varSigma}_{\bm{x}}\big|^{-1/2}\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|^{-1/2}\bigg]\\&amp;=\ln\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中我们使用了一些二次型的技巧：简要说明一下，以免显得突兀,其中 $\displaystyle \bm{S}$是对称矩阵,有二次型：$$\begin{align}Q&amp;=\frac{1}{2}(\bm{x}-\bm{\mu})^\text{T}\bm{S}(\bm{x}-\bm{\mu})=\frac{1}{2}\bm{x}^\text{T}\bm{S}\bm{x}-\bm{x}^\text{T}\bm{S}\bm{\mu}+\frac{1}{2}\bm{\mu}^\text{T}\bm{S}\bm{\mu}\\&amp;=\frac{1}{2}\bm{x}^\text{T}\bm{A}\bm{x}-\bm{x}^\text{T}\bm{B}+\frac{1}{2}\bm{\mu}^\text{T}\bm{A}\bm{\mu}\end{align}$$其中：$$\begin{align}\bm{S}&amp;=\bm{A}\\\bm{\mu}&amp;=\bm{A}^{-1}\bm{B}\end{align}$$也就是说我们可以通过观测一次项、二次项的系数来求得参数。 考察二次项：$$\begin{align}&amp;-\frac{1}{2}\bm{x}^\text{T}[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}]\bm{x}-\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}+\frac{1}{2}\bm{y}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}\bm{x}+\frac{1}{2}\bm{x}^\text{T}\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{y}\\&amp;=-\frac{1}{2}\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{cc}\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\end{array}\right]\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]\end{align}$$考虑一次项$$\begin{align}\bm{x}^\text{T}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}=\left[\begin{array}{c}\bm{x}\\\bm{y}\end{array}\right]^\text{T}\left[\begin{array}{c}\bm{\varSigma}_{\bm{x}}^{-1}\bm{\mu}_{\bm{x}}-\bm{A}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\\\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{b}\end{array}\right]\end{align}$$这样再通过分块矩阵求逆，和二次型的特点可以求得：$$\begin{align}\bm{\mu}_{\bm{x}}&amp;=\begin{bmatrix} \bm{\mu}_{\bm{x}} \ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix}\\\bm{\varSigma}_{\bm{z}}&amp;=\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\end{align}$$同时我们通过分块矩阵行列式注意到：$$\begin{align}\big|\bm{\varSigma}_{\bm{z}}\big|=\big|\bm{\varSigma}_{\bm{x}}\big|\big|\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big|\end{align}$$以上就是联合分布是高斯分布的推导细节。 【高斯线性回归模型的联合分布】若： $\displaystyle p(\bm{x})=\mathcal{N}\big(\bm{\mu}_{\bm{x}},\bm{\varSigma}_{\bm{x}}\big)$，另外有： $\displaystyle p\big(\bm{y}\mid \bm{x}\big)=\mathcal{N}\big(\bm{A}\bm{x}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}\big)$，令： $\displaystyle \bm{z}=\begin{bmatrix} \bm{x} \\\bm{y} \end{bmatrix}$ 则：$$\begin{align}\bm{z}\sim\mathcal{N}\Bigg(\begin{bmatrix} \bm{\mu}_{\bm{x}} \\\ \bm{A}\bm{\mu}_{\bm{x}}+\bm{b} \end{bmatrix},\begin{bmatrix}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\\\bm{A}\bm{\varSigma}_{\bm{x}}&amp;\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T} \end{bmatrix}\Bigg)\end{align}$$其中：$\displaystyle \bm{\varLambda}_{\bm{z}}=\begin{bmatrix} \bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;-\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\\\ -\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1}\bm{A}&amp;\bm{\varSigma}_{\bm{y}\mid\bm{x}}^{-1} \end{bmatrix}$###### 6、【引理6】精度矩阵与协方差矩阵再看出显然之前我们有必要叙述一精度矩阵的概念: 精度矩阵是协方差矩阵的逆$$\begin{align}\bm{\varLambda}=\bm{\varSigma}^{-1}=\begin{bmatrix} \bm{\varLambda}_{11} &amp; \bm{\varLambda}_{12} \\\bm{\varLambda}_{21}&amp;\bm{\varLambda}_{22}\end{bmatrix}\end{align}$$若 $\displaystyle \bm{\varSigma}_{11},\bm{\varSigma}_{22}$可以逆，根据上述引理我们容易知道：$\displaystyle \bm{\varLambda}_{11}=\big[\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\big]^{-1}$$\displaystyle \bm{\varLambda}_{22}=\big[\bm{\varSigma}_{22}-\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}\bm{\varSigma}_{12}\big]^{-1}$$\displaystyle \bm{\varLambda}_{12}=-\bm{\varLambda}_{11}\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}$$\displaystyle \bm{\varLambda}_{21}=-\bm{\varLambda}_{22}\bm{\varSigma}_{21}\bm{\varSigma}_{11}^{-1}$有了精度矩阵，我们可以重写条件分布定理： 若 $\displaystyle \begin{bmatrix} \bm{x}_1 \ \bm{x}_2 \end{bmatrix}\sim\mathcal{N}\bigg(\begin{bmatrix} \bm{\mu}_1 \ \bm{\mu}_2 \end{bmatrix},\begin{bmatrix} \bm{\varSigma}_{11} &amp; \bm{\varSigma}_{12}\\\bm{\varSigma}_{21}&amp;\bm{\varSigma}_{22} \end{bmatrix}\bigg)$, 那么条件分布 $\displaystyle \bm{x}_1\mid \bm{x}_2$依然是高斯分布： $$\begin{align} \bm{x}_1\mid \bm{x}_2\sim\mathcal{N}\bigg(\bm{\mu}_1+\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big],\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}\bigg)\end{align}$$且有$$\begin{align}\bm{\mu}_{1\mid2}&amp;=\bm{\mu}_1+\bm{\varSigma}_{11}\bm{\varSigma}_{22}^{-1}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\mu}_1- \bm{\varLambda}_{11}^{-1}\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\\&amp;=\bm{\varLambda}_{11}^{-1}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\\&amp;=\bm{\varSigma}_{1\mid 2}\bigg[\bm{\varLambda}_{11}\bm{\mu}_1-\bm{\varLambda}_{12}\big[\bm{x}_2- \bm{\mu}_2\big]\bigg]\end{align}$$ $$\begin{align}\bm{\varSigma}_{1\mid 2}=\bm{\varSigma}_{11}-\bm{\varSigma}_{12}\bm{\varSigma}_{22}^{-1}\bm{\varSigma}_{21}=\bm{\varLambda}_{11}^{-1}\end{align}$$ 7、最后的战斗有了联合分布我们就可以用应用条件分布定理了：结论是显然的：$$\begin{align}p\big(\bm{x}\mid \bm{y}\big)=\mathcal{N}\big(\bm{\mu}_{\bm{x}\mid \bm{y}},\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big)\end{align}$$其中$\displaystyle \bm{\varSigma}_{\bm{x}\mid \bm{y}}=\big[\bm{\varSigma}_{\bm{x}}^{-1}+\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\bm{A}\big]^{-1}=\bm{\varLambda}_{xx}^{-1}$ $$\begin{align}\bm{\mu}_{\bm{x}\mid \bm{y}}&amp;=\bm{\varSigma}_{\bm{x}\mid\bm{y}}\bigg[\bm{\varLambda}_{\bm{x}\bm{x}}\bm{\mu}_{\bm{x}}-\bm{\varLambda}_{\bm{x}\bm{y}}\big[\bm{y}-\bm{A}\bm{\mu}_{x}-\bm{b}\big]\bigg]\\&amp;=\bm{\varSigma}_{\bm{x}\mid \bm{y}}\big[\bm{A}^\text{T}\bm{\varSigma}_{\bm{y}\mid \bm{x}}^{-1}\big(\bm{y}-\bm{b}\big)+\bm{\varSigma}_{\bm{x}}^{-1} \bm{\mu}_{\bm{x}}\big]\end{align}$$ 还有：$$\begin{align}p\big(\bm{y}\big)=\mathcal{N}\big(\bm{A}\bm{\mu}_{\bm{x}}+\bm{b},\bm{\varSigma}_{\bm{y}\mid \bm{x}}+\bm{A}\bm{\varSigma}_{\bm{x}}\bm{A}^\text{T}\big)\end{align}$$ 四、评述1、我们来总结一下：$$\begin{align}\begin{cases}\text{马哈拉诺比斯变换定理}\\\text{一元高斯分布特征函数}\\\text{特征函数性质}\end{cases}\Rightarrow\text{多元高斯分布特征函数}\end{align}$$2、当然也可以使用定义与变量代换定理一步到位。3、有了特征函数，我们就可以证明一系列关键定理4、我们讨论一下逆矩阵计算的问题，定义精度矩阵，重写了条件分布定理5、然后我们显然得出了高斯线性模型的结果。6、当然高斯模型还有非常多性质，我们将继续踏上征途。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-09-多元高斯分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习概论]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2017-01-08-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00001%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、机器学习若干符号解释我们在表达概念时，通常用集合论，空间之类的术。这个时候，我们注意元素和集合的区别。而在我们表达运算时，我们通常用矩阵的概念，这个时候你要注意维度、列、行的概念。多加练习，你很快就会掌握这个表达。 1、输入空间用矩阵表示数据集$\mathcal{D} : \boldsymbol{X}=\left[\begin{array}{c}\boldsymbol{x}_{1}^\text{T}\\\boldsymbol{x}_{2}^\text{T}\\\vdots\\\boldsymbol{x}_{n}^\text{T}\end{array}\right]=\left[\begin{array}{c}\boldsymbol{x}_{1,:}^\text{T}\\\boldsymbol{x}_{2,:}^\text{T}\\\vdots\\\boldsymbol{x}_{n,:}^\text{T}\end{array}\right]=\left[\begin{array}{ccc}x_{1,1} &amp; \dots &amp; x_{1,k} \\\vdots &amp; \dots &amp; \vdots \\x_{n,1} &amp; \dots &amp; x_{n,k}\end{array}\right]$。$\displaystyle \boldsymbol{x}_i=\boldsymbol{x}_{i,:} $表示用 $\displaystyle \boldsymbol{X} $的第i行转置构造向量 $\displaystyle \underbrace {\boldsymbol{x}_i}_{k\times1} $用matlab举个例子： 1234567891011121314%矩阵与机器学习&gt;&gt; X=[12, 14, 15; 1.5, 5.4 ,6.7;20,3.4,5]X = 12.0000 14.0000 15.0000 1.5000 5.4000 6.7000 20.0000 3.4000 5.0000&gt;&gt; x1=X(1,:)'x1 = 12 14 15&gt;&gt; x1(2,1)ans = 14 所以 $\displaystyle \boldsymbol{x_1} $表示包含k个维度的一次观测(示例)。我们用集合论的方式再叙述一遍有n个样本的数据集或者样本空间 $\displaystyle X=\{\boldsymbol{x}_1,\,\boldsymbol{x}_2,\, …,\,\boldsymbol{x}_i,\,…,\,\boldsymbol{x}_n\} \subseteq \mathcal{X}^n$ ,其中 $\displaystyle \boldsymbol{x}_i $是样本点(样本)，我们把输入的所有可能取值集合 $\displaystyle \mathcal{X} $叫做输入空间,无监督学习下也可以称为样本空间。显然 $\displaystyle X\subseteq\mathcal{X}^n $。 输入空间的矩阵表示和集合表示我们需要多加熟悉、灵活运用。这是我们思考多维问题的基础。 2、输出空间集合$\displaystyle Y=\{y_1\,,y_2\,,..\,,y_i\,,…\,,y_n\} \subseteq \mathcal{Y} $，其中输出空间 $\displaystyle \mathcal{Y} $、 输入样本$\displaystyle Y $、输入样本点 $\displaystyle y_i $。 矩阵表示： $\displaystyle \boldsymbol{y}=[y_1\,,y_2\,,..\,,y_i\,,…\,,y_n]^{\text{T}} $ 在有监督学习中，我们把集合 $\displaystyle \{(\boldsymbol{x}_i,y_i)\mid 1 \leqslant i\leqslant n\}$也叫做训练集$\mathcal{D}$ ，$\displaystyle (\boldsymbol{x}_i,y_i) $表示第i个样本。 符号$\displaystyle P(y\mid \boldsymbol{x})=\mathcal{N}(y\mid \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$与$\displaystyle y\mid \boldsymbol{x}\sim\mathcal{N}( \boldsymbol{x}^{\text{T}}\boldsymbol{\beta},\sigma)$是同一个意思。注意 $\displaystyle \mid $的不同意思。[^1] 统计学中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{\beta}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{\beta}) $ 机器学习中，我们写成这样 $\displaystyle P(y\mid\boldsymbol{x},\boldsymbol{w}) $和 $\displaystyle P(\boldsymbol{x}\mid\boldsymbol{w}) $ 3、假设空间：1、如果真实的世界的关系是 $\displaystyle y=h(\boldsymbol{x})$， 世界充满噪声。所以 $\displaystyle y=h(\boldsymbol{x})+e$。现在我们有一个样本或者数据集 $\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$。我们想通过这个数据集 $\displaystyle \mathcal{D}$估计出 $\displaystyle f(\boldsymbol{x})$来找到 $\displaystyle h(\boldsymbol{x})$。其实我们能找到的 $\displaystyle f$有很多，现在我们把它汇集起来： $\displaystyle \mathcal{H}=\{f_i\}$。我们的模型是 $\displaystyle y=f(\boldsymbol{x})+\varepsilon$。 现在我们换一个说法:1、世界是这样的： $\displaystyle p(y=h(\boldsymbol{x})\mid\boldsymbol{x})$2、我们观察到的世界是这样的：$\displaystyle \mathcal{D}=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^{n}$3、我们假设世界是这样的： $\displaystyle p(y=f(\boldsymbol{x)\mid }\boldsymbol{x},\mathcal{D},M)$[^1]，其中 $\displaystyle M$是模型(算法)。于是$\displaystyle \varepsilon=y-f=y-h+h-f=y-h+h-\mathrm{E}[f]+\mathrm{E}[f]-f$$\displaystyle \mathrm{E}[\varepsilon^2]=\mathrm{Var}[e]+\left(h-\mathrm{E}[f]\right)^2+\mathrm{E}\left[\left(f-\mathrm{E}[f]\right)^2\right]$$$ 平方损失期望=噪声方差+偏误^2+模型方差$$ 4、我们还可以写成：$\displaystyle \mathcal{H} =\{f\mid p(y=f(\boldsymbol{x})\mid\boldsymbol{x}, \mathcal{D})\}=\{f(\boldsymbol{\beta})\mid p(y=f(\boldsymbol{x};\boldsymbol{\beta})\mid \boldsymbol{x},\mathcal{D};\boldsymbol{\beta}),\boldsymbol{\beta}\in \mathbb{R}^k\}$。这里的$\displaystyle \mathcal{H} $是模型 $f$的集合。 2、这里的符号有一个重要的解释：$\displaystyle y $是一个随机变量，它的取值是 $\displaystyle y=y_i $。 $\displaystyle \boldsymbol{x} $表示的是 $\displaystyle n $个 $\displaystyle k $维输入数据。也就是说 $\displaystyle \boldsymbol{x} $也是一个变量，不过是向量的形式。它的取值是 $\displaystyle \boldsymbol{x}=\boldsymbol{x}_i $。 3、换一个程序员比较好理解的说法：$\displaystyle y,\boldsymbol{x} $是一个类。而 $\displaystyle y_i,\boldsymbol{x}_i $是一个实例。所以一个实例 $\displaystyle P(y_i\mid \boldsymbol{x}_i,\mathcal{D})$，又有 $\displaystyle P(y_{n+1}\mid \boldsymbol{x}_{n+1},\mathcal{D})$是一个数或者一个概率。$\displaystyle \hat{y},\hat{y}_i $也是类和实例的区别。 输出的最佳估计： $\displaystyle \hat{y}=\hat{f}(x)=\mathop {\text{argmax}}\limits_{\hat{y}}P(y=\hat{y}\mid \boldsymbol{x},\mathcal{D})$ 4、算法空间$\displaystyle \zeta\in\mathcal{L} $，它是算法的集合。 5、参数空间$\displaystyle \boldsymbol{\beta} \in\mathbb{R}^k $。这里的元素我们将 $\displaystyle \mathbb{R}^k$的k维有序组与向量矩阵$\mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}$等同，以方便表达。 6、概念总结有了这些基本概念，我们就可以建立起机器学习的基本框架。一张图搞定: 机器学习框架 7、指示函数，或者叫示性函数$\displaystyle \mathrm{I}_x(A)=\begin{cases}1&amp;\text{if }x\in A\\0&amp;\text{if }x\notin A\end{cases}$ 8、评论这段概论，大部分是站在频率学派的角度解释的，以后我们还会用贝叶斯学派的观点。 我们注意到符号与文字的转换要非常熟练，这就像英语，如果做到同声翻译的水平，这将有利于快速理解。所以一套好的数学符号是非常关键，好数学符号令人赏心悦目。但是每个人都有不同的风格，这就有点无语，以至于不同的书，符号不一样。TMD这是英语有了方言啊。有些书上的符号真是丑的不堪入目啊。严重影响阅读学习体验。 二、回归模型1、线性回归模型：模型的一些表示方法$ y_i=\boldsymbol{x}_{i}^T\boldsymbol{\beta}+\epsilon_i=\boldsymbol{x}_{i,:\,}^T\boldsymbol{\beta}+\epsilon_i$$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $模型矩阵解释：$$\displaystyle \mathop{\boldsymbol{y}}\limits_{(n\times 1)}=\underbrace{\mathop{\boldsymbol{X}}\limits_{(n\times k)} \mathop{\boldsymbol{\beta}}\limits_{(k\times 1)}}_{n\times k} +\mathop{\boldsymbol{\epsilon}}\limits_{(n\times 1)} $$ 2、梯度下降算法：$\boldsymbol{\beta}: =\boldsymbol{\beta}-\alpha\nabla S$梯度下降算法的本质：可以使用相图的思想加以理解。例如有关系$\displaystyle F(x,y,t)=0$。如果我们画出$$\begin{cases}\dot{x}=-3x+5y\\\dot{y}=-5x-7\sin(y)\end{cases}$$动力系统的相图。那么如果是凸函数。相图上的曲线集就会流向平衡点。如图 相图所谓梯度就是图中的箭头乘以梯度的大小。代表了该点速度最快的方向。而 $\displaystyle\alpha_k$就是给梯度加了一个控制器。所以应该能够理解梯度下降算法了：$$\boldsymbol{\beta}_{k+1}=\boldsymbol{\beta}_{k}-\alpha_k\nabla S(\boldsymbol{\beta}_k)$$所以当系统比较复杂的时候，必然就面临问题。例如这种：$$\begin{cases}\dot{x}=-x+y\\\dot{y}=xy-1\end{cases}$$这个系统就非常复杂了。初始位置不同，我们将走向完全不同的结局。相图2 3、规范方程规范方程的本质可以如下理解： 线性回归几何解释 解决学习平方误差 $\displaystyle S$的最小化问题：$$\mathop {\min }\limits_\boldsymbol{\beta}S=\boldsymbol{\epsilon}^{\text{T}}\boldsymbol{\epsilon} $$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$ 现在我们用线性空间的概念来加以理解：$\displaystyle \boldsymbol{X}$张成的空间,或者说超平面 $\displaystyle span(\boldsymbol{X})=span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,j},…,\boldsymbol{x}_{:,k})$ 这里的 $\displaystyle\boldsymbol{x}_{:,j}=\left[\begin{array}{c}x_{1,j} \\x_{2,j}\\\vdots\\x_{n,j}\end{array}\right]$。如图我们很容发现要使得 $\displaystyle\boldsymbol{\epsilon} $的欧式距离最短。那么$\displaystyle\boldsymbol{\epsilon} $必然与 $\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$垂直。即有如下方程。$$\boldsymbol{X}^T\boldsymbol{\epsilon}=\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})=\boldsymbol{0}$$简单推理易得：$\hat{\boldsymbol{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}$。所以 $\displaystyle \boldsymbol{y} $的最佳估计量 $\displaystyle \hat{\boldsymbol{y} }$是 $\displaystyle \boldsymbol{y} $在$\displaystyle span(\boldsymbol{x}_{:,1},…,\boldsymbol{x}_{:,jj},…,\boldsymbol{x}_{:,k})$空间上的投影。 注释：[^1]: 如果 $\displaystyle \zeta$表示算法,可写为$\displaystyle P(y\mid \boldsymbol{x},\mathcal{D},\zeta) $ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2017-01-08-机器学习笔记0001/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>大数据</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三大分布]]></title>
    <url>%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%2F2017-01-08-%E4%B8%89%E5%A4%A7%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/统计学/2017-01-08-三大分布/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$分布1、定义$$\begin{align}x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0\end{align}$$ 我们有时候也这样书写： $\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\exp(-vx)$。我们称之为等待时机分布，其中 $\displaystyle v$是速度参数。 $\displaystyle n$是次数参数。随机变量的含义是：事件 $\displaystyle A$出现后，再次出现 $\displaystyle n$次需要的时间。 2、$\displaystyle \Gamma$分布的特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 我们来求 $\displaystyle \Gamma$分布的特征函数：$$\begin{align}\displaystyle \varphi(t)&amp;=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\times\mathrm{Ga}(x\mid n,v)\mathrm{d}x\\&amp;=\int_0^{\infty}\mathrm{e}^{\mathrm{i}t x}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\mathrm{d}x\\&amp;=\int_0^{\infty}\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-(vx-\mathrm{i}t x)}\mathrm{d}x\,,y=vx-\mathrm{i}t x\\&amp;=\frac{v^n}{\Gamma(n)}\int_A \left(v- \mathrm{i}t\right)^{-n}y^{n-1}\mathrm{e}^{-y}\mathrm{d}y\\&amp;=\frac{v^n}{\Gamma(n)}\frac{\Gamma(n)}{\left(v- \mathrm{i}t\right)^n}\\&amp;=\left(\frac{v}{v- \mathrm{i}t}\right)^n\end{align}$$ 3、$\displaystyle \Gamma$分布的数字特征期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=\left(-\mathrm{i}\right)^2n \frac{v^{n+1}}{\left(v- \mathrm{i}t\right)^{n+2}}\bigg|_{t=0}=\frac{n}{v}$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=\frac{n}{v^2}$ 众数$\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{Ga}(x\mid n,v)=\frac{n-1}{v}$其中：$\displaystyle \frac{\partial }{\partial x}\mathrm{Ga}(x\mid n,v)=-C\mathrm{e}^{-vx} \left( x^{n-1}v-x^{n-2}n+x^{n-2} \right)=0\to\mathrm{mode}[x] $ 4、$\displaystyle \Gamma$分布的可加性【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从$\displaystyle \Gamma$分布 $\displaystyle x_i\sim \mathrm{Ga}(x_i\mid m_i,v)$。那么$$\begin{align} \zeta=x_1+x_2+\cdots+x_n\sim\mathrm{Ga}(\zeta\mid m_1+m_2+\cdots+m_n,v)\end{align}$$ 证明：我们有 $\displaystyle \Gamma$分布的特征函数： $\displaystyle \varphi(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^n$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特征函数之积。知道： $$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)=\left(\frac{v}{v- \mathrm{i}t}\right)^{m_1+m_2+\cdots+m_n}\end{align}$$ 于是证明了结论。 二、$\displaystyle \chi^2(x\mid n)$分布1、定义$\displaystyle \chi^2(x\mid n)$分布$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 可以看出： $\displaystyle x\sim\chi^2(x\mid n)=\mathrm{Ga}(x\mid \frac{n}{2},\frac{1}{2}) $。 【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$是相互独立的标准高斯分布，那么随机变量：$$\begin{align}\xi=\chi^2=x_1^2+x_2^2+\cdots+x_n^2\sim\chi^2(\xi\mid n)\end{align}$$ 证明： 【引理】: 若 $\displaystyle x\sim \mathcal{N}(0,1)$,则 $\displaystyle x^2\sim\chi^2(x\mid 1)$ 有$\displaystyle y=x^2$的反函数：$$x=h(y)=\begin{cases} -\sqrt{y} \,,y&lt;0 \\\ \sqrt{y}\,,0\leqslant y&lt;+\infty \end{cases}$$容易知道导数的绝对值 $\displaystyle \left|h’(y)\right|=\frac{1}{2\sqrt{y}}$。由变量代换定理有：$$\begin{align}p(y)=\frac{1}{\sqrt{2\pi}}y^{-\frac{1}{2}}\mathrm{e}^{-\frac{y}{2}}=\frac{2^{-\frac{1}{2}}}{\Gamma \left(\frac{1}{2}\right)}y^{\frac{1}{2}-1}\mathrm{e}^{-y/2}\end{align}$$也就是说：$$\begin{align}p(y)=\chi^2(x\mid 1)=\mathrm{Ga}(x\mid \frac{1}{2},\frac{1}{2})\end{align}$$ 然后应用 $\displaystyle \Gamma$分布的可加性容易证明定理。 2、$\displaystyle \chi^2(x\mid n)$分布的特征函数和数字特征特征函数$$\begin{align}\displaystyle \varphi(t)=\mathrm{E}[\mathrm{e}^{\mathrm{i}t x}]=\left(1-2 \mathrm{i}t\right)^{-n/2}\end{align}$$ 期望 $\displaystyle \displaystyle\mathrm{E}[x]=(-\mathrm{i})^1\left.\frac{\partial{\varphi(t)}}{\partial{t}}\right| _{t=0}=-\left(\mathrm{i}\right)^2\left( 1-2\,it \right) ^{-n/2-1}n\bigg|_{t=0}=n$ 方差 $\displaystyle \displaystyle \mathrm{var}[x]=(-\mathrm{i})^2\left.\frac{\partial^2{\varphi(t)}}{\partial{t}^2}\right| _{t=0}-\mathrm{E}^2[x]=-(\mathrm{i})^2 \left( 1-2\,it \right) ^{-n/2-2} \left( n+2 \right) n-n^2=2n$ 众数 $\displaystyle \displaystyle \mathrm{mode}[x]=\mathop{\mathrm{argmax}}_{x}\,\mathrm{\chi}^2(x\mid n)=n-2$其中：$\displaystyle \frac{\partial }{\partial x}\chi^2(x\mid n)=1/2C\,\mathrm{e}^{-x/2}\left( x^{n/2-2}n-2\,x^{n/2-2}-x^{n/2-1} \right) =0\to\mathrm{mode}[x] $ 3、$\displaystyle \chi^2(x\mid n)$分布的可加性质【定理】若随机变量 $\displaystyle x_1,x_2,…,x_n$相互独立，并且都服从 $\displaystyle \chi^2$分布 $\displaystyle x_i\sim \chi^2(x_i\mid m_i)$。那么$$\begin{align} x_1+x_2+\cdots+x_n\sim\chi^2(x\mid m_1+m_2+\cdots+m_n)\end{align}$$证明：我们有 $\displaystyle \chi^2$分布的特征函数： $\displaystyle \varphi(t)=\left(1-2 \mathrm{i}t\right)^{-n/2}$。考虑上述相互独立的随机变量。由独立随机变量之和的特征函数是独立变量特殊函数之积。知道：$$\begin{align} \varphi_{ x_1+x_2+\cdots+x_n}(t)=\prod_{i=1}^{n}\varphi_{x_i}(t)= \left(1-2 \mathrm{i}t\right)^{-\left(m_1+m_2+\cdots+m_n\right)/2}\end{align}$$于是证明了结论。 三、 $\displaystyle t(x\mid n)$分布1、定义$$\begin{align}x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}\end{align}$$ 【定理】若 $\displaystyle x\sim\mathcal{N}(0,1),y\sim\chi^2(n)$,且 $\displaystyle x$和 $\displaystyle y$相互独立,那么：$$\begin{align}\tau=\dfrac{x}{\sqrt{y/n}}\sim t(\tau\mid n)\end{align}$$ 证明：知：$$\begin{align}p(x)=\frac{1}{\sqrt{2\pi}}\mathrm{e}^{-\frac{x^2}{2}}\,,-\infty&lt;x&lt;+\infty\end{align}$$ 又知：$$\begin{align}p(y)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$于是由变量代换定理有 $\displaystyle z=\sqrt{y/n}$的密度：$$\begin{align}p(z)&amp;=p(nz^2)\times\left|2nz\right|=\frac{2nz}{2^{n/2}\Gamma \left(\frac{n}{2}\right)}\left(nz^2\right)^{n/2-1}\mathrm{e}^{-(nz^2)/2}\\&amp;=\frac{\sqrt{2n}}{\Gamma \left(\frac{n}{2}\right)}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2}\end{align}$$由随机变量商的分布知：$$\begin{align}p(\tau)&amp;=\int_{-\infty}^{+\infty}p_x(\tau z)\times p_z(z)\times \left|z\right|\mathrm{d}z\\&amp;=(n\pi)^{-1/2}\frac{1}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}\left(nz^2/2\right)^{(n-1)/2}\mathrm{e}^{-(nz^2)/2 \left(1+\tau^2/n\right)}\,,s=(nz^2)/2 \left(1+\tau^2/n\right)\\&amp;=(n\pi)^{-1/2}\frac{\left(1+\frac{\tau^2}{n}\right)^{-(n+1)/2}}{\Gamma \left(\frac{n}{2}\right)}\int_0^{+\infty}s^{(n+1)/2-1}\mathrm{e}^{-s}\mathrm{d}s\\&amp;=(n\pi)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+\tau^2/n\right)^{-(n+1)/2}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-\frac{n+1}{2}}\\\end{align}$$ 即有：$$\begin{align}\tau\sim t(\tau\mid n)=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{\tau^2}{n}\big)^{-(n+1)/2}\end{align}$$ 证毕 2、$\displaystyle t(x\mid n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]\end{align}$$ 知：$\displaystyle t(-x\mid n)=t(x\mid n)$。所以：$$\begin{align}\mathrm{E}[x^k]=0,k\in\{2a\mid a\in \mathbb{R}\}\end{align}$$ 下面我们来考虑： $\displaystyle k\in\{2a+1\mid a\in \mathbb{R}\} $$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{-\infty}^{+\infty} x^kt(x\mid n)\mathrm{d}x=\int_{-\infty}^{+\infty} x^k\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{-\infty}^{+\infty} x^k\big(1+\frac{x^2}{n}\big)^{-(n+1)/2}\mathrm{d}x\,,y=1+\frac{x^2}{n}\\&amp;=\frac{n^{-\frac{1}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}2\int_{1}^{+\infty}n^{\frac{k}{2}}(y-1)^{\frac{k}{2}}y^{-\frac{n+1}{2}}\frac{1}{2}n^{\frac{1}{2}}(y-1)^{\frac{1}{2}}\mathrm{d}y\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{+\infty}(y-1)^{\frac{k-1}{2}}y^{-\frac{n+1}{2}}\mathrm{d}y\,,z=\frac{1}{y}\\&amp;=-\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{1}^{0}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(\frac{1}{z}-1)^{\frac{k-1}{2}}(\frac{1}{z})^{-\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{-\frac{k-1}{2}}z^{\frac{n+1}{2}}z^{-2}\mathrm{d}z\\&amp;=\frac{n^{\frac{k}{2}}}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\int_{0}^{1}(1-z)^{\frac{k+1}{2}-1}z^{\frac{n-k}{2}-1}\mathrm{d}z\\&amp;=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)}\\\end{align}$$ 即有：$$\begin{align}\mathrm{E}[x^k]=n^{\frac{k}{2}}\frac{\mathrm{B} \left(\frac{n-k}{2},\frac{k+1}{2}\right)}{\mathrm{B} \left(\frac{n}{2},\frac{1}{2}\right)},k\in\{2a+1\mid a\in \mathbb{R}\}\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=0\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n)}{n-2}\,,n&gt;2$ 四、 $\displaystyle F(x\mid m,n)$分布1、定义$$\begin{align}x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0\end{align}$$ 【定理】若有随机变量 $\displaystyle x_1\sim \chi^2(m),x_2\sim\chi^2(n)$，且 $\displaystyle x_1$和 $\displaystyle x_2$相互独立，那么：$$\begin{align}\varphi= \frac{x_1/m}{x_2/n}= \frac{\chi^2(m)/m}{\chi^2(n)/n}\sim F(\varphi\mid m,n)\end{align}$$证明：首先我们来分析一下： $\displaystyle x/n$的分布, 我们知道：$$\begin{align}x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0\end{align}$$ 令 $\displaystyle x=nz$，由变量代换定理知道： $$\begin{align} z\sim \frac{2^{-\frac{n}{2}}n}{\Gamma \left(\frac{n}{2}\right)}(nz)^{n/2-1}\mathrm{e}^{-nz/2}\,,z&gt;0 \end{align}$$于是有 $\displaystyle z_1=\varphi z_2$由随机变量商的分布知：$$\begin{align}p(\varphi)&amp;=\int_{0}^{+\infty}p_{z_1}(\varphi z_2)\times p_{z_2}(z_2)\times \left|z_2\right|\mathrm{d}z_2\\&amp;=C\int_{0}^{+\infty}(\varphi z_2)^{m/2-1}\mathrm{e}^{-m\varphi z_2/2}(z_2)^{n/2-1}\mathrm{e}^{-n z_2/2}z_2\mathrm{d}z_2\\&amp;=C\varphi^{m/2-1}\int_{0}^{+\infty}z_2^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}z_2(n+m\varphi)}\mathrm{d}z_2\,v=z_2(n+m\varphi)\\&amp;=C\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\int_{0}^{+\infty}v^{(m+n)/2-1}\mathrm{e}^{-\frac{1}{2}v}\mathrm{d}v\\&amp;=C\Gamma\big(\frac{m+n}{2}\big)2^{(m+n)/2}\varphi^{m/2-1}(n+m\varphi)^{-(m+n)/2}\,,C=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}2^{-(m+n)/2}}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\varphi^{m/2-1}\left(n+m\varphi\right)^{-(m+n)/2}\,,\varphi&gt;0\end{align}$$ 2、$\displaystyle F(x\mid m,n)$分布的数字特征我们来研究 $\displaystyle k$阶原点矩$$\begin{align}\mathrm{E}[x^k]&amp;=\int_{0}^{+\infty}x^kF(x\mid m,n)\mathrm{d}x\\&amp;=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\int_{0}^{+\infty}x^kx^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=C\int_{0}^{+\infty}x^{k+m/2-1}\left(n+mx\right)^{-(m+n)/2}\mathrm{d}x\\&amp;=Cn^{-(m+n)/2}\int_{0}^{+\infty}x^{k+m/2-1}\left(1+\frac{m}{n}x\right)^{-(m+n)/2}\mathrm{d}x\,,y=1+\frac{m}{n}x\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{+\infty}(y-1)^{k+m/2-1}y^{-(m+n)/2}\mathrm{d}y\,z=\frac{1}{y}\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{1}^{0}(\frac{1}{z}-1)^{k+m/2-1}z^{(m+n)/2}(-z^{-2})\mathrm{d}z\\&amp;=Cn^{-(m+n)/2}\big(\frac{n}{m}\big)^{k+m/2}\int_{0}^{1}(1-z)^{m/2+k-1}z^{n/2-k-1}\mathrm{d}z\\&amp;=\big(\frac{n}{m}\big)^k\frac{\mathrm{B}\left(\frac{m}{2}+k,\frac{n}{2}-k\right)}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}\,,2k&lt;n\\&amp;=\big(\frac{n}{m}\big)^k\frac{\Gamma\big(\frac{m}{2}+k\big)\Gamma\big(\frac{n}{2}-k\big)}{\Gamma\big(\frac{m}{2}\big)\Gamma\big(\frac{n}{2}\big)}\,,2k&lt;n\\\end{align}$$ 特别有：$\displaystyle \mathrm{E}[x]=\frac{n}{n-2}\,,n&gt;2$ $\displaystyle \mathrm{Var}[x]=\frac{n^2(2m+2n-4)}{m(n-2)^2(n-4)^2}\,,n&gt;4$ 五、评述我们通过伽马函数引出伽马分布，当然这有点突兀，怎么就突然冒出来了伽马函数？后面还有一个贝塔函数，似乎有一种神秘力量在背后。这两个函数反复出现是有原因的。有机会我们专门会抽空补充一下，让它更加自然。事实上我们可以通过指数分布引入伽马分布。 伽马分布：$\displaystyle x\sim \mathrm{Ga}(x\mid n,v)=\frac{v^n}{\Gamma(n)}x^{n-1}\mathrm{e}^{-vx}\,,x&gt;0$ 卡方分布：$\displaystyle x\sim\chi^2(x\mid n)=\frac{2^{-\frac{n}{2}}}{\Gamma \left(\frac{n}{2}\right)}x^{n/2-1}\mathrm{e}^{-x/2}\,,x&gt;0$ t分布：$\displaystyle x\sim t(x\mid n)=\left(n\pi\right)^{-1/2}\frac{\Gamma \left(\frac{n+1}{2}\right)}{\Gamma \left(\frac{n}{2}\right)}\left(1+x^2/n\right)^{-(n+1)/2}$ F分布：$\displaystyle x\sim F(x\mid m,n)=\frac{m^{\frac{m}{2}}n^{\frac{n}{2}}}{\mathrm{B}\left(\frac{m}{2},\frac{n}{2}\right)}x^{m/2-1}\left(n+mx\right)^{-(m+n)/2}\,,x&gt;0$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/统计学/2017-01-08-三大分布/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>三大分布</tag>
        <tag>伽马分布</tag>
        <tag>卡方分布</tag>
        <tag>t分布</tag>
        <tag>F分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伽马函数]]></title>
    <url>%2F%E6%A6%82%E7%8E%87%E8%AE%BA%2F2017-01-07-%E4%BC%BD%E9%A9%AC%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/概率论/2017-01-07-伽马函数/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、 $\displaystyle \Gamma$函数$$\begin{align}\Gamma(x)=\int_0^\infty u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\,,x&gt;0\end{align}$$ 它有如下性质：1、对于 $\displaystyle x\in(0,+\infty)$，有 $\displaystyle \Gamma(x+1)=x \Gamma(x)$成立2、 $\displaystyle \Gamma(n+1)=n!\,,n=1,2,3,…$3、 $\displaystyle \log \Gamma$在 $\displaystyle (0,+\infty)$上是凸的 1、若干引理在证明上诉性质前：我们要证明 $\displaystyle \Gamma(x)$在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数。我们先说明反常积分柯西判别法的两个引理 【引理1】设 $\displaystyle f$定义在 $\displaystyle [a,+\infty)$，在任何有限区间 $\displaystyle [a,v]$上可积，且$$\begin{align}\lim_{x\to +\infty}x^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle p&gt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^{+\infty}f(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\leqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 与无界反常积分类似，也存在类似的瑕积分的判别方法【引理2】设 $\displaystyle f$定义在 $\displaystyle(a,b]$，其中 $\displaystyle a$位瑕点，在任何 $\displaystyle [v,b]\subset(a,b]$上可积，且$$\begin{align}\lim_{x\to a^+}(x-a)^p \left|f(x)\right|=\lambda\end{align}$$则有：1）当 $\displaystyle 0&lt;p&lt;1,0\leqslant \lambda&lt;+\infty$时， $\displaystyle \int_a^b\left|f(x)\right|\mathrm{d}x$收敛，就是说：$\displaystyle \int_a^bf(x)\mathrm{d}x$收敛 2）当 $\displaystyle p\geqslant1,0&lt;\lambda\leqslant+\infty$时， $\displaystyle \int_a^{+\infty}\left|f(x)\right|\mathrm{d}x$发散 于是做如下分析：$$\begin{align}\Gamma(x)=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\end{align}$$ 根据引理1、2分析 $\displaystyle \Phi(x)，\Psi(x)$。我们容易得出 $\displaystyle \Gamma(x)$函数的定义域是： $\displaystyle (0,+\infty) $ 回忆一下：含参反常积分在 $\displaystyle E$上一致收敛概念： 然后我们归纳如下引理： 魏尔斯特拉斯 $\displaystyle M$判别法，以及含参反常积分一致收敛的可微性质。 【魏尔斯特拉斯 $\displaystyle M$判别法】 设有函数 $\displaystyle g(y)$，使得：$$\begin{align}\left|f(x,y)\right|\leqslant g(y)\,,(x,y)\in E\times[c,+\infty)\end{align}$$若 $\displaystyle \int_c^{+\infty}g(y)\mathrm{d}y$收敛。则 $\displaystyle \int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛 。【含参反常积分一致收敛的连续性质】设 $\displaystyle f(x.y)$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle [a,b]\in[c,+\infty)$上连续。 【含参反常积分一致收敛的可微性质】设 $\displaystyle f(x.y)$与 $\displaystyle \frac{\partial f}{\partial x}$在 $\displaystyle E\times[c,+\infty)$上连续。若 $\displaystyle F(x)=\int_c^{+\infty}f(x,y)\mathrm{d}y$在 $\displaystyle E$上收敛， $\displaystyle \int_c^{+\infty}\frac{\partial f}{\partial x}\mathrm{d}y$在 $\displaystyle E$上一致收敛，则 $\displaystyle F(x)$在 $\displaystyle E$上可微，且：$$\begin{align}\frac{\partial F}{\partial x}=\frac{\partial}{\partial x}\int_c^{+\infty}f(x,y)\mathrm{d}y=\int_c^{+\infty}\frac{\partial }{\partial x}f(x,y)\mathrm{d}y\end{align}$$ 2、$\displaystyle \Gamma(x)$是在定义域 $\displaystyle (0,+\infty)$内连续且可导的凸函数下面我们来说明 $\displaystyle \Gamma(x)$的内闭一致收敛性质，对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有 $$\begin{align}\Gamma(x)&amp;=\int_0^1u^{x-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{x-1}\mathrm{e}^{-u} \mathrm{d}u=\Phi(x)+\Psi(x)\\&amp;\leqslant\int_0^1u^{a-1}\mathrm{e}^{-u} \mathrm{d}u+\int_1^{+\infty}u^{b-1}\mathrm{e}^{-u} \mathrm{d}u&lt;\infty\end{align}$$同时考虑到 $\displaystyle u^{x-1}\mathrm{e}^{-u}&gt;0$且是连续函数。由魏尔斯特拉斯 $\displaystyle M$判别法和含参反常积分一致收敛的连续性质知道 $\displaystyle \Gamma(x)$在 $\displaystyle [a,b]\subset (0,+\infty)$一致收敛且在第一域上连续。 同时我们考虑到：$$\begin{align}\int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln u\mathrm{d}u\end{align}$$ 对于任意的 $\displaystyle [a,b]\subset (0,+\infty)$有$$\begin{align}&amp;\int_0^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u\right|\mathrm{d}y\\&amp;\leqslant\int_0^1\left|u^{x-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{x-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u\\&amp;\leqslant\int_0^1\left|u^{a-1}\mathrm{e}^{-u} \ln u\right|\mathrm{d}u+\int_1^{+\infty}\left|u^{b-1}\mathrm{e}^{-u}\ln u \right|\mathrm{d}u&lt;\infty\\\end{align}$$故 $\displaystyle \int_0^{+\infty}\frac{\partial }{\partial x}\left(u^{x-1}\mathrm{e}^{-u}\right)\mathrm{d}u$在 $\displaystyle [a,b]\subset (0,+\infty)$上一致收敛。于是由含参反常积分一致收敛的可微性质知道 $\displaystyle \Gamma(x)$在任意$\displaystyle [a,b]\subset (0,+\infty)$上可导，也是说在在定义域 $\displaystyle (0,+\infty)$上可导。且有：$$\begin{align}\Gamma^{(n)}(x)=\int_0^{+\infty}u^{x-1}\mathrm{e}^{-u}\ln^n u\mathrm{d}u\,,x&gt;0\end{align}$$容易知道： $\displaystyle \Gamma’’(x)&gt;0$。于是就证明了： $$\begin{align}\Gamma(x)是在定义域(0,+\infty)内连续且可导的凸函数。\end{align}$$证明中，一些细节并未详细说明，但是这是简单的。所以请注意。 3、其他性质的证明$$\begin{align}\int_0^a u^{x}\mathrm{e}^{-u}\mathrm{d}u&amp;=-u^x\mathrm{e}^{-u}\big|_0^a+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\&amp;=-a^x\mathrm{e}^{-a}+x\int_0^a u^{x-1}\mathrm{e}^{-u}\mathrm{d}u\\\end{align}$$令 $\displaystyle a\to+\infty$有：$$\begin{align}\Gamma(x+1)=x\Gamma(x)\end{align}$$ 若 $\displaystyle x\in \mathbb{Z}^+$有：$$\begin{align}\Gamma(n+1)=n(n-1)\cdots 2\cdot \Gamma(1)=n!\int_0^{+\infty}\mathrm{e}^{-u} \mathrm{d}u=n!\end{align}$$ 4、 $\displaystyle \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}$证明：令 $\displaystyle A=\Gamma\big(\frac{1}{2}\big)=\int_0^\infty u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u$，于是有：$$\begin{align}A&amp;=\int_0^{+\infty} u^{-\frac{1}{2}}\mathrm{e}^{-u}\mathrm{d}u\,,u=t^2\\&amp;=2\int_0^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\\&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-t^2}\mathrm{d}t\end{align}$$ 同时有：$$\begin{align}A^2&amp;=\int_{-\infty}^{+\infty}\mathrm{e}^{-x^2}\mathrm{d}x\int_{-\infty}^{+\infty}\mathrm{e}^{-y^2}\mathrm{d}y\\&amp;=\iint_{R^2}\mathrm{e}^{-(x^2+y^2)}\mathrm{d}x \mathrm{d}y\,,x=r\cos(\theta),y=r\sin(\theta)\\&amp;=\int_{0}^{2\pi}\int_{0}^{+\infty}r \mathrm{e}^{-r^2}\mathrm{d}r \mathrm{d}\theta=\frac{1}{2}\int_{0}^{2\pi}\mathrm{d}\theta=\pi\end{align}$$ 于是有：$$\begin{align} \Gamma\big(\frac{1}{2}\big)=\sqrt{\pi}\end{align}$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/概率论/2017-01-07-伽马函数/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习书单]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2016-12-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在列出一些书单。若有错误，请大家指正。关键词: 机器学习,,书单,学习资源 机器学习资源因为大数据的兴起，机器学习大热。这一行业吸引了大量人才涌入。然后现状是学习机器学习需要较高的数学知识，导致人才供应不足。跟不上行业发展需求。较高的工资和技术时尚吸引大量的人们争相学习机器学习、人工智能。故收集网络整理出这些书籍： 网络上建议的入门顺序：1、斯坦福大学公开课 ：机器学习课程2、李航.统计学习方法3、Machine Learning in Action(机器学习实战） 解释一下这个入门：在这之前你可以简单了解一下，周志华：数据挖掘与机器学习，，斯坦福的公开课可以带你入门。《统计学习方法》可以带你了解基本理论和推导。《机器学习实战》可以让你了解一下实操。这个之后有两个路径，看大神Bengio的《deep learning》，或者继续深化机器学习的知识。当然你也可以参加这个：数据分析竞赛kaggle 附照片&lt;img src=http://oiol5pi05.bkt.clouddn.com/Yoshua_Bengio.jpg width=20%&gt;Yoshua BengioFull ProfessorDepartment of Computer Science and Operations ResearchCanada Research Chair in Statistical Learning Algorithms 解决了机器学习入门问题，我们接下来要进阶：1、PRML（Pattern Recognition and Machine Learning）2、ESL（The Elements of Statistical Learning ）3、MLAPP(Machine Learning: a Probabilistic Perspective) 4、Deep learning-author Yoshua Bengio 上面的书单应该是经典的四本书了。个人比较偏爱《MLAPP》，符号比较优美，叙述比较全面。当然你也可以看模式识别的书，例如《统计模式识别（第3版）Statistical Pattern Reco》，那个封面是豹子的书。 &lt;img src=http://oiol5pi05.bkt.clouddn.com/%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB.jpg width=”50%”&gt; 一些理论当然看这些书的时候，你也许会发现，凸优化，图模型，EM，MCMC之类的，你可以通过下面的书深入一下：1、Convex Optimization2、Probabilistic Graphical Models3、The EM Algorithm and Extensions4、Simulation Fifth Edition Sheldon M. Ross 实战编程1、Python for Data Analysis2、SciPy and NumPy3、Machine Learning for Hackers（这本是用R的）4、集体智慧编程（这个书名有点误导人） 网上搜索很容易找到。现在（2017）又出来很多新书，也发生了很多事情。就不一一列举。 好玩的一本书1、Bad Data Handbook 最后这些书我也还没看。。。。。我会把它们看完的！！！！^_^ 1我们还是来点别的吧： 神经网络学习机器学习之前，你应该先了解、学习神经网络。我知道现在媒体关注点在深度学习，机器学习上。网络上的学习路线也多少是从机器学习开始：从线性模型到深度学习。 可以是，你要知道Hinton的深度学习是挖掘了神经网络的潜能，Hinton是要把被抛弃、被侮辱、几起几落的神经网络再度复兴。在被人摒弃的10年中，加拿大多伦多大学的Geoffery Hinton教授依然坚守。2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。 Geoffery Hinton教授&lt;img src=http://oiol5pi05.bkt.clouddn.com/Geoffrey%20Hinton.jpg width=100%&gt; 与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。 他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。所以我觉得从神经网络到深度学习或者说从神经元到深度学习是会对人工智能有更好的认识。而不是机器学习。神经网络有着传奇的历史，如果你了解它，你将为之着迷。 Andrew Ng对神经网络的看法：&lt;img src=http://oiol5pi05.bkt.clouddn.com/Andrew%20Ng%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%81%E5%BA%A6.jpg width=100%&gt; 神经网络的书籍在这里：1、人工神经网络教程 第一版、第二版 韩立群2、神经网络原理3、神经网络设计3、神经网络与机器学习4、Matlab与神经网络，这种书籍就非常多了。 下面是我口号：$$\displaystyle Life=\int_{Birth}^{Death} (Learning+Working) \mathrm{d}t$$ 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/机器学习/2016-12-27-机器学习书单/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>神经网络</tag>
        <tag>大数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行列式]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-24-%E8%A1%8C%E5%88%97%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[摘要：本文意在理清行列式的基础问题。若有错误，请大家指正。关键词: 行列式,机器学习 一、定义函数集合$\displaystyle V=\{f\mid f$是矩阵$M_n(K)$上的数量函数$\}$ $\displaystyle V_1=\{f\mid f\in V,$且满足列线性性$\} $ $\displaystyle V_2=\{f\mid f\in V_1,$且满足列反对称性$\}$ $\displaystyle V_3=\{f:f\in V_2,$且满足规范性$\} $ 容易验证， $\displaystyle V,V_1,V_2$关于加法和数乘运算构成函数空间。 二、性质性质1$\displaystyle V$是 $\displaystyle K$上的 $\displaystyle n^2$元函数空间，所以有 $\displaystyle \dim(V)=\infty$ 性质2$\displaystyle \dim(V_1)=n^n$，其一组基底是 $\displaystyle f_{i_1i_2\cdots i_n}(\varepsilon_{j_1}，\varepsilon_{j_2},\cdots,\varepsilon_{j_n})=\delta_{i_1j_1}\delta_{i_2j_2}\cdots\delta_{i_nj_n}$ 性质3$\displaystyle \dim(V_2)=1，V_2=span(\det(\cdot))$，其中 $\displaystyle \det(\cdot)$为行列式函数。 性质4$\displaystyle V_3=\{\det(\cdot)\}$ 三、证明性质2的证明定义 $\displaystyle n^n$维空间 $\displaystyle L=\{(c_{i_1i_2\cdots i_n}):1\leq i_1,i_2,\cdots,i_n \leq n \}$ 定义映射空间 $\displaystyle V_1$到 $\displaystyle L$的线性映射，从而有 $$\begin{align}\dim(V_1)=\dim(L)=n^n\end{align}$$ 1.【H是单射】 若 $\displaystyle f，g \in V_1且H(f)=H(g)=(c_{i_1i_2\cdots i_n})$则有 $$\begin{align}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)=c_{i_1i_2\cdots i_n}\end{align}$$又$$\begin{align}f\big(\alpha_1,\alpha_2,\cdots,\alpha_n)= f(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}f\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$ $$\begin{align}g\big(\alpha_1,\alpha_2,\cdots,\alpha_n\big)= g\big(\sum_{i=1}^{n}a_{i1}\varepsilon_i,\sum_{i=1}^n a_{i2}\varepsilon_i,\cdots,\sum_{i=1}^n a_{in}\varepsilon_i\big)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}g\big(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n}\big)\end{align}$$所以有 $$\begin{align}f=g\end{align}$$ 2.【H是满射】 同时任意给定 $\displaystyle (c_{i_1i_2\cdots i_n})\in L$，定义函数 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{1\leq i_1,i_2,\cdots,i_n\leq n} a_{i_1}a_{i_2}\cdots a_{i_n}c_{i_1i_2\cdots i_n}\end{align}$$ 可以验证 $\displaystyle f \in V_1$ ，此时有$$\begin{align}H(f)=(c_{i_1i_2\cdots i_n})\end{align}$$所以 $\displaystyle H$是满射。根据同态映射 $\displaystyle H$我们不难找到 $\displaystyle V_2$的一组基底 性质3的证明若 $\displaystyle f\in V_2$，由列反对称性，有 $$\begin{align}f(\varepsilon_{i_1}，\varepsilon_{i_2}，\cdots，\varepsilon_{i_n})=\begin{cases}0 &amp;\exists s,t,\to i_s=i_t\\(-1)^{\tau(i_1i_2\cdots i_n)}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)&amp; i_1,i_2,\cdots ,i_n \textit{ pairwise unequal }\end{cases}\end{align}$$ 其中 $\displaystyle \tau(i_1i_2\cdots i_n)$为排列 $\displaystyle i_1.i_2,\cdots,i_n$的逆序数。所以我们有 $$\begin{align}f(\alpha_1,\alpha_2,\cdots,\alpha_n)=\sum_{i_1,i_2,\cdots,i_n\textit{ pairwise unequal } }(-1)^{\tau(i_1i_2\cdots i_n)}a_{i_1}a_{i_2}\cdots a_{i_n}f(\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n)\end{align}$$从而有 $$\begin{align}f(A)=f(E)\cdot \det(A)\end{align}$$ 容易验证具有该表达式的函数 $\displaystyle f$属于 $\displaystyle V_2$ 性质4的证明若 $\displaystyle f\in V_3$,则有 $$\begin{align}f(A)=f(E)\cdot \det(A)=\det(A)\end{align}$$]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学分析笔记-数的建立]]></title>
    <url>%2F%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%2F2016-12-23-%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0-%E6%95%B0%E7%9A%84%E5%BB%BA%E7%AB%8B01%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 摘要：本文意在理清数的问题。若有错误，请大家指正。关键词: 有理数,实数 这是测试文章： 一、数的建立1、有理数的缝隙数的建立是数学分析的基础。实数的最小上界性质是我们开启现代数学的钥匙。 关于$p=\sqrt{2}$的重要定律： 构造一个数 $\displaystyle z=\frac{2x+2}{x+2}=x-\frac{x^2-2}{x+2}$ 同时令 $\displaystyle A=\{a\mid a^2&lt;2,a\in \mathbb{Q}^+\}$ 又有 $\displaystyle B=\{b\mid b^2&gt;2, b \in \mathbb{Q}^+\}$ 我们有 $\displaystyle z^2-2=\frac{2(x^2-2)}{(p+2)^2}$ 即可证明$\displaystyle A$ 里面没有最大的数$$\begin{align}x \in A,\to z \in A\,, z&gt;x\end{align}$$ $\displaystyle B$ 里面没有最小的数$$\begin{align}\,x \in B\to z \in B\,,0&lt;z&lt;x\end{align}$$ 这说明尽管有理数之间还有有理数，但是有理数系还是有空隙。而实数系填满了这些空隙，这就是实数系能在分析学中能起基础作用的主要原因。 $\displaystyle \ell \mathbb{ABC}$ 二、行列式行列式定义： $\displaystyle det: M_n(F) \to \Bbb{R} $ 行列式表达式$$\begin{align}det(A)=\sum _{\sigma \in S_n}sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }\end{align}$$ 其中$\sigma$是集合$X=\{1,2,\,…\,n\}$上的置换：$\sigma: X \to X$。$S_n$是置换$\sigma$的集合,易知$S_n$是一个对称群。$\tau(\sigma)$是$\sigma$的逆序数。$\displaystyle sgn(\sigma)=\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)=(-1)^{\tau(\sigma)}$是置换的符号函数。于是： $$\displaystyle det(A)=\sum _{\sigma \in S_n}\prod_{1\leqslant i&lt;j\leqslant n}sign\big(\sigma(i)-\sigma(j)\big)\prod _{i=1}^n A_{\sigma(i)i }=\sum _{\sigma \in S_n}(-1)^{\tau(\sigma)}\prod _{i=1}^n A_{\sigma(i)i }$$ 简记为：$$det(A)=\sum _{\sigma \in S_n} sgn(\sigma) \prod _{i=1}^n A_{\sigma(i)i }$$ 三、【函数的极限】令$X$和$Y$是度量空间，假设$E \subseteq X$、$\bm{f}$将$E$映入$Y$内、且$\bm{p}$是$E$的极限点。如果 $\forall \epsilon\,,\exists \delta&gt;0$，对于 $\{\bm{x} \in E\mid 0&lt;d_{X}(\bm{x},\bm{p})&lt;\delta\}$中一切点 $\bm{x}$，使得 $\displaystyle d_{Y}(\bm{f}(\bm{x}),\bm{q})&lt;\epsilon,\,\bm{q}\in Y$成立。就说: $$\lim_{\bf{x} \to\bm{p}} \bm{f}(\bm{x})=\bm{q}$$ 卷积$\displaystyle \mathbf{C}=\mathbf{X}*\mathbf{W}$ $\displaystyle \bm{C}=\bm{X}*\bm{W}$ 分段函数$$\displaystyle f(n)= \left\{\begin{matrix} n/2, &amp; \text {if $n$ is even} \\\ 3n+1, &amp; \text{if $n$ is odd} \end{matrix}\right. \\$$ $\displaystyle \begin{bmatrix} 1 &amp; 2 \\\ 3 &amp; 4 \end{bmatrix}$ 我们可以看到 session.run专业我们就可以Variable 1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print(sess.run(hello).decode('utf-8')) 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/数学分析/2016-12-23-数学分析笔记-数的建立01/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>数学分析</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
        <tag>数学</tag>
        <tag>现代数学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyhton-notebook主题颜色配置]]></title>
    <url>%2F%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5%2F2016-05-01-pyhton-notebook%E4%B8%BB%E9%A2%98%E9%A2%9C%E8%89%B2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 一、安装jupyter-themer插件12345678910111213➜ ~ cd anaconda➜ anaconda sudo pip install jupyter-themerPassword:The directory '/Users/xiaobai/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/xiaobai/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Collecting jupyter-themer Downloading jupyter-themer-0.3.0.tar.gz (40kB) 100% |████████████████████████████████| 40kB 35kB/sRequirement already satisfied: jupyter in ./lib/python3.6/site-packages (from jupyter-themer)Requirement already satisfied: notebook in ./lib/python3.6/site-packages (from jupyter-themer)Installing collected packages: jupyter-themer Running setup.py install for jupyter-themer ... doneSuccessfully installed jupyter-themer-0.3.0 二、设定主题1、语法介绍：1234567%语法格式如下usage: jupyter-themer [-c COLOR, --color COLOR] [-l LAYOUT, --layout LAYOUT] [-t TYPOGRAPHY, --typography TYPOGRAPHY] [-f CODE_FONT, --font CODE_FONT] [-b BACKGROUND, --background BACKGROUND] [-s OPTION, --show OPTION] 2、显示所有可选颜色主题：12345678910111213141516171819202122232425262728293031323334353637383940414243➜ anaconda jupyter-themer --show color3024-day3024-nightabcdefambiancebase16-darkbase16-lightblackboardcobaltcolorforthdraculaeclipseeleganterlang-darkicecoderlesser-darkliquibytematerialmbomdn-likemidnightmonokaineatneonightparaiso-darkparaiso-lightpastel-on-darkrubybluesetisolarized-darksolarized-lightthe-matrixtomorrow-night-brighttomorrow-night-eightiesttcntwilightvibrant-inkxq-darkxq-lightyetizenburn➜ anaconda 3、设定monokai主题12➜ anaconda jupyter-themer -c monokaiCustom jupyter notebook theme created - refresh any open jupyter notebooks to apply theme. 效果如下 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/工程实践/2016-05-01-pyhton-notebook主题颜色配置/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>工程实践</category>
      </categories>
      <tags>
        <tag>工程问题</tag>
        <tag>python</tag>
        <tag>notebook</tag>
        <tag>计算环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夜晓风]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2015-09-19-%E5%A4%9C%E6%99%93%E9%A3%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2015-09-19-夜晓风/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夜晓风皎月榕树头，夜晓风。 一曲江畔，感似岁月悠悠， 山河在，壮志未酬。 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2015-09-19-夜晓风/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[穗夜]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2014-06-19-%E7%A9%97%E5%A4%9C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2014-06-19-穗夜/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 穗夜夜声碎月光如练，榕树头江畔愁眠。 桥头浪卷已三更，沉沉幕夏雨如注。 引线小白写于2014-6-19 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2014-06-19-穗夜/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>穗夜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[再回大学城]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-05-20-%E5%86%8D%E5%9B%9E%E5%A4%A7%E5%AD%A6%E5%9F%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-05-20-再回大学城/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 再回大学城沉浸校园曲风气爽清秋节，叶至桂香来，小湖椰影廊桥，曾记否，谷围晓月，灯影朦胧，勘回首，沉浸校园曲风，且看从容，壮志未酬笑谈中。 引线小白写于2011-5-20 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-05-20-再回大学城/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>大学城</tag>
        <tag>诗歌</tag>
        <tag>小谷围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夏雨急]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2011-04-30-%E5%A4%8F%E9%9B%A8%E6%80%A5%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2011-04-30-夏雨急/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 夏雨急夏雨急，落如珠， 今朝且看风疾。 迎地扫，路人急， 此地空余。。。 小白写于2011-4-30日 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2011-04-30-夏雨急/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏雨</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初夏里]]></title>
    <url>%2F%E4%BC%A4%E6%84%9F%2F2011-04-27-%E5%88%9D%E5%A4%8F%E9%87%8C%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/伤感/2011-04-27-初夏里/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 初夏里清风夜，晓月枝头，照斑驳绿影初夏里，世事心愁，思长夜忧忧叹悲歌，一曲无穷，怎泪眼朦胧自惆怅，知与谁同，独望江水流 引线小白写于2011-4-27广州 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/伤感/2011-04-27-初夏里/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>伤感</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>初夏里</tag>
        <tag>引线小白</tag>
        <tag>诗词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[烈日炎炎]]></title>
    <url>%2F%E8%AF%97%E9%9B%86%2F2010-08-04-%E7%83%88%E6%97%A5%E7%82%8E%E7%82%8E%2F</url>
    <content type="text"><![CDATA[作者: 引线小白-本文永久链接：http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/知识共享许可协议: 本博客采用署名-非商业-禁止演绎4.0国际许可证 烈日炎炎日炎炎，清风绝。惜雨荫少，浪浪绵绵。汗易求冰难接。是个酷热天！ 引线小白写于2010-8-4 版权声明由引线小白创作并维护的柠檬CC博客采用署名-非商业-禁止演绎4.0国际许可证。本文首发于柠檬CC [ http://www.limoncc.com ] , 版权所有、侵权必究。本文永久链接http://www.limoncc.com/诗集/2010-08-04-烈日炎炎/ window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];]]></content>
      <categories>
        <category>诗集</category>
      </categories>
      <tags>
        <tag>诗集</tag>
        <tag>诗歌</tag>
        <tag>夏天</tag>
        <tag>日日炎炎</tag>
      </tags>
  </entry>
</search>
