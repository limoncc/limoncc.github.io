<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="大语言模型,LLM,强化学习,GRPO,PPO" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="本文主要总结了大语言模型中的强化学习的若干基本问题，和我自己的一些体会。">
<meta name="keywords" content="大语言模型,LLM,强化学习,GRPO,PPO">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型中的强化学习——大语言模型研究05">
<meta property="og:url" content="https://www.limoncc.com/post/c0a3be9c86b2b4cd/index.html">
<meta property="og:site_name" content="柠檬CC">
<meta property="og:description" content="本文主要总结了大语言模型中的强化学习的若干基本问题，和我自己的一些体会。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://www.limoncc.com/images/强化学习2.png">
<meta property="og:image" content="https://www.limoncc.com/images/强化学习1.png">
<meta property="og:image" content="https://www.limoncc.com/images/cc.png">
<meta property="og:image" content="https://www.limoncc.com/images/avatar.png">
<meta property="og:updated_time" content="2025-02-05T06:30:42.156Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大模型中的强化学习——大语言模型研究05">
<meta name="twitter:description" content="本文主要总结了大语言模型中的强化学习的若干基本问题，和我自己的一些体会。">
<meta name="twitter:image" content="https://www.limoncc.com/images/强化学习2.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: undefined,
      author: '博主'
    }
  };
</script>

  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <title> 大模型中的强化学习——大语言模型研究05 | 柠檬CC </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-73837972-3', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?d225b8f8559eb2eb6a8bd8792e01ebb9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=60130136";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">柠檬CC</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">小白爱吃柠檬O(∩_∩)O</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-poetry">
          <a href="/poetry" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-leaf"></i> <br />
            
            诗集
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                大模型中的强化学习——大语言模型研究05
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2025-01-31T18:30:56+08:00" content="2025-01-31">
              2025-01-31
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/post/c0a3be9c86b2b4cd/" class="leancloud_visitors" data-flag-title="大模型中的强化学习——大语言模型研究05">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: <a href="https://www.limoncc.com">引线小白</a>-本文永久链接：httpss://<a href="http://www.limoncc.com/post/c0a3be9c86b2b4cd/">www.limoncc.com/post/c0a3be9c86b2b4cd/</a><br>知识共享许可协议: 本博客采用<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名-非商业-禁止演绎4.0</a>国际许可证</p>
<blockquote>
<p><strong>摘要</strong>: 本文意在理清大语言模型中的强化学习。若有错误，请大家指正。<br><strong>关键词</strong>: <code>大语言模型</code>,<code>reinforcement learing</code>,<code>强化学习</code></p>
</blockquote>
<p>[TOC]</p>
<h4 id="一、理论基础"><a href="#一、理论基础" class="headerlink" title="一、理论基础"></a>一、理论基础</h4><h5 id="1-1、数学符号"><a href="#1-1、数学符号" class="headerlink" title="1.1、数学符号"></a>1.1、数学符号</h5><p>首先来熟悉一下数学符号</p>
<p>1、$s$: 状态(state)<br>2、$a$: 表示动作(action)<br>3、$r$: 表示奖励(reward), 奖励依赖当前状态、当前动作、未来状态。也就说 $\displaystyle r_t=R_t\left(s_t,a_t,s_{t+1}\right)$<br>4、$u$: 表示回报(return)，它是奖励的折现累积: $\displaystyle u_t = \sum_{i=t}^n\gamma^{i-t}r_i$, 如不特殊说明，一般有小写表示值，大写表示函数或者随机变量。例如</p>
<p>$$\begin{align}<br>U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots<br>\end{align}$$<br>有时候也不加区分。请根据上下文自行判断</p>
<p>5、$\pi$: 策略(policy) 是指如何根据观测到的状态做出决策。随机策略函数是一个概率密度函数$\pi:\mathcal{S}\times \mathcal{A}\to [0,1]$ 记为 $\pi(a\mid s)$。对于策略函数的参数 $\bm{\theta}$，这样也记做 $\displaystyle \pi_{\bm{\theta}}(a\mid s)$, $\displaystyle \pi(a\mid s,\bm{\theta})$</p>
<p>价值函数是回报的期望，即未来期望获得的奖励之和。价值函数反应了现状的好坏，价值函数越大说明现状越有利，知道 $U_t$的随机性来自奖励 $\{R_i\}_{i\geqslant t}$，奖励 $R_t$的随机性来自 $S_t$、 $A_t$、 $S_{t+1}$。</p>
<p>6、动作价值函数(action-value function)</p>
<p>$$\begin{align}<br>Q_{\pi}(s_t,a_t) = \mathbb{E}_{\mathcal{S}_{t+1:},\mathcal{A}_{t+1:}}\left[U_t\mid S_t=s_t,A_t=a_t\right]<br>\end{align}$$</p>
<p>其中：</p>
<p>$\displaystyle \mathcal{S}_{t+1:}=\{S_{t+1},S_{t+2},\cdots\}$<br>$\displaystyle \mathcal{A}_{t+1:}=\{A_{t+1},A_{t+2},\cdots\}$</p>
<p>有时候也用下面简写表示，以节约符号：</p>
<p>$$\begin{align}<br>Q_{\pi}(s_t,a_t)<br>= \mathbb{E}_{t+1:}\left[u_t\mid s_t,a_t\right]<br>= \mathbb{E}_{\mathcal{S}_{t+1:},\mathcal{A}_{t+1:}}\left[U_t\mid s_t,a_t\right]<br>\end{align}$$</p>
<p>7、最优动作价值函数(optimal action-value function)</p>
<p>$$\begin{align}<br>Q_*(s_t,a_t)= \max_{\pi}Q_{\pi}(s_t,a_t) \qquad \forall s_t \in \mathcal{S}, a_t \in \mathcal{A}<br>\end{align}$$</p>
<p>含义就是有多种策略函数 $\pi$可供选择，选择最好的策略函数：</p>
<p>$$\begin{align}<br>\pi^* =\arg\max_{\pi}Q_\pi(s_t,a_t) \qquad \forall s_t \in \mathcal{S}, a_t \in \mathcal{A}<br>\end{align}$$</p>
<p>8、状态价值函数(sata-value function)</p>
<p>$$\begin{align}<br>V_{\pi}(s_t)<br>&amp;= \mathbb{E}_{A_t\sim \pi(\cdot\mid s_t)}\left[Q_{\pi}\left(s_t,A_t\right)\right]\\<br>&amp;= \sum_{a\in \mathcal{A}}\pi(a\mid s_t)\cdot Q_{\pi}(s_t,a)<br>\end{align}$$</p>
<p>也就是说</p>
<p>$$\begin{align}<br>V_{\pi}(s_t) = \mathbb{E}_{\mathcal{S}_{t+1:},\mathcal{A}_{t:}}\left[U_t\mid S_t=s_t\right]<br>\end{align}$$</p>
<p>9、$A_{\pi}(s_t,a_t)$: 优势函数(advantage function)</p>
<p>$$\begin{align}<br>A_{\pi}(s_t,a_t) = Q_{\pi}(s_t,a_t) - V_{\pi}(s_t)<br>\end{align}$$</p>
<p>优势函数是动作价值函数和状态价值函数之差。反应了动作相对于平均值的优势。</p>
<h5 id="1-2、策略学习与策略梯度"><a href="#1-2、策略学习与策略梯度" class="headerlink" title="1.2、策略学习与策略梯度"></a>1.2、策略学习与策略梯度</h5><p>策略学习的目标在于调整策略使得状态价值 $V_{\pi}(S)$的均值最大。目标函数是</p>
<p>$$\begin{align}<br>\mathcal{J}(\bm{\theta})<br>&amp;= \mathbb{E}_{s\in \mathcal{S}}[V_{\pi}(s)]\\<br>&amp;= \mathbb{E}_{\mathcal{S}_{t:},\mathcal{A}_{t:}}\left[U_t\right]<br>\end{align}$$</p>
<p>其中 $\bm{\theta}$是策略网络的参数。</p>
<p>说的明白点就是消除了状态和动作的 $\mathcal{S},\mathcal{A}$的随机性。求它们的期望。这样就只剩下 $\displaystyle \pi_{\bm{\theta}}$。最大化<br>$$\begin{align}<br>\max_{\bm{\theta}}\mathcal{J}(\bm{\theta})<br>\end{align}$$</p>
<p>然后有策略梯度定理:</p>
<p>$$\begin{align}<br>\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \bm{\theta}}<br>= \frac{1-\gamma^n}{1-\gamma}\cdot \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\left[\frac{\partial \log\pi_{\bm{\theta}}\left(a_t\mid s_t\right)}{\partial \bm{\theta}}\cdot Q_{\pi}(s_t,a_t)\right]<br>\end{align}$$<br>通常会忽略 $\frac{1-\gamma^n}{1-\gamma}$，简写成</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>= \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot Q_{\pi}(s_t,a_t) \right]<br>\end{align}$$</p>
<h5 id="1-3、带基线的策略梯度定理"><a href="#1-3、带基线的策略梯度定理" class="headerlink" title="1.3、带基线的策略梯度定理"></a>1.3、带基线的策略梯度定理</h5><p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>= \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot \left(Q_{\pi}(s_t,a_t)-b\right)\right]<br>\end{align}$$</p>
<p>其中 $b$是任意函数，且不依赖 $a_t$, 要证明上述定理只需证明</p>
<p>$$\begin{align}<br>\mathbb{E}_{a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t)\cdot b\right] = 0<br>\end{align}$$</p>
<p>实际上有 $b$不依赖于 $a_t$可以把 $b$提取到期望外面</p>
<p>$$\begin{align}<br>\mathbb{E}_{a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t)\cdot b\right]<br>&amp;= b\cdot \mathbb{E}_{a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t)\right] \\<br>&amp;=b\cdot \sum_{a_t\in \mathcal{A}}\big[\pi_{\bm{\theta}}(a_t\mid s_t)\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t)\big]\\<br>&amp;=b\cdot \sum_{a_t\in \mathcal{A}}\bigg[\pi_{\bm{\theta}}(a_t\mid s_t)\cdot\frac{1}{\pi_{\bm{\theta}}(a_t\mid s_t)}\cdot\frac{\partial \pi_{\bm{\theta}}(a_t\mid s_t)}{\partial \bm{\theta}}\bigg]\\<br>&amp;=b\cdot  \sum_{a_t\in \mathcal{A}}\bigg[\frac{\partial \pi_{\bm{\theta}}(a_t\mid s_t)}{\partial \bm{\theta}}\bigg] \\<br>&amp; = b\cdot \frac{\partial }{\partial \bm{\theta}}\sum_{a_t\in \mathcal{A}}\pi_{\bm{\theta}}(a_t\mid s_t)\\<br>&amp; = b\cdot \frac{\partial 1}{\partial \bm{\theta}}\\<br>&amp; =0<br>\end{align}$$</p>
<p>这就证明了定理。不妨设 $\displaystyle b=V_{\pi}(s_t)$，这样有策略梯度</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>= \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\left[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot \left(Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)\right)\right]<br>\end{align}$$</p>
<p>而 $Q_{\pi}(s_t,a_t)-V_{\pi}(s_t)$实际上是优势函数 $A_{\pi}(s_t,a_t)$，这样有如下策略梯度</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>= \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\Big[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot A_{\pi}(s_t,a_t)\Big]<br>\end{align}$$</p>
<p>这个策略梯度是各类RLHF(PPO,DPO,GRPO)的基础。当然看到这里好事者可能会问，为什么要引入优势函数，策略梯度定理如何证明等。那这里笔者就提示一下：<br>1、策略梯度定理证明稍微复杂，且证明技巧于实际应用比较远。笔者会在附录中介绍。<br>2、引入优势函数是减小方差、加速收敛。当然还有其他优点，这些笔者在后面会慢慢细说</p>
<p>最后笔者说说自己的心得，在这个技术高度集成的年代，理论前沿和应用前沿大多依赖很多底层基础知识。有些是应该弄明白的，对继续探索和实践是有益的。例如强化学习基础知识和策略梯度定理。但是有些其实可以放一放，没有必要深究。如策略梯度定理的证明，这就犹如神经网络的万能近似定理的证明，其实对你继续探索和实践深度学习影响甚微。此时如不能抑制自己的好奇心，非要搞清楚，你将会浪费大量时间和精力在非核心问题上。所以请记住<strong>适当抑制好奇心，才能走的更远</strong>。</p>
<h5 id="1-4、GAE广义优势估计与价值网络学习"><a href="#1-4、GAE广义优势估计与价值网络学习" class="headerlink" title="1.4、GAE广义优势估计与价值网络学习"></a>1.4、GAE广义优势估计与价值网络学习</h5><p>这节我们将讨论使用GAE广义优势估计[^2]来计算优势函数， $A_{\pi}(s_t,a_t) = Q_{\pi}(s_t,a_t) - V_{\pi}(s_t) $ 并于价值网络学习联系起来。</p>
<p>回顾一下动作价值函数的贝尔曼方程</p>
<p>$$\begin{align}<br>Q_{\pi}(a_t\mid s_t)=\mathbb{E}_{S_{t+1}\sim \rho}\bigg[R_t+\gamma V_{\pi}(S_{t+1})\mid S_t=s_t,A_t=a_t\bigg]<br>\end{align}$$<br>那么有</p>
<p>$$\begin{align}<br>A_{\pi}(s_t,a_t) = \mathbb{E}_{S_{t+1}\sim \rho}\bigg[R_t+\gamma V_{\pi}(S_{t+1})\mid S_t=s_t,A_t=a_t\bigg] - V_{\pi}(s_t)<br>\end{align}$$<br>给定状态 $s_t$，智能体执行动作 $a_t$，环境会给出奖励 $r_t$和新的状态 $s_{t+1}$, 用观测到的 $r_t$、 $s_{t+1}$对期望做蒙特卡洛近似, 并用价值网络 $v_{\bm{w}}(s_t)$替换价值函数 $V_{\pi}(s_t)$，这样有优势函数的估计值:</p>
<p>$$\begin{align}<br>\hat{A}_{\pi}(s_t,a_t)=r_t + \gamma v_{\bm{w}}(s_{t+1})-v_{\bm{w}}(s_t)<br>\end{align}$$</p>
<p>下面将目光注意到如何学习价值网络$v_{\bm{w}}(s_t)$，对状态价值函数有贝尔曼方程：</p>
<p>$$\begin{align}<br>V_{\pi}(s_t) = \mathbb{E}_{A_t\sim \pi}\bigg[\mathbb{E}_{S_{t+1}\sim \rho}\bigg[R_t+\gamma V_{\pi}(S_{t+1})\bigg]\bigg]<br>\end{align}$$</p>
<p>同样对期望做近似，具体就是在叙述一遍：给定状态 $s_t$，智能体执行动作 $a_t$，环境会给出奖励 $r_t$和新的状态 $s_{t+1}$, 用观测到的 $r_t$、 $s_{t+1}$对期望做蒙特卡洛近似, 并用价值网络 $v_{\bm{w}}(s_t)$替换价值函数 $V_{\pi}(s_t)$，这样有价值函数的估计值:</p>
<p>$$\begin{align}<br>\mathrm{\text{TD_Target}}_{k=1}:= \hat{y}_t = r_t + \gamma v_{\bm{w}}(s_{t+1})<br>\end{align}$$</p>
<p>又叫一步时间差分目标。如果对强化学习不熟悉，可能对反复出现的贝尔曼方程、TD(temporal difference,时间差分)等概念不太熟悉，这里稍微做点解释。希望给没有强化学习背景的同学减少一点困恼。<br>1、 $v_{\bm{w}}(s_t)$是价值网络值时刻 $t$做出的预测，其他没有任何事实成分<br>2、对于上式的一步时间差分目标是价值网络在时刻 $t+1$做出的预测，它部分基于真实观测的奖励 $r_t$。$\hat{y}_t$和 $v_{\bm{w}}(s_t)$都是对状态价值函数的估计，但 $\hat{y}_t$是部分基于事实的，因此比 $v_{\bm{w}}(s_t)$更加可信，应该鼓励 $v_{\bm{w}}(s_t)$接近 $\hat{y}_t$, 这样可以定义损失函数</p>
<p>$$\begin{align}<br>\ell(\bm{w}) = \frac{1}{2}\bigg[\hat{y}_t-v_{\bm{w}}(s_t)\bigg]^2<br>\end{align}$$<br>有梯度</p>
<p>$$\begin{align}<br>\nabla_{\bm{w}}\ell(\bm{w}) = \big[\underbrace{\hat{y}_t-v_{\bm{w}}(s_t)}_{\text{TD_Error}:\delta_t}\big]\nabla_{\bm{w}}v_{\bm{w}}(s_t)<br>\end{align}$$</p>
<p>称 $\hat{y}_t-v_{\bm{w}}(s_t)$为TD误差，或者叫时刻 $t$的一步时间差分误差 $\delta_t$:</p>
<p>$$\begin{align}<br>\mathrm{\text{TD_Error}}_{k=1}=\delta_t = \hat{y}_t-v_{\bm{w}}(s_t) = r_t + \gamma v_{\bm{w}}(s_{t+1}) - v_{\bm{w}}(s_t)<br>\end{align}$$</p>
<p>可以看到TD误差其实和优势函数的估计是一样的，对于多步时间差分误差，也会有多步优势函数估计：</p>
<p>$$\begin{align}<br>\hat{A}(k=1)=\mathrm{\text{TD_Error}}_{k=1}<br>&amp;= r_t + \gamma v_{\bm{w}}(s_{t+1}) - v_{\bm{w}}(s_t)<br>= \delta_t\\<br>\hat{A}(k=2)=\mathrm{\text{TD_Error}}_{k=2}<br>&amp;= r_t + \gamma \bigg[r_{t+1} + \gamma v_{\bm{w}}(s_{t+2})\bigg] - v_{\bm{w}}(s_t)\\<br>&amp;= r_t + \gamma v_{\bm{w}}(s_{t+1}) - v_{\bm{w}}(s_t) + \gamma \bigg[r_{t+1} + \gamma v_{\bm{w}}(s_{t+2}) - v_{\bm{w}}(s_{t+1})\bigg] \\<br>&amp;= \delta_t + \gamma \delta_{t+1}\\<br>\hat{A}(k=3)=\mathrm{\text{TD_Error}}_{k=3}<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2}\\<br>&amp;\vdots\\<br>\hat{A}(k)=\mathrm{\text{TD_Error}}_{k}<br>&amp;= \sum_{i=1}^{k}\gamma^{i-1}\delta_{t+i-1}\\<br>&amp;\vdots\\<br>\hat{A}(k=\infty)=\mathrm{\text{TD_Error}}_{\infty}<br>&amp;= \sum_{i=1}^{\infty}\gamma^{i-1}\delta_{t+i-1}<br>\end{align}$$</p>
<p>所谓多步TD误差其实就是尽可能用多步观测到的值去估计价值函数。从而可以降低估计的偏差和误差。通常为了平衡偏差和误差，会引入一个超参数 $\lambda\in [0,1]$, 令</p>
<p>$$\begin{align}<br>\hat{A}_t(k)=\mathrm{\text{TD_Error}}_{k}<br>&amp;= \sum_{i=1}^{k}\big(\gamma\lambda\big)^{i-1}\delta_{t+i-1}\\<br>&amp;=\delta_t + \gamma\lambda\bigg[\delta_{t+1}+\big(\gamma\lambda\big)\delta_{t+2}+\cdots +\big(\gamma\lambda\big)^{k-2}\delta_{t+k-1} \bigg]\\<br>&amp; = \delta_t + \gamma\lambda\hat{A}_{t+1}(k-1)<br>\end{align}$$</p>
<p>注意一下边界条件，当轨迹在 $T$时刻终止 后续状态价值为 $V_{\pi}(s_{T+1})=0$，在价值函数学习代码的实现中，一般会先递归计算优势函数估计。然后使用如下公式计算TD目标。</p>
<p>$$\begin{align}<br>\hat{y}_t=\mathrm{\text{TD}}_{k} = \hat{A}_t(k) +  v_{\bm{w}}(s_t)<br>\end{align}$$</p>
<p>当然如果对概率论缺乏直觉的朋友，可能一下子无法理解这种做法为啥能平衡偏差和误差。思考如下问题：求一组随机变量和的均值，那么如果我观测到其中 $k$的随机变量的值，替换对应的随机变量求均值，那么均值的方差和偏差如何变化？，不妨设这组随机变量是独立同分布的高斯分布 $x_i\sim \mathcal{N}(\mu,\sigma^2)$， $s_n= \frac{1}{n}\sum_{i=1}^n x_i$</p>
<p>如果我们没有观测到任何值<br>$$\begin{align}<br>\mathbb{E}[s_n] = \mu,\mathrm{var}[s_n] = \frac{\sigma^2}{n}<br>\end{align}$$</p>
<p>如果观测到k组值,通常假设观测存在偏差 $\mathring{x}_i=\mu+\delta_i$</p>
<p>那么均值有<br>$$\begin{align}<br>\mathbb{E}[s_n] = \frac{1}{n}\big[\sum_i^k \mathring{x}_i +(n-k)\mu\big] = \mu+ \frac{\sum_i^k\delta_i}{n} \geqslant \mu<br>\end{align}$$</p>
<p>那么误差有</p>
<p>$$\begin{align}<br>\mathrm{var}[s_n] = \frac{(n-k)\sigma^2}{n^2} \leqslant  \frac{\sigma^2}{n}<br>\end{align}$$</p>
<p>也就是说方差变小，偏差变大。如果我们给观测值加一个超参数 $\lambda\in [0,1]$，$\mathring{x}_i=\lambda\mu+\lambda\delta_i$, 来平衡偏差和误差</p>
<p>那么均值有</p>
<p>$$\begin{align}<br>\mathbb{E}s_n(\lambda)<br>=\frac{1}{n}\big[\sum_i^k \mathring{x}_i +(n-k)\mu\big]<br>=\frac{n-k(1-\lambda)}{n}\mu+ \frac{\sum_i^k \lambda\delta_i}{n}<br>\leqslant \mu+ \frac{\sum_i^k\delta_i}{n}<br>\end{align}$$</p>
<p>相信通过这个例子，应该会有所感觉。</p>
<h4 id="二、PPO近端策略优化"><a href="#二、PPO近端策略优化" class="headerlink" title="二、PPO近端策略优化"></a>二、PPO近端策略优化</h4><h5 id="2-1、代理目标函数"><a href="#2-1、代理目标函数" class="headerlink" title="2.1、代理目标函数"></a>2.1、代理目标函数</h5><p>网络上很多文章，写的多有错误，本文笔者将尽可能用清晰的数学符号，澄清各类问题。经过前期铺垫，终于可以谈论PPO了。为了使用自动微分，不太可能使用强化学习的原始目标函数 $\mathcal{J}(\bm{\theta}) = \mathbb{E}_{s\sim \rho}[V_{\pi}(s)]$, 因为需要对个步骤对状态和动作求积分或求和，这是非常困难的，这里引入了一个代理目标函数(surrogate objective)，只要确保代理目标函数的梯度和原始目标函数一致即可。回顾一下策略梯度</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>= \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\Big[\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot A_{\pi}(s_t,a_t)\Big]<br>\end{align}$$</p>
<p>那么设定这样的代理目标函数是显而易见的：</p>
<p>$$\begin{align}<br>\mathcal{L}^{PG}(\bm{\theta}) = \mathbb{E}_{s_t\sim \rho,a_t\sim \pi}\Big[\log\pi_{\bm{\theta}}(a_t\mid s_t) \cdot A_{\pi}(s_t,a_t)\Big]<br>\end{align}$$</p>
<p>也就说可一灵活设计目标函数，只要确保该目标函数的梯度和策略梯度定理一致即可。这也是经常看到论文和文章中总是提及策略梯度定理原因。因为只要保证如下公式</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}} \mathcal{L}^{PG}(\bm{\theta})<br>=\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})<br>\end{align}$$</p>
<p>优化方向上就能在理论上保证和原始的强化学习目标是一致的。然后实践证明这个最明显的目标函数表现并不好。PPO论文[^3]使用了TRPO(trus region policy optimuization,置信域策略优化中提出的函数)，回到原始目标函数</p>
<p>$$\begin{align}<br>\mathcal{J}(\bm{\theta})<br>&amp;= \mathbb{E}_{s_t\sim \rho}[V_{\pi}(s_t)]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho}\bigg[\mathbb{E}_{a_t\sim \pi}\big[Q_{\pi}(s_t, a_t)\big]\bigg]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho}\bigg[\sum_{a_t\in \mathcal{A}}\pi_{\bm{\theta}}(a_t\mid s_t)\cdot Q_{\pi}(s_t, a_t)\bigg]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho}\bigg[\sum_{a_t\in \mathcal{A}}\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)\frac{\pi_{\bm{\theta}}(a_t\mid s_t)}{\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)}\cdot Q_{\pi}(s_t, a_t)\bigg]\\<br>&amp; = \mathbb{E}_{s_t\sim \rho,a_t\sim \pi_{\bm{\theta}_\text{old}}}\bigg[\frac{\pi_{\bm{\theta}}(a_t\mid s_t)}{\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)}\cdot Q_{\pi}(s_t, a_t)\bigg]\\<br>\end{align}$$</p>
<p>使用优势函数$A_{\pi}(s_t,a_t) = Q_{\pi}(s_t,a_t) - V_{\pi}(s_t) $ ，在这个基础上PPO提出了裁剪版的代理目标函数</p>
<p>$$\begin{align}<br>\mathcal{L}^{\text{CLIP}}(\bm{\theta})<br>=\mathbb{E}_{s_t\sim \rho,a_t\sim \pi_{\bm{\theta}_\text{old}}}\bigg[\min\left[\frac{\pi_{\bm{\theta}}(a_t\mid s_t)}{\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(a_t\mid s_t)}{\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(s_t,a_t)\bigg]<br>\end{align}$$</p>
<p>其中令 $\displaystyle \eta_t(\bm{\theta})=\frac{\pi_{\bm{\theta}}(a_t\mid s_t)}{\pi_{\bm{\theta}_\text{old}}(a_t\mid s_t)}$</p>
<p>$$\begin{align}<br>\mathcal{L}^{\text{CLIP}}(\bm{\theta})<br>=\mathbb{E}_{s_t\sim \rho,a_t\sim \pi_{\bm{\theta}_\text{old}}}\bigg[\min\big[\eta_t(\bm{\theta}),\mathrm{\text{clip}}[\eta_t(\bm{\theta}),1-\epsilon,1+\epsilon]\big]\cdot A_{\pi}(s_t,a_t)\bigg]<br>\end{align}$$</p>
<p>用优势函数替代动作价值函数, 同时引入clip。有诸多的好处：</p>
<ul>
<li>1、已经证明这个操作不会改变梯度的期望值，当通过减小 $Q_{\pi}(s_t,a_t)$的绝对波动幅度，降低了方差。</li>
<li>2、优势函数提供了更加明确的优化信号，因为优势函数衡量的是动作的相对好坏，使得策略更新方向更清晰，相比于使用动作价值函数，优势函数提供的梯度信号更稳定，减少了噪声干扰，从而加速收敛。</li>
<li>3、与广义优势估计（GAE）结合，通过多步TD残差的加权和计算优势函数的估计。GAE平衡了蒙特卡洛方法（低偏差、高方差）和单步TD方法（高偏差、低方差），进一步优化了方差-偏差权衡，提升收敛效率。</li>
<li>4、优势函数通过相对价值调整，而非依赖绝对回报，使得策略更新的幅度更加合理。结合PPO的裁剪机制（Clip机制），可限制策略更新的步长，避免因单次更新过大导致的策略崩溃。</li>
<li>5、优势函数结合GAE，充分利用多步回报信息，减少对单一轨迹片段的依赖。这使得算法在相同样本量下能提取更多有效信息，提升数据利用率。</li>
<li>6、优势函数指导策略优先选择高优势动作（利用），但PPO的熵正则项（Entropy Bonus）可保留一定随机性（探索）。两者结合避免了策略过早陷入局部最优。注意上面的公式，没有加入熵正则。</li>
<li>7、优势函数依赖状态价值函数的估计，这要求Critic网络准确预测状态价值。PPO通过联合优化策略网络（Actor）和价值网络（Critic），使两者相互促进：Critic为Actor提供低方差梯度，Actor生成的数据帮助Critic更准确估计价值。</li>
<li>8、在稀疏奖励任务中，绝对回报可能长期为零或变化微小，而优势函数通过比较动作的相对价值，仍能提供有效的梯度信号，帮助策略在早期阶段逐步改进。</li>
</ul>
<h5 id="2-2、大语言模型场景"><a href="#2-2、大语言模型场景" class="headerlink" title="2.2、大语言模型场景"></a>2.2、大语言模型场景</h5><p>上述分析都是在强化学习的状态动作场景。这里将转化为大模型场景。在LLM里，状态是每个token的之前的token。一般我们会从提示词或者问题 $q$出发，迭代输出每步动作，也就是token $o_t$</p>
<p>$$\begin{align}<br>\mathbb{E}_{s_t\sim \rho,a_t\sim \pi_{\bm{\theta}_\text{old}}}\to<br>\mathbb{E}_{q\sim \rho(Q),o_t\sim \pi_{\bm{\theta}_\text{old}}}<br>\end{align}$$</p>
<p>也就是说有问题对数据集 $\displaystyle \mathcal{D}=\{(q_i,o_1 \cdots o_{t_i})\}_{i=1}^n$</p>
<p>$$\begin{align}<br>\mathcal{L}^{\text{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho(Q),o_t\sim \pi_{\bm{\theta}_\text{old}}}\bigg[\min\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,o_t)\bigg]<br>\end{align}$$</p>
<h5 id="2-3、PPO中的KL散度"><a href="#2-3、PPO中的KL散度" class="headerlink" title="2.3、PPO中的KL散度"></a>2.3、PPO中的KL散度</h5><p>为减轻奖励模型的过度优化，PPO还会引入参考模型的 $\text{KL}$散度,在计算每个token奖励时，减去 $\{o_1,\cdots,o_t\}$上的散度,</p>
<p>$$\begin{align}<br>r_t=\pi_{\bm{\theta}_{\textit{rm}}}(q,o_{:t})-\beta \mathbb{KL}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]<br>\end{align}$$</p>
<p>其中 </p>
<p>$\pi_{\bm{\theta}_{\textit{rm}}}$是奖励模型(reward model)， $\displaystyle \mathbb{KL}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big] = \log \frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\textit{ref}}(o_t\mid q,o_{:t})}$</p>
<p>在这个地方，肯定有人会问为啥是把这个所谓的KL在计算token奖励时加入，而不是基于正则的方式加入到损失函数或者目标函数里面：</p>
<p>$$\begin{align}<br>\mathcal{L}^{\text{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho,o_t\sim \pi_{\bm{\theta}_\text{old}}}\bigg[\min\left[\frac{\pi_{\bm{\theta}}(o_t,\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,o_t)\bigg]-\beta \mathbb{\hat{KL}}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]<br>\end{align}$$</p>
<p>这样才是更加常规做法。奈何GPT2，GPT3相关论文代码确实是这么写的，可以参考知乎上的讨论<a href="https://www.zhihu.com/question/629107126" target="_blank" rel="noopener">在强化学习 PPO 算法中，为什么可以把 KL 散度直接放进负奖励？</a></p>
<p>PPO技术要点差不多已经理清，这里再全局回顾一下：<br>1、有两个可以训练的模型:策略模型 $\pi_{\bm{\theta}}$、 价值模型$v_{\bm{w}}$<br>2、有一已经训练好的奖励模型 $\pi_{\bm{\theta}_{\textit{rm}}}$<br>3、还有一个冻结参数的参考模型 $\pi_{\bm{\theta}_\textit{ref}}$</p>
<p>deepseek math的论文图画的很好，笔者略微修改一下，并添加注释，以便更加清晰理解：</p>
<div align="center"><img src="/images/强化学习2.png" width="600" ==""><br></div>

<h4 id="三、GRPO组相对策略优化"><a href="#三、GRPO组相对策略优化" class="headerlink" title="三、GRPO组相对策略优化"></a>三、GRPO组相对策略优化</h4><h5 id="3-1、GRPO的目标函数"><a href="#3-1、GRPO的目标函数" class="headerlink" title="3.1、GRPO的目标函数"></a>3.1、GRPO的目标函数</h5><p>注意到PPO中有四个模型，实属非常浪费资源。GRPO[^4]提出了更加节约资源的版本。对于PPO，deepseek math给出了更加明晰的公式, 适合照着公式写代码。明确加入了 $\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}$，对于一个问题 $q$的输出 $\bm{o}=[o_1,o_2,\cdots,o_t,\cdots,o_T]$, 明确了处理方式。之前的目标函数代码实现的时候默认如此，但是并没有显示的写出来。</p>
<p>$$\begin{align}<br>\mathcal{L}_{\textit{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\bigg[\min\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,o_t)\bigg]<br>\end{align}$$</p>
<p>deepseek math论文提出了，GRPO组相对策略优化,目标函数是</p>
<p>$$\begin{align}<br>&amp;\mathcal{L}_{\textit{GRPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\{\bm{o}_i\}_{i=1}^G\sim \pi_{\bm{\theta}_\text{old}}}\\<br>&amp;\frac{1}{G}\sum_{i=1}^G\frac{1}{|\bm{o}_i|}\sum_{t=1}^{|\bm{o}_i|}<br>\bigg[\min\left[\frac{\pi_{\bm{\theta}}(o_{i,t}\mid q,o_{i,:t})}{\pi_{\bm{\theta}_\text{old}}(o_{i,t}\mid q,o_{i,:t})},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(o_{i,t}\mid q,o_{i,:t})}{\pi_{\bm{\theta}_\text{old}}(o_{i,t}\mid q,o_{i,:t})},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,o_{i,t})-\beta \mathbb{\hat{KL}}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]\bigg]<br>\end{align}$$</p>
<h5 id="3-2、GRPO的KL散度"><a href="#3-2、GRPO的KL散度" class="headerlink" title="3.2、GRPO的KL散度"></a>3.2、GRPO的KL散度</h5><p>对于 $\text{KL}$散度的计算实际代码实现是通过蒙特卡洛估计来实现的，这也是为什么KL项在期望符号里面的原因。实际上这样的。</p>
<p>$$\begin{align}<br>\mathbb{\hat{KL}}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big] =\frac{\pi_{\bm{\theta}_\textit{ref}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}- \log \frac{\pi_{\bm{\theta}_\textit{ref}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}-1<br>\end{align}$$</p>
<p>为何使用这个估计，这篇博文写了如何对KL散度做蒙特卡洛近似，能实现偏差与方差平衡：<a href="https://joschu.net/blog/kl-approx.html" target="_blank" rel="noopener">https://joschu.net/blog/kl-approx.html</a>[^5]，这里啰嗦几句。KL通常是基于分布整体来计算的：</p>
<p>$$\begin{align}<br>\mathbb{KL}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]<br>=\sum_{o_t \in \mathcal{Vocab}}\pi_{\bm{\theta}} \log \frac{\pi_{\bm{\theta}}}{\pi_{\bm{\theta}_\textit{ref}}}<br>\end{align}$$</p>
<p>这里笔者稍微做点解释，做蒙特卡洛模拟最简单就是使用 $\displaystyle \log \frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\textit{ref}}(o_t\mid q,o_{:t})}$来做估计。但是这个是估计的方差比较大。因为计算 $log(\cdot)$会出现负值，而 $\mathbb{KL}$是非负的。要减少方差需要添加一个与 $\log \frac{\pi_{\bm{\theta}}}{\pi_{\bm{\theta}_\textit{ref}}}$变化方向相反，且均值为零的量。 $\displaystyle \frac{\pi_{\bm{\theta}_\textit{ref}}}{\pi_{\bm{\theta}}}-1$似乎是一个不错的选择。而且 $\log(x)\leqslant x-1$,这样估计量 $\displaystyle \log \frac{\pi_{\bm{\theta}}}{\pi_{\bm{\theta}_\textit{ref}}}+\frac{\pi_{\bm{\theta}_\textit{ref}}}{\pi_{\bm{\theta}}}-1$始终是非负。具体可以参见<a href="https://joschu.net/blog/kl-approx.html" target="_blank" rel="noopener">博文</a>中的关于Bregman距离的论述。</p>
<h5 id="3-3、GRPO的优势函数"><a href="#3-3、GRPO的优势函数" class="headerlink" title="3.3、GRPO的优势函数"></a>3.3、GRPO的优势函数</h5><p>对于优势函数的估计体现了GRPO组相对策略优化这个名字的含义, 在GRPO中没有价值模型，而是使用同一个问题的一组输出数据 $\{\{q,\bm{o}_i\}\}_{i=1}^G$，来计算相对优势。</p>
<ul>
<li>1、对于过程监督(process supervision)，也就是说同一个问题 $q$个一组输出中的每个token的优势估计是这样计算的：</li>
</ul>
<p>$$\begin{align}<br>\hat{A}_{i,t}=\hat{A}_{\pi}(q,o_{i,t}) = \frac{r(o_{i,t})-\text{mean}(\bm{R})}{\text{std}(\bm{R})}<br>\end{align}$$</p>
<p>其中 $\displaystyle \bm{R} = \{\{r(o_{1,1}),\cdots r(o_{1,t})\}_1,\cdots,\{r(o_{i,1}),\cdots r(o_{i,t})\}_i,\cdots,\{r(o_{G,1}),\cdots r(o_{G,t})\}_G\}$</p>
<ul>
<li>2、对于结果监督(outcome supervison)有, 也就是说同一个问题 $q$个的一个输出中的每个token的优势估计相同的：</li>
</ul>
<p>$$\begin{align}<br>\hat{A}_{i,t}=\hat{A}_{\pi}(q,\bm{o}_{i}) = \frac{r(\bm{o}_{i})-\text{mean}(\bm{R})}{\text{std}(\bm{R})}<br>\end{align}$$</p>
<p>其中 $\displaystyle \bm{R} = \{r(\bm{o}_1),\cdots,r(\bm{o}_i),\cdots,r(\bm{o}_G)\}$</p>
<h5 id="3-4、deepseek-R1模型的目标函数"><a href="#3-4、deepseek-R1模型的目标函数" class="headerlink" title="3.4、deepseek R1模型的目标函数"></a>3.4、deepseek R1模型的目标函数</h5><p>deepseek R1模型的目标函数[^1]和deepseek math中的目标函数稍微不太一样。deepseek的思维链模型，没有使用过程奖励。目标函数中可以看到是整体计算整个输出。而且R1除了没有价值模型，还没有奖励模型。奖励的计算基于规则、代码验证器、格式验证来计算的。这大大节约了资源，也减少了学习难度。</p>
<p>$$\begin{align}<br>&amp;\mathcal{L}_{\textit{GRPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\{\bm{o}_i\}_{i=1}^G\sim \pi_{\bm{\theta}_\text{old}}}\\<br>&amp;\frac{1}{G}\sum_{i=1}^G<br>\bigg[\min\left[\frac{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}_\text{old}}(\bm{o}_{i}\mid q)},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}_\text{old}}(\bm{o}_{i}\mid q)},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,\bm{o}_i)-\beta \mathbb{\hat{KL}}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]\bigg]<br>\end{align}$$</p>
<p>其中<br>$\displaystyle \hat{A}_{i}=\hat{A}_{\pi}(q,\bm{o}_{i}) = \frac{r(\bm{o}_{i})-\text{mean}(\bm{R})}{\text{std}(\bm{R})}$</p>
<p>$\displaystyle \mathbb{\hat{KL}}\big[\pi_{\bm{\theta}} ||\pi_{\bm{\theta}_\textit{ref}}\big]<br>=\frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}- \log \frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}-1$</p>
<p>$\displaystyle \pi_{\bm{\theta}}(\bm{o}_{i}\mid q) = \prod_{t=1}^T\pi_{\bm{\theta}}(o_{i,t}\mid q,o_{i,:t})$</p>
<div align="center"><img src="/images/强化学习1.png" width="600" ==""><br></div>

<h4 id="四、大语言模型中强化学习的统一范式"><a href="#四、大语言模型中强化学习的统一范式" class="headerlink" title="四、大语言模型中强化学习的统一范式"></a>四、大语言模型中强化学习的统一范式</h4><p>deepseek math论文中对各类RLHF(SFT、RFT、DPO、PPO、GRPO)提出了一个统一强化学习框架，下面是统一的策略梯度公式：</p>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{J}_{\mathcal{\textcolor{red}{A}}}<br>=\underbrace{\mathbb{E}_{(q,o)\sim \textcolor{red}{\mathcal{D}}}}_{\textit{Data Source}}<br>\Bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\underbrace{\textit{GC}_{\mathcal{A}}(q,o,r,\textcolor{red}{\pi_{\bm{\theta}_\textit{rm}}}}_{\textit{Gradient Coefficient}})\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\Bigg]<br>\end{align}$$</p>
<p>有三个关键组件<br>1、 Data Source $\mathcal{D}$ 它决定了训练数据<br>2、 Reward Function $\pi_{\bm{\theta}_\textit{rm}}$ 它是训练奖励信号的来源<br>3、 Algorithm $\mathcal{A}$ 它处理了训练数据和梯度系数(Gradient Coefficient)中的奖励信号，生成决定数据的惩罚项或强化幅度。</p>
<p>下面来一一考察：</p>
<h5 id="4-1、SFT"><a href="#4-1、SFT" class="headerlink" title="4.1、SFT"></a>4.1、SFT</h5><p>监督微调(Supervised Fine-tuning)</p>
<ul>
<li><p>目标函数<br>$$\begin{align}<br>\mathcal{L}_{\textit{SFT}}(\bm{\theta})<br>=\mathbb{E}_{q,\bm{o}\sim \rho(Q,O)}<br>\bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\bigg]<br>\end{align}$$</p>
</li>
<li><p>数据集<br>在问题对数据集中抽样, 其中 $q$是问题， $\bm{o}=[o_1,\cdots,o_{|\bm{o}|}]$是回答的每个token。</p>
</li>
</ul>
<p>$$\begin{align}<br>\mathcal{D}=\{(q_i,\bm{o}_i)\}_{i=1}^n<br>\end{align}$$</p>
<ul>
<li><p>梯度<br>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{L}_{\textit{SFT}}(\bm{\theta})<br>=\mathbb{E}_{q,\bm{o}\sim \rho(Q,O)}<br>\bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\bigg]<br>\end{align}$$</p>
</li>
<li><p>奖励函数<br>$$\begin{align}<br>\pi_{\bm{\theta}_\textit{rm}}=1<br>\end{align}$$</p>
</li>
<li><p>梯度系数<br>$$\begin{align}<br>\textit{GC}_{\textit{SFT}}(q,o,r,\pi_{\bm{\theta}_\textit{rm}})=1<br>\end{align}$$</p>
</li>
</ul>
<h5 id="4-2、RFT"><a href="#4-2、RFT" class="headerlink" title="4.2、RFT"></a>4.2、RFT</h5><p>决绝采样微调(Rejection Sampling Fine-tuning)</p>
<ul>
<li><p>目标函数<br>$$\begin{align}<br>\mathcal{L}_{\textit{RFT}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\textit{sft}}}<br>\bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}<br>\mathbb{I}(\bm{o})<br>\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\bigg]<br>\end{align}$$</p>
</li>
<li><p>数据集<br>RFT是在已经训练好的SFT模型 $ \pi_{\textit{sft}}$上, 对问题 $q$生成多个回答，并标注正确和错误的回答。$\bm{o}=[o_1,\cdots,o_{|\bm{o}|}]$是回答的每个token。</p>
</li>
</ul>
<p>$$\begin{align}<br>\mathcal{D}=\{(q_i,\bm{o}_i^{True}\,or\,\bm{o}_i^{False})\}_{i=1}^n<br>\end{align}$$</p>
<ul>
<li>梯度</li>
</ul>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{L}_{\textit{RFT}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\textit{sft}}}<br>\bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}<br>\mathbb{I}(\bm{o})<br>\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\bigg]<br>\end{align}$$</p>
<ul>
<li><p>奖励函数<br>$$\begin{align}<br>\pi_{\bm{\theta}_\textit{rm}}=1<br>\end{align}$$</p>
</li>
<li><p>梯度系数<br>$$\begin{align}<br>\textit{GC}_{\textit{RFT}}(q,o,r,\pi_{\bm{\theta}_\textit{rm}})=\mathbb{I}(\bm{o})=\begin{cases}1&amp;\text{if } \bm{o}=True\\0&amp;\text{if } \bm{o}=False \end{cases}<br>\end{align}$$</p>
</li>
</ul>
<h5 id="4-3、Online-RFT"><a href="#4-3、Online-RFT" class="headerlink" title="4.3、Online RFT"></a>4.3、Online RFT</h5><p>在线决绝采样微调(Online Rejection Sampling Fine-tuning)和RFT的区别，在于数据会通过实时策略模型 $\pi_{\bm{\theta}}$生成。而不是离线已经训练好的 $\pi_{\textit{sft}}$模型。</p>
<ul>
<li>目标函数<br>$$\begin{align}<br>\mathcal{L}_{\textit{OnRFT}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\bm{\theta}}}<br>\bigg[\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}<br>\mathbb{I}(\bm{o})<br>\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\bigg]<br>\end{align}$$</li>
</ul>
<h5 id="4-4、DPO"><a href="#4-4、DPO" class="headerlink" title="4.4、DPO"></a>4.4、DPO</h5><p>直接偏好优化(Direct Preference Optimization)</p>
<ul>
<li><p>目标函数<br>$$\begin{align}<br>\mathcal{L}_{\textit{DPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\bm{o}^{+},\bm{o}^{-}\sim \pi_{\textit{sft}}}\left[<br>\log\sigma\bigg(<br>\beta\frac{1}{|\bm{o}^+|}\sum_{t=1}^{|\bm{o}^+|}<br>\log \frac{\pi_{\bm{\theta}}(o_t^+\mid q,o_{:t}^+)}{\pi_{\bm{\theta}_\textit{ref}}(o_t^+\mid q,o_{:t}^+)}<br>-\beta\frac{1}{|\bm{o}^-|}\sum_{t=1}^{|\bm{o}^-|}<br>\log \frac{\pi_{\bm{\theta}}(o_t^-\mid q,o_{:t}^-)}{\pi_{\bm{\theta}_\textit{ref}}(o_t^-\mid q,o_{:t}^-)}<br>\bigg)<br>\right]<br>\end{align}$$</p>
</li>
<li><p>数据集<br>DPO是是问题 $q$上的偏好数据，并标注接受回答 $+$和拒绝回答 $-$。$\bm{o}=[o_1,\cdots,o_{|\bm{o}|}]$是回答的每个token。</p>
</li>
</ul>
<p>$$\begin{align}<br>\mathcal{D}=\{(q_i,\bm{o}_i^{+}\,or\,\bm{o}_i^{-})\}_{i=1}^n<br>\end{align}$$</p>
<ul>
<li>梯度</li>
</ul>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{L}_{\textit{DPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\bm{o}^{+},\bm{o}^{-}\sim \pi_{\textit{sft}}}\left[<br>\beta\cdot\textit{GC}_{\textit{DPO}}(q,o,t)\bigg(<br>\frac{1}{|\bm{o}^+|}\sum_{t=1}^{|\bm{o}^+|}<br>\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t^+\mid q,o_{:t}^+)<br>-\frac{1}{|\bm{o}^-|}\sum_{t=1}^{|\bm{o}^-|}<br>\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t^-\mid q,o_{:t}^-)<br>\bigg)<br>\right]<br>\end{align}$$</p>
<ul>
<li><p>奖励函数<br>$$\begin{align}<br>\pi_{\bm{\theta}_\textit{rm}}=1<br>\end{align}$$</p>
</li>
<li><p>梯度系数<br>$$\begin{align}<br>\textit{GC}_{\textit{DPO}}(q,o,t)<br>=\sigma\bigg(<br>\beta<br>\log \frac{\pi_{\bm{\theta}}(o_t^+\mid q,o_{:t}^+)}{\pi_{\bm{\theta}_\textit{ref}}(o_t^+\mid q,o_{:t}^+)}<br>-\beta<br>\log \frac{\pi_{\bm{\theta}}(o_t^-\mid q,o_{:t}^-)}{\pi_{\bm{\theta}_\textit{ref}}(o_t^-\mid q,o_{:t}^-)}<br>\bigg)<br>\end{align}$$</p>
</li>
</ul>
<h5 id="4-5、PPO"><a href="#4-5、PPO" class="headerlink" title="4.5、PPO"></a>4.5、PPO</h5><p>近端策略优化(Proximal Policy Optimization)</p>
<ul>
<li>目标函数</li>
</ul>
<p>$$\begin{align}<br>\mathcal{L}_{\textit{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\bigg[\min\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},\mathrm{\text{clip}}\left[\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})},1-\epsilon,1+\epsilon\right]\right]\cdot A_{\pi}(q,o_t)\bigg]<br>\end{align}$$</p>
<p>为了简化分析，假定模型在每个探索阶段之后只有一个更新，从而确保 $\pi_{\textit{old}}=\pi_{\bm{\theta}}$,在这种情况下，删除最小值和裁剪操作有简化的目标函数</p>
<p>$$\begin{align}<br>\mathcal{L}_{\textit{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\bigg[<br>\frac{\pi_{\bm{\theta}}(o_t\mid q,o_{:t})}{\pi_{\bm{\theta}_\text{old}}(o_t\mid q,o_{:t})}\cdot A_{\pi}(q,o_t)<br>\bigg]<br>\end{align}$$</p>
<ul>
<li>数据集<br>PPO是问题 $q$上的探索数据轨迹(输出结果)，$\bm{o}=[o_1,\cdots,o_{|\bm{o}|}]$是回答的每个token。</li>
</ul>
<p>$$\begin{align}<br>\mathcal{D}=\{(q_i,\bm{o}_i)\}_{i=1}^n<br>\end{align}$$</p>
<ul>
<li><p>梯度系数<br>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{L}_{\textit{PPO}}(\bm{\theta})<br>&amp;=\mathbb{E}_{q\sim \rho(Q),\bm{o}\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{|\bm{o}|}\sum_{t=1}^{|\bm{o}|}\bigg[<br>\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(o_t\mid q,o_{:t})\cdot A_{\pi}(q,o_t)<br>\bigg]<br>\end{align}$$</p>
</li>
<li><p>奖励函数<br>奖励函数就是已经训练好的奖励模型<br>$$\begin{align}<br>\pi_{\bm{\theta}_\textit{rm}}<br>\end{align}$$</p>
</li>
<li><p>梯度系数<br>$$\begin{align}<br>\textit{GC}_{\textit{PPO}}(q,o,r,\pi_{\bm{\theta}_\textit{rm}},v_{\bm{w}})=A_t(q,o_t)<br>\end{align}$$</p>
</li>
</ul>
<h5 id="4-6、GRPO"><a href="#4-6、GRPO" class="headerlink" title="4.6、GRPO"></a>4.6、GRPO</h5><p>组相对策略优化(Group Relative Policy Optimization)</p>
<ul>
<li>目标函数</li>
</ul>
<p>和PPO一样简化，删除最小值和裁剪操作，有简化的目标函数:</p>
<p>$$\begin{align}<br>\mathcal{L}_{\textit{GRPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\{\bm{o}_i\}_{i=1}^G\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{G}\sum_{i=1}^G<br>\bigg[\frac{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}_\text{old}}(\bm{o}_{i}\mid q)}<br>\cdot A_{\pi}(q,\bm{o}_i)-\beta\left(\frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}- \log \frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}-1\right)\bigg]<br>\end{align}$$</p>
<ul>
<li>数据集<br>GRPO是在一个问题 $q$上的探索一组数据轨迹(输出结果)，$\bm{o}=[o_1,\cdots,o_{|\bm{o}|}]$是回答的每个token。</li>
</ul>
<p>$$\begin{align}<br>\mathcal{D}=\{(q_i,\{\bm{o}_{i,g}\}_{g=1}^G)\}_{i=1}^n<br>\end{align}$$</p>
<ul>
<li>梯度系数</li>
</ul>
<p>$$\begin{align}<br>\nabla_{\bm{\theta}}\mathcal{L}_{\textit{GRPO}}(\bm{\theta})<br>=\mathbb{E}_{q\sim \rho(Q),\{\bm{o}_i\}_{i=1}^G\sim \pi_{\bm{\theta}_\text{old}}}\frac{1}{G}\sum_{i=1}^G<br>\left[<br>\bigg[<br>A_{\pi}(q,\bm{o}_i)<br>+\beta\left(\frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}-1\right)<br>\bigg]<br>\nabla_{\bm{\theta}}\log \pi_{\bm{\theta}}(\bm{o}_{i}\mid q)<br>\right]<br>\end{align}$$</p>
<ul>
<li><p>奖励函数<br>奖励的计算基于规则、代码验证器、格式验证来计算的</p>
</li>
<li><p>梯度系数</p>
</li>
</ul>
<p>$$\begin{align}<br>\textit{GC}_{\textit{GRPO}}(q,o,r,\pi_{\bm{\theta}_\textit{rm}})<br>=A_{\pi}(q,\bm{o}_i)<br>+\beta\left(\frac{\pi_{\bm{\theta}_\textit{ref}}(\bm{o}_{i}\mid q)}{\pi_{\bm{\theta}}(\bm{o}_{i}\mid q)}-1\right)<br>\end{align}$$</p>
<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><p>[^1]: DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., et al. (2025, January 22). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv. <a href="https://doi.org/10.48550/arXiv.2501.12948" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.2501.12948</a></p>
<p>[^2]: Schulman, J., Moritz, P., Levine, S., Jordan, M., &amp; Abbeel, P. (2018, October 20). High-Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv. <a href="https://doi.org/10.48550/arXiv.1506.02438" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.1506.02438</a></p>
<p>[^3]: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017, August 28). Proximal Policy Optimization Algorithms. arXiv. <a href="https://doi.org/10.48550/arXiv.1707.06347" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.1707.06347</a></p>
<p>[^4]: Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., et al. (2024, April 27). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv. <a href="https://doi.org/10.48550/arXiv.2402.03300" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.2402.03300</a></p>
<p>[^5]: J. Schulman. Approximating kl divergence, 2020. URL <a href="http://joschu.net/blog/kl-approx.html" target="_blank" rel="noopener">http://joschu.net/blog/kl-approx.html</a>.</p>
<p><hr></p>
<p><table border="1" width="100%"><tr><td align="center" width="18%">版权声明</td><td align="left" width="82%"><img src="https://www.limoncc.com/images/cc.png" width="18%"></td></tr><tr><td align="center" width="18%"><img src="https://www.limoncc.com/images/avatar.png" width="100%"></td><td align="left" width="82%">由<a href="https://www.limoncc.com">引线小白</a>创作并维护的<a href="http://www.limoncc.com">柠檬CC</a>博客采用<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名-非商业-禁止演绎4.0</a>国际许可证。<br>本文首发于柠檬CC <a href="https://www.limoncc.com">[ https://www.limoncc.com ]</a> , 版权所有、侵权必究。</td></tr><tr><td align="center" width="18%">本文永久链接</td><td align="left" width="82%">httpss://<a href="http://www.limoncc.com/post/c0a3be9c86b2b4cd/">www.limoncc.com/post/c0a3be9c86b2b4cd/</a></td></tr></table><table border="0" width="100%" style="display:inline !important;font-size:14px !important;color:#555555;"><tr><td align="left" width="100%" style="font-family:helvetica;font-size:14px !important;font-weight:bold;">如果您需要引用本文，请参考：</td></tr><tr><td align="left" width="100%">引线小白. (Jan. 31, 2025). 《大模型中的强化学习——大语言模型研究05》[Blog post]. Retrieved from <a href="https://www.limoncc.com/post/c0a3be9c86b2b4cd">https://www.limoncc.com/post/c0a3be9c86b2b4cd</a></td></tr><tr><td align="left" width="100%">@online{limoncc-c0a3be9c86b2b4cd,<br>title={大模型中的强化学习——大语言模型研究05},<br>author={引线小白},<br>year={2025},<br>month={Jan},<br>date={31},<br>url={\url{<a href="https://www.limoncc.com/post/c0a3be9c86b2b4cd}}">https://www.limoncc.com/post/c0a3be9c86b2b4cd}}</a>,<br>}</td></tr></table><div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a href="#" class="bds_duitang" data-cmd="duitang" title="分享到堆糖"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a></div></p>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/生成模型/" rel="tag">#生成模型</a>
          
            <a href="/tags/大语言模型/" rel="tag">#大语言模型</a>
          
            <a href="/tags/LLM/" rel="tag">#LLM</a>
          
            <a href="/tags/强化学习/" rel="tag">#强化学习</a>
          
            <a href="/tags/RLFT/" rel="tag">#RLFT</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/post/d9321405ef13c11b/" rel="next" title="Bone微调,超越LoRA系列的高效微调方法——大语言模型研究04">
                <i class="fa fa-chevron-left"></i> Bone微调,超越LoRA系列的高效微调方法——大语言模型研究04
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


<!--           </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div> -->
        <div id="vcomments"></div>
        <script>new Valine({
          el: "#vcomments",
      appId: "BVjuNRCpkVSkz82jFmadIvY8-gzGzoHsz",
    appKey: "bRjXPp55dop7RTC2xgunGdiP"})
  </script>'
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="引线小白" />
          <p class="site-author-name" itemprop="name">引线小白</p>
          <p class="site-description motion-element" itemprop="description">小湖椰影廊桥,曾记否,谷围晓月,灯影朦胧。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">53</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">99</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="" target="_blank" title="Design">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Design
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://www.behance.net/limoncc" target="_blank" title="Behance">
                  
                    <i class="fa fa-fw fa-behance"></i>
                  
                  Behance
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://www.pinterest.com/aegeanfan/" target="_blank" title="Pinterest">
                  
                    <i class="fa fa-fw fa-pinterest"></i>
                  
                  Pinterest
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://github.com/limoncc" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="http://weibo.com/3483157951" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="http://www.zhihu.com/people/limoncc" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              设计不止，折腾不息。
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://chuangzaoshi.com" title="创造狮" target="_blank">创造狮</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、理论基础"><span class="nav-number">1.</span> <span class="nav-text">一、理论基础</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1、数学符号"><span class="nav-number">1.1.</span> <span class="nav-text">1.1、数学符号</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2、策略学习与策略梯度"><span class="nav-number">1.2.</span> <span class="nav-text">1.2、策略学习与策略梯度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3、带基线的策略梯度定理"><span class="nav-number">1.3.</span> <span class="nav-text">1.3、带基线的策略梯度定理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4、GAE广义优势估计与价值网络学习"><span class="nav-number">1.4.</span> <span class="nav-text">1.4、GAE广义优势估计与价值网络学习</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二、PPO近端策略优化"><span class="nav-number">2.</span> <span class="nav-text">二、PPO近端策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1、代理目标函数"><span class="nav-number">2.1.</span> <span class="nav-text">2.1、代理目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2、大语言模型场景"><span class="nav-number">2.2.</span> <span class="nav-text">2.2、大语言模型场景</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3、PPO中的KL散度"><span class="nav-number">2.3.</span> <span class="nav-text">2.3、PPO中的KL散度</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三、GRPO组相对策略优化"><span class="nav-number">3.</span> <span class="nav-text">三、GRPO组相对策略优化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1、GRPO的目标函数"><span class="nav-number">3.1.</span> <span class="nav-text">3.1、GRPO的目标函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2、GRPO的KL散度"><span class="nav-number">3.2.</span> <span class="nav-text">3.2、GRPO的KL散度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3、GRPO的优势函数"><span class="nav-number">3.3.</span> <span class="nav-text">3.3、GRPO的优势函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-4、deepseek-R1模型的目标函数"><span class="nav-number">3.4.</span> <span class="nav-text">3.4、deepseek R1模型的目标函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四、大语言模型中强化学习的统一范式"><span class="nav-number">4.</span> <span class="nav-text">四、大语言模型中强化学习的统一范式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1、SFT"><span class="nav-number">4.1.</span> <span class="nav-text">4.1、SFT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2、RFT"><span class="nav-number">4.2.</span> <span class="nav-text">4.2、RFT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-3、Online-RFT"><span class="nav-number">4.3.</span> <span class="nav-text">4.3、Online RFT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-4、DPO"><span class="nav-number">4.4.</span> <span class="nav-text">4.4、DPO</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-5、PPO"><span class="nav-number">4.5.</span> <span class="nav-text">4.5、PPO</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-6、GRPO"><span class="nav-number">4.6.</span> <span class="nav-text">4.6、GRPO</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参考文献"><span class="nav-number">5.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">引线小白&nbsp &nbsp |&nbsp &nbsp  一个理想主义者，再造自我，以期未来。</span>
</div>

<div class="powered-by">
  由 <a rel="external nofollow" class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a rel="external nofollow" class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>&nbsp &nbsp| &nbsp &nbsp Hosted by  <a href="https://github.com" style="font-weight: bold">Github Pages</a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



<!--   
   
  

  

 -->
  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
<!--   <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
     showMathMenu: false,
     showMathMenuMSIE: false,
     tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {autoNumber: 'AMS'},
          Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': {
          imageFont: null
        }
    });
  </script> -->
<!--     MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    }); -->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
    processEscapes: true,
    tags: 'ams',
    macros: {
                                            bm: "\\boldsymbol",
					    T:"\\intercal",
                                            oiint: "{\\iint\\kern{-20mu}{\\unicode{x2B2D}}}",
                                            oiiint: "{\\iiint\\kern{-24.5mu}\\large{\\unicode{x2B2D}}}"
                                },
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
  <script type="text/x-mathjax-config">
MathJax.texReset();
MathJax.typeset();
  </script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG-full"></script> -->


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("Es7wAnqi0QiGhMLoyl7mkrQo-gzGzoHsz", "BxPXaoPFp3PzWqBTSe6VUuQS");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</body>
</html>
