<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,概率图,隐马尔可夫模型" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="隐马尔可夫是一个古老的模型，开始我们回顾了一下它的基本问题。然后我们使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。中间我们使用EM算法，解决了参数求解的问题。人类可以通过一些精巧的设计，来获取超乎直观想象结果，当人类的思想开始集成，开始向深处，广处延伸时，上帝似乎很惊异。">
<meta name="keywords" content="机器学习,概率图,隐马尔可夫模型">
<meta property="og:type" content="article">
<meta property="og:title" content="隐马尔可夫模型">
<meta property="og:url" content="http://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/index.html">
<meta property="og:site_name" content="柠檬CC">
<meta property="og:description" content="隐马尔可夫是一个古老的模型，开始我们回顾了一下它的基本问题。然后我们使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。中间我们使用EM算法，解决了参数求解的问题。人类可以通过一些精巧的设计，来获取超乎直观想象结果，当人类的思想开始集成，开始向深处，广处延伸时，上帝似乎很惊异。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.limoncc.com/images/隐马尔可夫模型.png">
<meta property="og:image" content="https://www.limoncc.com/images/cc.png">
<meta property="og:image" content="https://www.limoncc.com/images/avatar.png">
<meta property="og:updated_time" content="2023-03-27T12:31:35.990Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="隐马尔可夫模型">
<meta name="twitter:description" content="隐马尔可夫是一个古老的模型，开始我们回顾了一下它的基本问题。然后我们使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。中间我们使用EM算法，解决了参数求解的问题。人类可以通过一些精巧的设计，来获取超乎直观想象结果，当人类的思想开始集成，开始向深处，广处延伸时，上帝似乎很惊异。">
<meta name="twitter:image" content="http://www.limoncc.com/images/隐马尔可夫模型.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: undefined,
      author: '博主'
    }
  };
</script>

  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <title> 隐马尔可夫模型 | 柠檬CC </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-73837972-3', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?d225b8f8559eb2eb6a8bd8792e01ebb9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=60130136";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">柠檬CC</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">小白爱吃柠檬O(∩_∩)O</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-poetry">
          <a href="/poetry" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-leaf"></i> <br />
            
            诗集
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                隐马尔可夫模型
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-11T21:37:57+08:00" content="2017-03-11">
              2017-03-11
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          
             <span id="/post/9b154bbdc2a51d2ea34ec070684b5132/" class="leancloud_visitors" data-flag-title="隐马尔可夫模型">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">阅读次数 </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>作者: <a href="https://www.limoncc.com">引线小白</a>-本文永久链接：<a href="https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/">https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/</a><br>知识共享许可协议: 本博客采用<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名-非商业-禁止演绎4.0</a>国际许可证</p>
<blockquote>
<p><strong>摘要</strong>：本文意在理清隐马尔可夫模型的问题。若有错误，请大家指正。<br><strong>关键词</strong>: <code>隐马尔可夫模型</code>,<code>前向-后向算法</code>,<code>维特比算法</code>,<code>鲍姆-韦尔奇算法</code></p>
</blockquote>
<h4 id="一、马尔可夫模型"><a href="#一、马尔可夫模型" class="headerlink" title="一、马尔可夫模型"></a>一、马尔可夫模型</h4><h5 id="1-1、基本概念"><a href="#1-1、基本概念" class="headerlink" title="1.1、基本概念"></a>1.1、基本概念</h5><h6 id="1-1-1、符号"><a href="#1-1-1、符号" class="headerlink" title="1.1.1、符号"></a>1.1.1、符号</h6><p>我们开始讨论，离散时间离散状态，也就是离散随机序列。假定：离散时间： $\displaystyle \{1,\cdots,T\}$；离散状态：$\displaystyle x_t\in \{1,\cdots,c,\cdots,C\}$。我们称</p>
<p>$$\begin{align}<br>p(\bm{x}_{1:T})=p(x_1)\prod_{t=2}^Tp\big(x_t\mid x_{t-1}\big)<br>\end{align}$$<br>为一阶马尔可夫模型。下面来初步认识一下这个模型。</p>
<h6 id="1-1-2、转移概率"><a href="#1-1-2、转移概率" class="headerlink" title="1.1.2、转移概率"></a>1.1.2、转移概率</h6><p>转移概率: 从状态 $\displaystyle i$到状态 $\displaystyle j$的概率 $\displaystyle A_{ij}=p\big(x_t=j\mid x_{t-1}=i\big)$，于是有转移矩阵$\displaystyle \bm{A}=\big[A_{ij}\big]$，且有 $\displaystyle \sum_{j=1}^KA_{ij}=\bm{I}^\text{T}\bm{a}_i=1$ 也就说 $\displaystyle \bm{A}$的每一行相加等于1。这样的矩阵我们称之为随机矩阵 $\displaystyle \textit{(stochastic matrix)}$。注意这个时候我们定义的转移矩阵与离散的时间没有关系。</p>
<p>下面我们定义第 $\displaystyle n$步转移矩阵：$\displaystyle A_{ij}\big(n\big)=p\big(x_{t+n}=j\mid x_{t}=i\big)$，已知 $\displaystyle \bm{A}(1)=\bm{A}$，我们有查普曼-柯尔莫哥洛夫等式 $\displaystyle \textit{(Chapman-Kolmogorov equations)}$</p>
<p>$$\begin{align}<br>A_{ij}(m+n)=\sum_{k=1}^KA_{ik}(m)A_{kj}(n)<br>\end{align}$$亦有：<br>$$\begin{align}<br>\bm{A}(m+n)=\bm{A}(m)\bm{A}(n)<br>\end{align}$$<br>于是有 $\displaystyle \bm{A}(n)=\bm{A}\bm{A}(n-1)=\bm{A}\bm{A}\bm{A}(n-2)=\cdots=\bm{A}^n$</p>
<h5 id="1-2、应用"><a href="#1-2、应用" class="headerlink" title="1.2、应用"></a>1.2、应用</h5><p>马尔可夫模型可以用于多个领域。例如把词序列看成是概率分布，状态空间是所有词的集合。我们可以定义一元模型、二元模型、三元模型、$\displaystyle n$ 元模型。进而可用于这些领域：句子补全、数据压缩、文本分类、自动写作等</p>
<h5 id="1-3、极大似然法与二元模型马尔可夫过程"><a href="#1-3、极大似然法与二元模型马尔可夫过程" class="headerlink" title="1.3、极大似然法与二元模型马尔可夫过程"></a>1.3、极大似然法与二元模型马尔可夫过程</h5><p>考虑单变量历史状态模型 $\displaystyle x_t\in\{1,\cdots,C\}$，同时定义一条链的数据集合 $\displaystyle x_{1:T}=\bm{x}$ 有：</p>
<p>$$\begin{align}<br>p\big(x_{1:T}\mid \bm{\pi},\bm{A}\big)<br>&amp;=p\big(x_1\big)A\big(x_1,x_2\big)\cdots A\big(x_{T-1},x_{T}\big)\\<br>p\big(\bm{x}\mid \bm{\pi},\bm{A}\big)&amp;=\mathrm{Cat}\big(x_1\mid\bm{\pi}\big)\cdots A\big(x_{T-1},x_{T}\big)\\<br>&amp;=\prod_{c=1}^C\pi_c^{\mathbb{I}(x_1=c)}\cdot\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t=s,x_{t-1}=c\right)}<br>\end{align}$$</p>
<p>定义：$\displaystyle \bm{x}_i=x_{1:T}^i$，进而定义链的集合 $\displaystyle \mathcal{D}=\{\bm{x}_i\}_{i=1}^N$，有该链集的对数似然函数：<br>$$\begin{align}<br> &amp;\ell\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big)<br>=\ln p\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big) \\<br>&amp;=\ln \prod_{i=1}^Np\big(\bm{x}_i\mid \bm{\pi},\bm{A}\big)<br>=\sum_{i=1}^N\ln\left[\prod_{c=1}^K\pi_c^{\mathbb{I}(x_1^i=c)}\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)}\right]\\<br>&amp;=\sum_{i=1}^N\ln \left[\prod_{c=1}^C\pi_c^{\mathbb{I}(x_1^i=c)}\right]+\sum_{i=1}^N\ln \left[\prod_{t=2}^T\prod_{c=1}^C\prod_{s=1}^CA_{cs}^{\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)}\right]\\<br>&amp;=\sum_{c=1}^C \left[\sum_{i=1}^N\mathbb{I}(x_1^i=c)\ln\pi_c\right]+\sum_{c=1}^C\sum_{s=1}^C \left[\sum_{i=1}^N\sum_{t=2}^T\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)\ln A_{cs}\right]\\<br>&amp;=\sum_{c=1}^CN_{c}^{1}\ln\pi_c+\sum_{c=1}^C\sum_{s=1}^C N_{cs}\ln A_{cs}\\<br>&amp;={\bm{N}_1}^\text{T}\ln \bm{\pi}+ \bm{I}^\text{T}\left[\bm{N}\odot\ln\bm{A}\right]\bm{I}<br>\end{align}$$</p>
<p>其中：$\displaystyle N_c^1=\sum_{i=1}^N\mathbb{I}(x_1^i=c)$，$\displaystyle  N_{cs}=\sum_{i=1}^N\sum_{t=2}^T\mathbb{I}\left(x_t^i=s,x_{t-1}^i=c\right)$，且有$\displaystyle \bm{I}^\text{T}\bm{\pi}=1, \bm{A}\bm{I}=\bm{I}$。将约束代入，我们有：<br>$$\begin{align}<br>\ell\big(\mathcal{D}\mid \bm{\pi},\bm{A}\big) ={\bm{N}_1}^\text{T}\ln \bm{\pi}+ \bm{I}^\text{T}\left[\bm{N}\odot\ln\bm{A}\right]\bm{I}+\lambda \left[\bm{I}^\text{T}\bm{\pi}-1\right]+ \left[\bm{A}\bm{I}-\bm{I}\right]^\text{T}\bm{\delta}<br>\end{align}$$<br>于是求先验概率参数有：</p>
<p>$$\begin{align}<br>\frac{\partial \ell}{\partial \bm{\pi}}&amp;=\frac{\bm{N}_1}{\bm{\pi}}+\lambda \bm{I}=0\\<br>\frac{\partial \ell}{\partial \lambda}&amp;=\bm{I}^\text{T}\bm{\pi}-1=0<br>\end{align}$$得 $$\begin{align}<br>\bm{\pi}=\frac{\bm{N}_1}{\bm{I}^\text{T}\bm{N}_1}<br>\end{align}$$</p>
<p>又求转移概率有：</p>
<p>$$\begin{align}<br>\frac{\partial \ell}{\partial \bm{A}}=\frac{\bm{I}\bm{I}^\text{T}\odot\bm{N}}{\bm{A}}+ \bm{I}^\text{T}\bm{\delta}=\frac{\bm{N}}{\bm{A}}+\bm{I}^\text{T}\bm{\delta}=\bm{0}\\<br>\frac{\partial \ell}{\partial \bm{\delta}}&amp;=\bm{A}\bm{I}-\bm{I}=\bm{0}<br>\end{align}$$</p>
<p>得<br>$$\begin{align}<br>\bm{A}=\frac{\bm{N}}{\bm{N}\bm{I}}=\frac{\bm{N}}{\bm{N}_c}<br>\end{align}$$</p>
<p>我们注意到：对于 $\displaystyle n$元模型 参数将会指数级增加 $\displaystyle O(C^n)$。对于5万个单词的语言模型，参数将到达惊人的25亿。显然我们是无法凑齐足够多数据，来使用极大似然法求参数。我们将面临严重的 <code>零数问题</code>。</p>
<h5 id="1-4、平滑方法"><a href="#1-4、平滑方法" class="headerlink" title="1.4、平滑方法"></a>1.4、平滑方法</h5><h6 id="1-4-1、拉普拉斯加一平滑"><a href="#1-4-1、拉普拉斯加一平滑" class="headerlink" title="1.4.1、拉普拉斯加一平滑"></a>1.4.1、拉普拉斯加一平滑</h6><p>当然我们可以使用 <code>拉普拉斯加一平滑</code>。当这不是唯一平滑的方法。</p>
<h6 id="1-4-2、删除插值"><a href="#1-4-2、删除插值" class="headerlink" title="1.4.2、删除插值"></a>1.4.2、删除插值</h6><p>$$\begin{align}<br>A=(1-\lambda)\frac{N_{cs}}{N_c}+\lambda \frac{N_s}{N}<br>\end{align}$$</p>
<p>删除插值约等于一个贝叶斯模型，假设一个狄利克雷先验，易得之</p>
<h6 id="1-4-3、古德-图灵估计与卡茨退避法"><a href="#1-4-3、古德-图灵估计与卡茨退避法" class="headerlink" title="1.4.3、古德-图灵估计与卡茨退避法"></a>1.4.3、古德-图灵估计与卡茨退避法</h6><p>我们有 <code>古德-图灵估计</code> $\displaystyle \textit{(Good-Turing Estimate)}$</p>
<h4 id="二、隐马尔可夫模型"><a href="#二、隐马尔可夫模型" class="headerlink" title="二、隐马尔可夫模型"></a>二、隐马尔可夫模型</h4><h5 id="2-1、基本定义"><a href="#2-1、基本定义" class="headerlink" title="2.1、基本定义"></a>2.1、基本定义</h5><p>1、隐变量的一阶马尔可夫假设 $\displaystyle  p\big(z_t\mid z_{t-1},\cdots,z_1\big)=p\big(z_t\mid z_{t-1}\big)$<br>2、观测变量的独立假设 $\displaystyle p\big(\bm{x}_t\mid \bm{x}_{t-1},\cdots,\bm{x}_1,z_t,\cdots,z_1\big)=p \big(\bm{x}_t\mid z_t\big)$</p>
<p>隐变量数据集 $\displaystyle \mathcal{D}_T^z=\{z_t\}_{t=1}^T$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}=\{\bm{x}_t\}_{t=1}^T$,完全数据集 $\displaystyle \mathcal{D}^+_T=\{\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\}$，且有：<br>$$\begin{align}<br>p \big(\mathcal{D}_T^+\big)<br>=p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)<br>=p\big(\mathcal{D}_T^\bm{x}\mid \mathcal{D}_T^z\big)p\big(\mathcal{D}^z\big)<br>\end{align}$$</p>
<div align="center"><img src="http://www.limoncc.com/images/隐马尔可夫模型.png" width="550" ==""><br>隐马尔可夫模型</div>

<p>为了简洁记，我们来定义一些符号。考虑隐变量有 $\displaystyle K$个状态 $\displaystyle z_t\in\{1,\cdots,K\}$，观测变量有 $\displaystyle O$个状态 $\displaystyle \bm{x}_t\in\{1,\cdots,O\}$。注意这里形式化的使用 $ o$来表示向量 $ \bm{x}_t$的状态，也就是说我们并没有定义向量是什么，因为这样能节约符号，和叙述上不必要的麻烦。继续定义若干参数：<br>$\displaystyle \pi[i]=p(z_1=i)$<br>$\displaystyle A[i,j]=p(z_t=j\mid z_{t-1}=i)$<br>$\displaystyle B[i,o]=p(\bm{x}_t=o\mid z_t=i)$<br>令参数集合为：<br>$$\begin{align}<br>\bm{\theta}=\{\bm{\pi},\bm{A},\bm{B}\}<br>\end{align}$$</p>
<p>注意这里的 $ \bm{\theta}$应该做形式化的理解，它既不是矩阵，也不是向量，而是一个相当于集合符号。在机器学习领域 $ \bm{\theta}$经常被用来表示参数，不对其不加具体定义。而对于观测变量是连续的，我们定义条件概率分布 $\displaystyle \rho_t[i]=p\big(\bm{x}_t\mid z_t=i\big)$ ，注意如果 $\displaystyle \bm{x}_t$未知，则 $\displaystyle \rho_t[i]$是个概率分布。而如果我们有数据集 $\displaystyle \mathcal{D}_t^{\bm{x}}=\{\bm{x}_1,\cdots,\bm{x}_t\}$， 也就说 $\displaystyle \bm{x}_t$已知，那么 $\displaystyle \rho_t[i]$是个数。那么参数集合是：</p>
<p>$$\begin{align}<br>\bm{\theta}=\{\bm{\pi},\bm{A},\bm{\rho}_t\}<br>\end{align}$$</p>
<p>注意：关于符号 $\displaystyle p(x)$和 $\displaystyle p(x=a)$的解释，这时的符号 $\displaystyle p$有点点转换。由一个函数变成了一个数。</p>
<h5 id="2-2、向前向后算法"><a href="#2-2、向前向后算法" class="headerlink" title="2.2、向前向后算法"></a>2.2、向前向后算法</h5><h6 id="2-2-1、直接计算"><a href="#2-2-1、直接计算" class="headerlink" title="2.2.1、直接计算"></a>2.2.1、直接计算</h6><p>我们要解决如何高效求观测变量数据集联合分布的问题，下面我们要分析一下这个基本问题。考虑离散隐变量 $\displaystyle z_t$，离散观测变量 $\displaystyle x_t$，这时隐马尔可夫模型观测数据集 $\displaystyle \mathcal{D}_T^{\bm{x}}=\{\bm{x}_t=o(t)\}_{t=1}^T$：<br>$$\begin{align}<br>p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)=p\big(\mathcal{D}_T^\bm{x}\mid \mathcal{D}_T^z\big)p\big(\mathcal{D}_T^z\big)=p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\Bigg[\prod_{t=2}^Tp\big(z_t\mid z_{t-1}\big)p\big(\bm{x}_t\mid z_t\big)\Bigg]<br>\end{align}$$整理写成：<br>$$\begin{align}<br>&amp;p\big(\mathcal{D}_T^\bm{x}\mid \bm{\theta}\big)=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K p\big(\mathcal{D}_T^{\bm{x}},\mathcal{D}_T^z\mid \bm{\theta}\big)\\<br>&amp;=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K \left[\Bigg[p(z_1)\prod_{t=2}^Tp\big(z_t\mid z_{t-1}\big)\Bigg]\Bigg[\prod_{t=1}^Tp\big(x_t\mid z_t\big)\Bigg]\right]\\<br>&amp;=\sum_{i(1)=1}^K \cdots\sum_{i(T)=1}^K  \left[\pi\big[i(1)\big]\cdot\prod_{t=2}^TA_t\big[i(t-1),i(t)\big]\cdot\prod_{t=1}^TB_t\big[i(t),o(t)\big]\right]\\<br>&amp;=\sum_{i(1)=1}^K\cdots\sum_{i(T)=1}^K\Bigg[\pi\big[i(1)\big]B_1\big[i(1),o(1)\big]\cdot\prod_{t=2}^T \bigg[A_t\big[i(t-1),i(t)\big]\cdot B_t\big[i(t),o(t)\big]\bigg]\Bigg]<br>\end{align}$$</p>
<p>观察一下大中括号里面的式子 </p>
<p>$$\begin{align}<br>\pi\big[i(1)\big]B_1\big[i(1),o(1)\big]A_2\big[i(1),i(2)\big] B_2\big[i(2),o(2)\big]\cdots A_T\big[i(T-1),i(T)\big] B_T\big[i(T),o(T)\big]<br>\end{align}$$</p>
<p>若观测变量是连续的有：<br>$$\begin{align}<br>p\big(\mathcal{D}_T^\bm{x}\mid \bm{\theta}\big)=\sum_{i_1=1}^K\cdots\sum_{i_T=1}^K\Bigg[\pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]\Bigg]<br>\end{align}$$<br>这个方法的计算复杂度是 $\displaystyle O\big({2TK}^T\big)$, 例如 $\displaystyle K=5,T=10$，那么就要约计算 $\displaystyle 1.95\times10^8$已经非常夸张了。为了更快的计算观测变量的联合分布，我们需要充分利用隐马尔可夫的性质：</p>
<h6 id="2-2-2、前向算法"><a href="#2-2-2、前向算法" class="headerlink" title="2.2.2、前向算法"></a>2.2.2、前向算法</h6><p>现在定义一个新的数据集，或者叫做在线数据集 $\displaystyle \mathcal{D}_t^{\bm{x}}=\{\bm{x}_i\}_{i=1}^t$我们定义 $\displaystyle \alpha_t[j]=p\big(\mathcal{D}_{t}^\bm{x}, z_{t}=j\big)$于是有：<br>$$\begin{align}\require{cancel}<br>&amp;\alpha_{t}[j]=p\big(\mathcal{D}_t^{\bm{x}},z_t=j)<br>=p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)p \big(\mathcal{D}_{t-1}^{\bm{x}}, z_t=j\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_t=j,z_{t-1}=i\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_t=j\mid z_{t-1}=i\big) p \big(z_{t-1}=i\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}}\mid \cancel{z_t=j},z_{t-1}=i\big)p \big(z_t=j\mid z_{t-1}=i\big) p \big(z_{t-1}=i\big)\\<br>&amp;=p \big(\bm{x}_t\mid z_t=j\big)\sum_{i=1}^Kp \big(\mathcal{D}_{t-1}^{\bm{x}},z_{t-1}=i\big)p \big(z_t=j \mid z_{t-1}=i\big)\\<br>&amp;=\rho_t[j]\sum_{i=1}^K\bigg[ \alpha_{t-1}[i]A[i,j]\bigg]\\<br>\end{align}$$<br>写成矩阵形式有：<br>$$\begin{align}<br>\bm{\alpha}_{t}=\bm{\rho}_t\odot\left[\bm{A}^\text{T}\bm{\alpha}_{t-1}\right]<br>\end{align}$$</p>
<p>于是有观测变量数据集联合分布<br>$$\begin{align}<br>  p\big(\mathcal{D}_T^{\bm{x}}\mid\bm{\theta}\big)<br>=\sum_{j=1}^K \alpha_T[j]<br>=\bm{I}^\text{T}\bm{\alpha}_{T}<br>\end{align}$$</p>
<p>计算复杂度变为了 $\displaystyle O({TK}^2)$。</p>
<h6 id="2-2-3、前向后向算法。"><a href="#2-2-3、前向后向算法。" class="headerlink" title="2.2.3、前向后向算法。"></a>2.2.3、前向后向算法。</h6><p>之前我们定义了 $\displaystyle \alpha_t[j]=p\big(\mathcal{D}_{t}, z_{t}=j\big)$，知道观测变量的条件概率：$\displaystyle \rho_t[j]=p\big(\bm{x}_t\mid z_t=j\big)$。我们再定义未来数据集的条件似然(有些书上也称后向概率)$\displaystyle \beta_t[i]=p\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=i\big)$，利用隐马尔可夫的条件假设，推导得到：<br>$$\begin{align}\require{cancel}<br>\beta_{t-1}[i]<br>&amp;=p\big(\mathcal{D}_{t:T}^{\bm{x}}\mid z_{t-1}=i\big)\\<br>&amp;=\sum_{j=1}^Kp\big(z_t=j,\bm{x}_t,\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_{t-1}=i\big)\\<br>&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j,\cancel{z_{t-1}=i},\cancel{\bm{x}_t}\big)p\big(z_t=j,\bm{x}_t\mid z_{t-1}=i\big)\\<br>&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p\big(\bm{x}_t\mid z_t=j,\cancel{z_{t-1}=i}\big)p\big(z_t=j\mid z_{t-1}=i\big)\\<br>&amp;=\sum_{j=1}^Kp\big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p\big(\bm{x}_t\mid z_t=j\big)p\big(z_t=j\mid z_{t-1}=i\big)\\<br>&amp;=\sum_{j=1}^K\beta_{t}[j]\cdot\rho_t[j]\cdot A[i,j]<br>\end{align}$$写成矩阵形式。<br>$$\begin{align}<br>\bm{\beta}_{t-1}=\bm{A} \left[\,\bm{\rho}_t\odot\bm{\beta}_t\right]<br>\end{align}$$</p>
<p>其中有 $\displaystyle \beta_{T}[i]=p\big(\mathcal{D}_{T+1:T}^{\bm{x}}\mid z_T=i\big)<br>=p \big(\emptyset\mid z_T=i\big)=1$，于是有 $\displaystyle \bm{\beta}_T=\bm{I}$<br>用这个似然和初始概率计算观测变量数据集联合分布有：</p>
<p>$$\begin{align}<br> p\big(\mathcal{D}_T^{\bm{x}}\mid\bm{\theta}\big)<br> = \sum_{i=1}^K\alpha_1[i]\beta_1[i]<br> =\sum_{i=1}^K\alpha_t[i]\beta_t[i]<br>=\bm{I}^\text{T}\big[\bm{\alpha}_t\odot\bm{\beta}_{t}\big]=\bm{I}^\text{T}\bm{\alpha}_{T}<br>\end{align}$$</p>
<h6 id="2-2-5、隐变量后验边际概率"><a href="#2-2-5、隐变量后验边际概率" class="headerlink" title="2.2.5、隐变量后验边际概率"></a>2.2.5、隐变量后验边际概率</h6><p>定义隐变量后验边际概率 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T\big)$，这个概率对我们颇为重要，下面使用向前向后算法计算它<br>$$\begin{align}<br> \eta_t[j]<br> &amp;=p\big(z_t=j\mid \mathcal{D}_T^{\bm{x}}\big)<br> =\frac{p \big(\mathcal{D}_T^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)}{p \big(\mathcal{D}_T\big)}\\<br> &amp;=\frac{p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid z_t=j\big)p \big(z_t=j\big)}{p \big(\mathcal{D}_T^{\bm{x}}\big)}<br> =\frac{p \big(\mathcal{D}_t^{\bm{x}}\mid z_t=j\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}, z_t=j\big)}{p \big(\mathcal{D}_T^{\bm{x}}\big)}\\<br> &amp;=\frac{\alpha_t[j]\cdot\beta_t[j]}{\displaystyle\sum_{i=1}^K \alpha_t[j]\cdot\beta_t[j]}<br> \propto \alpha_t[j]\cdot\beta_t[j]<br>\end{align}$$写成矩阵形式<br>$$\begin{align}<br> \bm{\eta}_t<br> = \frac{\bm{\alpha}_t\odot \bm{\beta}_t}{\bm{I}^\text{T}\big[\bm{\alpha}_t\odot \bm{\beta}_t\big]}<br> \propto  \bm{\alpha}_t\odot \bm{\beta}_t<br>\end{align}$$</p>
<h6 id="2-2-4、隐变量两点边际概率"><a href="#2-2-4、隐变量两点边际概率" class="headerlink" title="2.2.4、隐变量两点边际概率"></a>2.2.4、隐变量两点边际概率</h6><p>隐变量两点边际概率在模型的参数估计中有重要应用，使用向前向后算法可以计算它<br>$$\begin{align}<br>p \big(\bm{z}_{t-1}, \bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)<br>=\frac{p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{z}_{t-1},\bm{z}_t\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}<br>=\frac{p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}<br>\end{align}$$</p>
<p>单独考虑<br>$$\begin{align}\require{cancel}<br>p \big(\mathcal{D}_T^\bm{x}\mid\bm{z}_{t-1},\bm{z}_t\big)<br>&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{x}_t,\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_{t-1},\bm{z}_t\big)\\<br>&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}},\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_{t-1},\bm{z}_t\big)p \big(\bm{x}_t\mid\cancel{\bm{z}_{t-1}},\bm{z}_t\big)\\<br>&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1},\cancel{\bm{z}_t}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\cancel{\bm{z}_{t-1}},\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)\\<br>&amp;=p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)<br>\end{align}$$</p>
<p>代入得到<br>$$\begin{align}<br>p \big(\bm{z}_{t-1}, \bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)<br>&amp;=\frac{p \big(\mathcal{D}_{t-1}^{\bm{x}}\mid\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\<br>&amp;=\frac{p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{z}_{t-1}\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\<br>&amp;=\frac{p \big(\bm{z}_t\mid \bm{z}_{t-1}\big)p \big(\mathcal{D}_{t-1}^{\bm{x}},\bm{z}_{t-1}\big)p \big(\bm{x}_t\mid\bm{z}_t\big)p \big(\mathcal{D}_{t+1:T}^{\bm{x}}\mid\bm{z}_t\big)}{p \big(\mathcal{D}_T^\bm{x}\big)}\\<br>\end{align}$$</p>
<p>我们令 $\displaystyle \xi_{t}[i,j]=p\big(z_{t-1}=i,z_t=j\mid \mathcal{D}_T^\bm{x}\big)$则有：<br>$$\begin{align}<br> \xi_{t}[i,j]\propto A[i,j]\cdot\alpha_{t-1}[i]\cdot\rho_{t}[j]\cdot\beta_t[j]<br>\end{align}$$<br>如果同时令 $\displaystyle \bm{Z}_t=\bm{z}_{t-1}\bm{z}_t^\text{T}$， $\displaystyle \bm{\varXi}_t=\big[\xi_{t}[i,j]\big]$。亦有 $\displaystyle \bm{Z}_t\sim \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)$，注意这是一个矩阵分类分布。</p>
<p>$$\begin{align}<br>\mathrm{E}_q\big[\bm{Z}_t\big]=\mathrm{E}_q\big[\bm{z}_{t-1}\bm{z}_t^\text{T}\big]=\sum_{\bm{Z}\in\{0,1\}^{K\times K}} \bm{Z}_t \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)=\bm{\varXi}_t=\frac{\bm{A}\odot\bigg[\bm{\alpha}_{t-1}\big[\bm{\rho}_t\odot \bm{\beta}_t\big]^\text{T}\bigg]}{\bm{I}^\text{T}\big[\bm{\alpha}_t\odot\bm{\beta}_{t}\big]}<br>\end{align}$$</p>
<p>也就说 $\displaystyle  \mathrm{E}_q\big[z_{t-1}=i,z_t=j\big]=\xi_{t}[i,j]$。</p>
<h5 id="2-3、鲍姆-韦尔奇算法"><a href="#2-3、鲍姆-韦尔奇算法" class="headerlink" title="2.3、鲍姆-韦尔奇算法"></a>2.3、鲍姆-韦尔奇算法</h5><h6 id="2-3-1、完全数据集对数似然函数期望"><a href="#2-3-1、完全数据集对数似然函数期望" class="headerlink" title="2.3.1、完全数据集对数似然函数期望"></a>2.3.1、完全数据集对数似然函数期望</h6><p>对于隐变量我们有 $\displaystyle \mathcal{D}_T^z=\{z_t\}_{t=1}^T$， $\displaystyle z_t\in \{1,\cdots,K\}$。现在我们要改变隐变量符号的表示方法，即 $\displaystyle \mathcal{D}_T^\bm{z}=\{\bm{z}_t\}_{t=1}^T$, $\displaystyle \bm{z}_t\in \{0,1\}^K$。这个向量表示方法与之前的表示方法是等价的，务必熟悉这两个表示方法。<br>$$\begin{align}<br>\bm{z}_t=[ 0\cdots1\cdots0 ]^\text{T}\iff \bm{z}_t[k]=z_{tk}=1\iff z_t=k<br>\end{align}$$</p>
<p>我们有隐变量初始先验分布、观测变量条件分布、转移概率。<br>$$\begin{align}\begin{cases}<br>p \big(\bm{z}_t\mid \bm{\pi}\big)&amp;\displaystyle=\mathrm{Cat} \big(\bm{z}_t\mid \bm{\pi}\big)=\prod_{k=1}^K\pi_k^{z_t[k]}\\<br>p \big(\bm{x}_t\mid \bm{z}_t,\bm{\varphi}\big)&amp;\displaystyle=\prod_{i=1}^Kp \big(\bm{x}_t\mid \varphi_i\big)^{z_t[i]}\\<br>p \big(\bm{z}_t\mid \bm{z}_{t-1},\bm{A}\big)&amp;\displaystyle=\prod_{i=1}^K\prod_{j=1}^K A_{ij}^{z_t[i]z_{t-1}[j]}<br>\end{cases}\end{align}$$</p>
<p>代入有联合概率分布：<br>$$\begin{align}<br>p \big(\mathcal{D}_T^+\big)<br>&amp;=p\big(\bm{z}_1\big)p\big(\bm{x}_1\mid z_1\big)\Bigg[\prod_{t=2}^Tp\big(\bm{z}_t\mid \bm{z}_{t-1}\big)p\big(\bm{x}_t\mid \bm{z}_t\big)\Bigg]\\<br>&amp;=\prod_{k=1}^K\pi_k^{z_1[k]}\prod_{k=1}^Kp \big(\bm{x}_1\mid\varphi_k\big)^{z_1[k]}\left[\prod_{t=2}^T\Bigg[\prod_{i=1}^K\prod_{j=1}^K A_{ij}^{z_{t-1}[i]z_{t}[j]}\prod_{j=1}^Kp \big(\bm{x}_t\mid\varphi_j\big)^{z_t[j]}\Bigg]\right]<br>\end{align}$$<br>同时我们有 $\displaystyle q=q \big(\mathcal{D}_T^\bm{z}\mid \mathcal{D}_T^\bm{x},\theta\big)$。 为简洁记，我们定义一个数 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T^\bm{x}\big)$，是隐变量后验边际概率。这样有 $\displaystyle p\big(\bm{z}_t\mid \mathcal{D}_T^\bm{x}\big)=\mathrm{Cat} \big(\bm{z}_t\mid \bm{\eta}_t\big)=\prod_{j=1}^K\big(\eta_t[j]\big)^{z_t[j]}$。根据 $\displaystyle \textit{EM}$算法，我们来推导一下完全数据集对数似然函数期望具体表示形式。<br>$$\begin{align}<br>&amp;\ell \big(\mathcal{D}_T^\bm{x}\mid \theta\big)=\ln p \big(\mathcal{D}_T^\bm{x}\mid \theta\big)<br>=\mathrm{E}_{q}\big[\ell\big(\mathcal{D}_T^+\mid\theta\big)\big]+\mathrm{H}\big[q\big]+\mathrm{KL}\big[q\parallel p\big]\\<br>&amp;\propto\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}_T^+\mid \theta\big)\big]<br>=\mathrm{E}^\text{T}[\bm{z}_1]\ln\bm{\pi}+\sum_{t=1}^T\sum_{k=1}^K\mathrm{E}\big[z_t[k]\big]\ln p \big(\bm{x}_t\mid \varphi_k\big)+\sum_{t=2}^T\bm{I}^\text{T}\left[\mathrm{E}\big[\bm{Z}_t\big]\odot\ln\bm{A}\right]\bm{I}\\<br>&amp;=\bm{\eta}_1^\text{T}\ln\bm{\pi}+\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}<br>\end{align}$$</p>
<p>其中：<br>1、 $\displaystyle \eta_t[j]=p\big(z_t=j\mid \mathcal{D}_T^\bm{x}\big)$，也就说有 $\displaystyle \mathrm{E}_q\big[z_t[k]\big]=\eta_t[k]$，这根据分类分布不难理解。写成向量形式</p>
<p>$$\begin{align}<br>\mathrm{E}_q\big[\bm{z}_t\big]=\bm{\eta}_t<br>\end{align}$$</p>
<p>2、 $\displaystyle \xi_{t}[i,j]=p\big(z_{t-1}=i,z_t=j\mid \mathcal{D}_T^\bm{x}\big)$是隐变量两点边际概率。我们可以定义 $\displaystyle \bm{Z}_t=\bm{z}_{t-1}\bm{z}_t^\text{T}$，那么根据我们上面的推导有 $$\begin{align}<br>\mathrm{E}_q\big[\bm{Z}_t\big]=\sum_{\bm{Z}\in\{0,1\}^{K\times K}} \bm{Z}_t \mathrm{Cat} \big(\bm{Z}_t\mid \bm{\varXi}_t\big)=\bm{\varXi}_t<br>\end{align}$$也就说 $\displaystyle  \mathrm{E}_q\big[z_{t-1}=i,z_t=j\big]=\xi_{t}[i,j]$。</p>
<h6 id="2-3-2、求解-displaystyle-bm-pi-、-displaystyle-bm-A"><a href="#2-3-2、求解-displaystyle-bm-pi-、-displaystyle-bm-A" class="headerlink" title="2.3.2、求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$"></a>2.3.2、求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$</h6><p>显然若要求解参数 $\displaystyle \bm{\theta}=\{\bm{\pi},\bm{A},\bm{\rho}\}$，就必须知道 $\displaystyle \bm{\eta}_t，\bm{\varXi}_t$。这在之后的前向先后算法中我们已经求出，也就是 <strong>E </strong>步。现在推导 $\displaystyle \textit{EM}$算法的 <strong>M </strong>步。有：<br>$$\begin{align}<br>\exists \,\theta_{end}\to\max\ell \big(\mathcal{D}^\bm{x}\mid \theta\big)\iff\max \mathrm{E}_{q}\big[\ell\big(\mathcal{D}^+\mid \theta\big)\big]<br>\end{align}$$记$$\begin{align}<br>\mathrm{E}_{\mathcal{D}_T^+}=\mathrm{E}_{q}\big[\ln p\big(\mathcal{D}_T^+\mid \theta\big)\big]=\bm{\eta}_1^\text{T}\ln\bm{\pi}+\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}<br>\end{align}$$忽略 $\displaystyle p \big(\bm{x}_t\mid \varphi_k\big)$的严格定义 。先求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$。我们有：<br>$$\begin{align}<br>\mathrm{E}_{\mathcal{D}_T^+}<br>\propto \bm{\eta}_1^\text{T}\ln\bm{\pi}+\lambda\big[\bm{I}^\text{T}\bm{\pi}-1\big]<br>\end{align}$$得：$$\begin{align}<br>\begin{cases}<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \bm{\pi}}&amp;\displaystyle=\frac{\bm{\eta}_1}{\bm{\pi}}+\lambda \bm{I}=\bm{0}\\<br>\\<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \lambda}&amp;=\bm{I}^\text{T}\bm{\pi}-1=0<br>\end{cases}<br>\end{align}$$<br>易得：$$\begin{align}<br>\hat{\bm{\pi}}=\frac{\bm{\eta}_1}{\bm{I}^\text{T}\bm{\eta}_1}<br>\end{align}$$<br>考虑 $ \bm{A}$有：$$\begin{align}<br>\mathrm{E}_{\mathcal{D}_T^+}<br>\propto \sum_{t=2}^T\bm{I}^\text{T}\left[\bm{\varXi}_t\odot\ln\bm{A}\right]\bm{I}+\big[\bm{A}\bm{I}-\bm{I}\big]^\text{T}\bm{\delta}<br>\end{align}$$有：$$\begin{align}<br>\begin{cases}<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \bm{A}}&amp;\displaystyle= \frac{\sum_{t=2}^T{\bm{I}\bm{I}}^\text{T}\odot \bm{\varXi}_t}{\displaystyle\bm{A}}+\bm{I}^\text{T}\bm{\delta}=\bm{0}\\<br>\\<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \bm{\delta}}&amp;=\bm{A}\bm{I}-\bm{I}=\bm{0}<br>\end{cases}<br>\end{align}$$得：$$\begin{align}<br>\hat{\bm{A}}=\frac{\sum_{t=2}^T\bm{\varXi}_t}{\sum_{t=2}^T\bm{\varXi}_t \bm{I}}=\sum_{t=2}^\text{T} \frac{\bm{\varXi}_t}{\bm{\varXi}_t \bm{I}}<br>\end{align}$$</p>
<h6 id="2-3-3、求解观测变量条件概率参数"><a href="#2-3-3、求解观测变量条件概率参数" class="headerlink" title="2.3.3、求解观测变量条件概率参数"></a>2.3.3、求解观测变量条件概率参数</h6><p>考虑 $\displaystyle \bm{x}_t$服从分类分布，有条件密度：<br>$$\begin{align}<br>\rho_t[k]=p \big(\bm{x}_t\mid \varphi_k\big)=\mathrm{Cat}\big(\bm{x}_t\mid \bm{\mu}_k\big)=\prod_{c=1}^C\mu_{kc}^{x_t[c]}<br>\end{align}$$同时我们定义 $\displaystyle \bm{U}=[\mu_{ck}]_{K\times C}$，有：<br>$$\begin{align}<br>\mathrm{E}_{\mathcal{D}_T^+}<br>&amp;\propto\sum_{t=1}^T\bm{\eta}_t^\text{T}\ln \bm{\rho}_t+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}<br>=\sum_{t=1}^T\sum_{k=1}^K\eta_t[k]\ln\bigg[\prod_{c=1}^C\mu_{kc}^{x_t[c]}\bigg]+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}\\<br>&amp;=\sum_{t=1}^T\sum_{k=1}^K\eta_t[k] \cdot\bm{x}_t ^\text{T}\ln \bm{\mu}_k+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}\\<br>&amp;=\sum_{t=1}^T \bm{\eta}_t^\text{T}\cdot\ln [\bm{U}]\cdot\bm{x}_t+\big[\bm{U}\bm{I}-\bm{I}\big]^\text{T}\bm{\kappa}<br>\end{align}$$ 求极值有：<br>$$\begin{align}<br>\begin{cases}<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \bm{U}}&amp;\displaystyle= \frac{\sum_{t=1}^T\bm{\eta}_t \bm{x}_t ^\text{T}}{\bm{U}}+\bm{I}^\text{T}\bm{\kappa}=\bm{0}\\<br>\displaystyle\frac{\partial \mathrm{E}_{\mathcal{D}_T^+}}{\partial  \bm{\kappa}}&amp;=\bm{U}\bm{I}-\bm{I}=\bm{0}<br>\end{cases}<br>\end{align}$$可得<br>$$\begin{align}<br>\hat{\bm{U}}=\sum_{t=1}^T \frac{\bm{\eta}_t \bm{x}_t ^\text{T}}{\bm{\eta}_t \bm{x}_t ^\text{T} \bm{I}}<br>\end{align}$$</p>
<h5 id="2-4、维特比算法"><a href="#2-4、维特比算法" class="headerlink" title="2.4、维特比算法"></a>2.4、维特比算法</h5><h6 id="2-4-1、问题描述"><a href="#2-4-1、问题描述" class="headerlink" title="2.4.1、问题描述"></a>2.4.1、问题描述</h6><p>定义隐变量数据集 $\displaystyle \mathcal{D}_T^z$，观测变量数据集 $\displaystyle \mathcal{D}_T^\bm{x}$,完全数据集 $\displaystyle \mathcal{D}^+_T$。维特比算法要解决的是求隐变量最可能状态序列。可以理解为：已知观测变量数据集，推断隐变量数据集：</p>
<p>$$\begin{align}<br>\hat{\mathcal{D}}_T^\bm{z}=\mathop{\mathrm{argmax}}_{\mathcal{D}_T^\bm{z}}\,p \big(\mathcal{D}_T^\bm{z}\mid\mathcal{D}_T^\bm{x}\big)<br>\end{align}$$注意到：<br>$$\begin{align}<br>p \big(\mathcal{D}_{t}^z\mid \mathcal{D}_t^\bm{x}\big)=p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)\big/p \big(\mathcal{D}_t^\bm{x}\big)\propto p \big(\mathcal{D}_t^\bm{x},\mathcal{D}_{t}^z \big)<br>\end{align}$$相差一个常数 $ p \big(\mathcal{D}_t^\bm{x}\big)$，通过直接优化完全数据集会简化问题。我们选择最大化下式：<br>$$\begin{align}<br>\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)<br>=\max_{\mathcal{D}_t^z}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\<br>\end{align}$$其中完全数据集的概率 $\displaystyle \pi[i_1]\rho_1[i_1]\cdot\prod_{t=2}^T \bigg[A_t[i_{t-1},i_{t}]\cdot \rho_t[i_t]\bigg]$，写成对数形式有：<br>$$\begin{align}<br>\ln\pi[i_1]+\ln\rho_1[i_1]+\sum_{t=2}^T \bigg[\ln A_t[i_{t-1},i_{t}]+\ln \rho_t[i_t]\bigg]<br>\end{align}$$这个形式有利于我们着手分析。</p>
<h6 id="2-4-2、前向计算"><a href="#2-4-2、前向计算" class="headerlink" title="2.4.2、前向计算"></a>2.4.2、前向计算</h6><p>我们可以拆分隐变量数据集 $ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t\}$，拆分的关键直觉是时刻 $ t$的最可能路径必须有是由时刻 $ t-1$的最可能路径组成。问题变为<br>$$\begin{align}<br>\max_{\mathcal{D}_t^z} p\big(\mathcal{D}_t^\bm{x},\mathcal{D}_t^z\big)<br>=\max_{z_t}\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t, \mathcal{D}_t^\bm{x}\big)<br>\end{align}$$追寻这一关键思想，下面来具体化：我们假设 $ t$时刻的状态为 $ i_t$，进而定义路径$ \mathcal{D}_t^z=\mathcal{D}_{t-1}^z\cup \{z_t=i_t\}$的最大概率(权重)：<br>$$\begin{align}<br>\delta_t[i_t]=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)<br>\end{align}$$为了充分利用隐马尔可夫模型的条件独立性质和动态规划思想，假设 $ t-1$的状态为 $ i_{t-1}$。继续拆分数据集于是有：<br>$$\begin{align}<br>\delta_{t}[i_t]<br>&amp;=\max_{\mathcal{D}_{t-1}^z} p \big(\mathcal{D}_{t-1}^z,z_t=i_t, \mathcal{D}_t^\bm{x}\big)\\<br>&amp;=\max_{\mathcal{D}_{t-2}^z,z_{t-1}=i_{t-1}}\left[ p\big(z_1\big)p\big(\bm{x}_1\mid z_1\big)\bigg[\prod_{\tau=2}^tp\big(z_\tau\mid z_{\tau-1}\big)p\big(\bm{x}_\tau\mid z_\tau\big)\bigg]\right]\\<br>&amp;=\max_{i_{t-1}}\bigg[p\big(z_t=i_t\mid z_{t-1}=i_{t-1}\big)p\big(\bm{x}_t\mid z_t=i_t\big)\max_{\mathcal{D}_{t-2}^z} p \big(\mathcal{D}_{t-2}^\bm{x},\mathcal{D}_{t-1}^z,z_{t-1}=i_{t-1}\big)\bigg]\\<br>&amp;=\max_{i_{t-1}}  \delta_{t-1}[i_{t-1}]A[i_{t-1},i_t]\rho_t[i_t]\\<br>\end{align}$$<br>也就是说：时刻 $ t$行至状态 $ i_t$的最可能路径必须有是由时刻 $ t-1$ 行至其他状态 $ i_{t-1}$的最可能路径组成。<br>$$\begin{align}<br>\delta_{t}[i_t]=\max_{i_{t-1}}\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]<br>\end{align}$$</p>
<p>写成矩阵形式固定 $\displaystyle i_t$，令$\displaystyle a[i_{t-1}]=A[i_{t-1},i_t]$则有</p>
<p>$$\begin{align}<br>\delta_t[i_t],\widehat{i}_{t-1}[z_t=i_t]<br>&amp;=\max[\bm{\delta}_{t-1}\odot\bm{a}_{t-1}]\rho_t[i_t]\\<br>\bm{\delta}_t,\widehat{\bm{i}}_{t-1}<br>&amp;=\big[\mathop{\mathrm{colmax}}[\bm{\delta}_{t-1}\odot\bm{A}]^\text{T}\odot\bm{\rho}_t<br>\end{align}$$</p>
<p>加上维度应该更容易理解</p>
<p>$$\begin{align}<br>\bm{\delta}_t,\widehat{\bm{i}}_{t-1}<br>&amp;=\big[\underbrace{\mathop{\mathop{\mathrm{colmax}}[\underbrace{\mathop{\bm{\delta}_{t-1}}}_{1\times C}\odot\underbrace{\mathop{\bm{A}}}_{C \times C}}}_{C\times 1}\big]^\text{T}\odot\underbrace{\mathop{\bm{\rho}_t}}_{1\times C}<br>\end{align}$$</p>
<p>定义初始状态 $ \delta_1[i_1]=\pi[i_1]\rho_1[i_1]$，同时我们注意到 $\displaystyle i_t$是可以任意的，递归计算我们得到一个网格图 <strong><code>trellis diagram</code></strong>。</p>
<p>$$\begin{align}<br>\Delta=\{\bm{\delta}_1,\cdots,\bm{\delta}_t,\cdots,\bm{\delta}_T\}<br>\end{align}$$</p>
<h6 id="2-4-3、后向回溯"><a href="#2-4-3、后向回溯" class="headerlink" title="2.4.3、后向回溯"></a>2.4.3、后向回溯</h6><p>回到我们的问题：已知观测变量数据集，推断隐变量数据集。最大化联合概率问题变为<br>$$\begin{align}<br>\max_{\mathcal{D}_T^z} p\big(\mathcal{D}_T^\bm{x},\mathcal{D}_T^z\big)<br>=\max_{i_T}\max_{i_{T-1}}A[i_{T-1},i_t] \rho_T[i_T]\cdots\max_{i_1}\delta_1[i_1]A[i_{1},i_2] \rho_2[i_2]<br>\end{align}$$</p>
<p>为了解决这个问题，回顾动态规划思想：最优路径 $ \hat{\mathcal{D}}_{1:T}^z$的一部分 $ \hat{\mathcal{D}}_{t:T}^z$对于 $ t:T$的所有可能路径 $ \mathcal{D}_{t:T}^z$必然是最优。如果存在另外一条路径 $  \tilde{\mathcal{D}}_{t:T}^z$是最优的，那么会出现矛盾 $ \hat{\mathcal{D}}_{1:t}^z\cup   \tilde{\mathcal{D}}_{t:T}^z\neq \hat{\mathcal{D}}_{1:T}^z $，所以 $ \hat{\mathcal{D}}_{t:T}^z$ 必须是最优的。根据这一思想，我们定义回溯操作 <strong>traceback </strong>: $ \omega_t[\cdot]$，来从后向前还原最优状态序列。<br>$$\begin{align}<br>\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\delta_{t-1}[i_{t-1}]A[i_{t-1},i_t] \rho_t[i_t]<br>\end{align}$$定义 $ T$时刻最优状态 $\displaystyle \hat{z}_{T}=\mathop{\mathrm{argmax}}_{i_T}\,\delta_T[i_T]$。应用回溯操作，得到最优路径：<br>$$\begin{align}<br>\hat{\mathcal{D}}_{T}^z=\{\hat{z}_{t-1}=\omega_t[\hat{z}_{t}]\}_{t=T}^1<br>\end{align}$$</p>
<p>为了解决数据下溢问题，我们可以取对数<br>$$\begin{align}<br>&amp;\ln\delta_{t}[i_t]<br>=\max_{i_{t-1}}\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]\\<br>&amp;\hat{z}_{t-1}=\omega_t[i_t]=\mathop{\mathrm{argmax}}_{i_{t-1}}\,\big[\ln\delta_{t-1}[i_{t-1}]+\ln A[i_{t-1},i_t] +\ln\rho_t[i_t]\big]<br>\end{align}$$</p>
<hr>
<p>算法：维特比算法</p><p></p>
<hr>
<p><!-- 作者：引线小白
邮箱：limoncc@icloud.com
网址：www.limoncc.com  --><font face="Times" new="" roman="" size="3" color="#0066CC">1 $\displaystyle \bm{\delta}_1=\bm{\pi}$<br>2 $\displaystyle \text{ for }\,t=2:T$<br> $\displaystyle<br>\quad\begin{array}{|lc}<br>\text{ for }\,i_{t}=1:C \\<br>\quad\begin{array}{|lc}<br>\displaystyle \text{Traceback_Data}=\big[\ln\delta_t[i_t],\omega_{t-1}[i_t]\big]=\big[\mathop{\mathrm{colmax}}[\ln\bm{\delta}_{t-1}+\ln\bm{A}\big]^\text{T}+\ln\bm{\rho}_t<br>\end{array}\\<br>\text{ end}\\<br>\end{array}<br>$<br>3 end<br>$\displaystyle [\ln\delta_T,\hat{z}_{T}]=\max_{i_T}\,\ln\bm{\delta}_T$<br>4 $\displaystyle \text{ for }\,t=T:2$<br> $\displaystyle<br>\quad\begin{array}{|lc}<br>\hat{z}_{t-1}=\omega_{t}[\hat{z}_{t}]<br>\end{array}<br>$<br>5 end<br>6 $\displaystyle \hat{\bm{z}}$</font></p>
<hr>
<h4 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h4><p>1、隐马尔可夫是一个古老的模型，开始我们回顾了一下它的基本问题<br>2、然后我们使用前向后向算法和维比特算法，解决了求值和求隐状态的问题。<br>3、中间我们使用 $\displaystyle EM$算法，解决了参数求解的问题。<br>4、人类可以通过一些精巧的设计，来获取超乎直观想象结果，当人类的思想开始集成，开始向深处，广处延伸时，上帝似乎很惊异。</p>
<p><hr></p>
<p><table border="1" width="100%"><tr><td align="center" width="18%">版权声明</td><td align="left" width="82%"><img src="https://www.limoncc.com/images/cc.png" width="18%"></td></tr><tr><td align="center" width="18%"><img src="https://www.limoncc.com/images/avatar.png" width="100%"></td><td align="left" width="82%">由<a href="https://www.limoncc.com">引线小白</a>创作并维护的<a href="http://www.limoncc.com">柠檬CC</a>博客采用<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="external">署名-非商业-禁止演绎4.0</a>国际许可证。<br>本文首发于柠檬CC <a href="https://www.limoncc.com">[ https://www.limoncc.com ]</a> , 版权所有、侵权必究。</td></tr><tr><td align="center" width="18%">本文永久链接</td><td align="left" width="82%"><a href="https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/">https://www.limoncc.com/post/9b154bbdc2a51d2ea34ec070684b5132/</a></td></tr></table><div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a href="#" class="bds_duitang" data-cmd="duitang" title="分享到堆糖"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a></div></p>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{},"image":{"viewList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","douban","weixin","sqq","duitang","qzone","fbook","twi"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
            <a href="/tags/概率图/" rel="tag">#概率图</a>
          
            <a href="/tags/隐马尔可夫模型/" rel="tag">#隐马尔可夫模型</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/post/b9646fbf6c8d542fdb68ac01ad3c1c5a/" rel="next" title="概率图基础">
                <i class="fa fa-chevron-left"></i> 概率图基础
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/post/875b887a5e7e39e56eea929a72336d4e/" rel="prev" title="条件随机场模型">
                条件随机场模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


<!--           </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div> -->
        <div id="vcomments"></div>
        <script>new Valine({
          el: "#vcomments",
      appId: "BVjuNRCpkVSkz82jFmadIvY8-gzGzoHsz",
    appKey: "bRjXPp55dop7RTC2xgunGdiP"})
  </script>'
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="引线小白" />
          <p class="site-author-name" itemprop="name">引线小白</p>
          <p class="site-description motion-element" itemprop="description">小湖椰影廊桥,曾记否,谷围晓月,灯影朦胧。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">49</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="" target="_blank" title="Design">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Design
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://www.behance.net/limoncc" target="_blank" title="Behance">
                  
                    <i class="fa fa-fw fa-behance"></i>
                  
                  Behance
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://www.pinterest.com/aegeanfan/" target="_blank" title="Pinterest">
                  
                    <i class="fa fa-fw fa-pinterest"></i>
                  
                  Pinterest
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="https://github.com/limoncc" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="http://weibo.com/3483157951" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a rel="external nofollow" href="http://www.zhihu.com/people/limoncc" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              设计不止，折腾不息。
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://chuangzaoshi.com" title="创造狮" target="_blank">创造狮</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、马尔可夫模型"><span class="nav-number">1.</span> <span class="nav-text">一、马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1、基本概念"><span class="nav-number">1.1.</span> <span class="nav-text">1.1、基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-1-1、符号"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1、符号</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-1-2、转移概率"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2、转移概率</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2、应用"><span class="nav-number">1.2.</span> <span class="nav-text">1.2、应用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3、极大似然法与二元模型马尔可夫过程"><span class="nav-number">1.3.</span> <span class="nav-text">1.3、极大似然法与二元模型马尔可夫过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4、平滑方法"><span class="nav-number">1.4.</span> <span class="nav-text">1.4、平滑方法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-4-1、拉普拉斯加一平滑"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.4.1、拉普拉斯加一平滑</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-4-2、删除插值"><span class="nav-number">1.4.2.</span> <span class="nav-text">1.4.2、删除插值</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#1-4-3、古德-图灵估计与卡茨退避法"><span class="nav-number">1.4.3.</span> <span class="nav-text">1.4.3、古德-图灵估计与卡茨退避法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二、隐马尔可夫模型"><span class="nav-number">2.</span> <span class="nav-text">二、隐马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1、基本定义"><span class="nav-number">2.1.</span> <span class="nav-text">2.1、基本定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2、向前向后算法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2、向前向后算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-1、直接计算"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.1、直接计算</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-2、前向算法"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.2、前向算法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-3、前向后向算法。"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.2.3、前向后向算法。</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-5、隐变量后验边际概率"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.2.5、隐变量后验边际概率</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4、隐变量两点边际概率"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.4、隐变量两点边际概率</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3、鲍姆-韦尔奇算法"><span class="nav-number">2.3.</span> <span class="nav-text">2.3、鲍姆-韦尔奇算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-1、完全数据集对数似然函数期望"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1、完全数据集对数似然函数期望</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-2、求解-displaystyle-bm-pi-、-displaystyle-bm-A"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2、求解 $\displaystyle \bm{\pi}$、 $\displaystyle \bm{A}$</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-3、求解观测变量条件概率参数"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3、求解观测变量条件概率参数</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-4、维特比算法"><span class="nav-number">2.4.</span> <span class="nav-text">2.4、维特比算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-4-1、问题描述"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.4.1、问题描述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-4-2、前向计算"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.4.2、前向计算</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-4-3、后向回溯"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.4.3、后向回溯</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三、总结"><span class="nav-number">3.</span> <span class="nav-text">三、总结</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">引线小白&nbsp &nbsp |&nbsp &nbsp  一个理想主义者，再造自我，以期未来。</span>
</div>

<div class="powered-by">
  由 <a rel="external nofollow" class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a rel="external nofollow" class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>&nbsp &nbsp| &nbsp &nbsp Hosted by  <a href="https://github.com" style="font-weight: bold">Github Pages</a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



<!--   
   
  

  

 -->
  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
<!--   <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
     showMathMenu: false,
     showMathMenuMSIE: false,
     tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
          processEscapes: true
        },
        TeX: {
          equationNumbers: {autoNumber: 'AMS'},
          Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': {
          imageFont: null
        }
    });
  </script> -->
<!--     MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    }); -->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
    processEscapes: true,
    tags: 'ams',
    macros: {
                                            bm: "\\boldsymbol",
                                            oiint: "{\\iint\\kern{-20mu}{\\unicode{x2B2D}}}",
                                            oiiint: "{\\iiint\\kern{-24.5mu}\\large{\\unicode{x2B2D}}}"
                                },
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
  <script type="text/x-mathjax-config">
MathJax.texReset();
MathJax.typeset();
  </script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG-full"></script> -->


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("Es7wAnqi0QiGhMLoyl7mkrQo-gzGzoHsz", "BxPXaoPFp3PzWqBTSe6VUuQS");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>

</body>
</html>
